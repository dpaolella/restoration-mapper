{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_x_mini = np.load('data/gan_processed_data_original/lab_x_mini.npy')\n",
    "lab_y_mini = np.load('data/gan_processed_data_original/lab_y_mini.npy')\n",
    "val_x_mini = np.load('data/gan_processed_data_original/val_x_mini.npy')\n",
    "val_y_mini = np.load('data/gan_processed_data_original/val_y_mini.npy')\n",
    "unlab_x_mini = np.load('data/gan_processed_data_original/unlab_x_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_x_1000 = np.load('data/gan_processed_data_original/lab_x_1000.npy')\n",
    "lab_y_1000 = np.load('data/gan_processed_data_original/lab_y_1000.npy')\n",
    "val_x_500 = np.load('data/gan_processed_data_original/val_x_500.npy')\n",
    "val_y_500 = np.load('data/gan_processed_data_original/val_y_500.npy')\n",
    "unlab_x_2000 = np.load('data/gan_processed_data_original/unlab_x_2000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_x_mini.shape)\n",
    "print(lab_y_mini.shape)\n",
    "print(val_x_mini.shape)\n",
    "print(val_y_mini.shape)\n",
    "print(unlab_x_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_x_1000.shape)\n",
    "print(lab_y_1000.shape)\n",
    "print(val_x_500.shape)\n",
    "print(val_y_500.shape)\n",
    "print(unlab_x_2000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am just adjusting the data Lucien created to match that of their network:\n",
    "1. The multiple images stacked on top of each other is their last dimension - whereas in the npy from Lucien it is the first\n",
    "    - this is something that we should fix at the data generation point this is just a patch that can be removed if Lucien regenarates the data concatenating it on the last dimension\n",
    "2. I am picking one of our time dimensions\n",
    "3. I am picking one of our 16 bands for now - haven't gotten to seeing how the code would handle it if I set num_channels = 16, so want to just get it to run like this first\n",
    "4. I am removing a pixel around the rim - not sure why our inputs are 16x16 but our masks are 14x14, but don't want that to trip it up for now\n",
    "\n",
    "    #L: the masks represent the labels, so our tree labels are 14x14 instead of the input dimensions of 16x16. This is something unique to our data, I'd imagine in their case (and most cases for segmentation) the dimensionality of the labels and the input data is the same. So we will have to adjust the architecture to allow for this.\n",
    "    \n",
    "    #L: we need to get the input back to 16x16 because all the convolutional arithmetic depends on that. will change mask to 16x16 for now and keep in mind that we need to fix that to 14x14 and address that in the GAN code.\n",
    "    \n",
    "    - though they have some mechanism for fixing this in utils.load_val_images, where they append 0's to match the image sizes, but not sure why their image dimensions don't match either... and that part is also commented out for now\n",
    "5. we might have to adjust the size of this tiny toy dataset to be a little bit bigger\n",
    " - see my comment titled: \"#D: we might have to adjust this if our mini sample is too small compared to batch size\" \n",
    "     - I got rid of code that would duplicate data if the input dataset is too small, so if you get far enough in the training this might become a problem \n",
    "     - so I am letting you know here so you know that is the problem when you get to an error that might be caused by it / maybe = even better: we should just feed it a little bit of a bigger dataset. The batch_size = 20, so let's jsut feed it more than that - I think we will do that anyway, we are not connerned about proving that it works with a super tiny training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L: todo later, we need to make sure we are selecting a single band and not a single x/y value,\n",
    "#I forget how the data is originally structured\n",
    "lab_x_mini2 = np.moveaxis(lab_x_mini,0,-1)[0,:,:,:,:]\n",
    "val_x_mini2 = np.moveaxis(val_x_mini,0,-1)[0,:,:,:,:]\n",
    "unlab_x_mini2 = np.moveaxis(unlab_x_mini,0,-1)[0,:,:,:,:]\n",
    "#npad = ((1, 1), (1, 1), (0, 0))\n",
    "#lab_y_mini2 = np.pad(np.moveaxis(lab_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "#val_y_mini2 = np.pad(np.moveaxis(val_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "lab_y_mini2 = np.moveaxis(lab_y_mini,0,-1)\n",
    "val_y_mini2 = np.moveaxis(val_y_mini,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L: todo later, we need to make sure we are selecting a single band and not a single x/y value,\n",
    "#I forget how the data is originally structured\n",
    "lab_x_1000_2 = np.moveaxis(lab_x_1000,0,-1)[0,:,:,:,:]\n",
    "val_x_500_2 = np.moveaxis(val_x_500,0,-1)[0,:,:,:,:]\n",
    "unlab_x_2000_2 = np.moveaxis(unlab_x_2000,0,-1)[0,:,:,:,:]\n",
    "#npad = ((1, 1), (1, 1), (0, 0))\n",
    "#lab_y_mini2 = np.pad(np.moveaxis(lab_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "#val_y_mini2 = np.pad(np.moveaxis(val_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "lab_y_1000_2 = np.moveaxis(lab_y_1000,0,-1)\n",
    "val_y_500_2 = np.moveaxis(val_y_500,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L: todo later, we need to make sure we are selecting a single band and not a single x/y value,\n",
    "#I forget how the data is originally structured\n",
    "lab_x_1000_3 = np.moveaxis(lab_x_1000,0,-1)[:,:,:,:,:]\n",
    "lab_x_1000_3 = np.mean(lab_x_1000_3, axis=0)\n",
    "\n",
    "val_x_500_3 = np.moveaxis(val_x_500,0,-1)[:,:,:,:,:]\n",
    "val_x_500_3 = np.mean(val_x_500_3, axis=0)\n",
    "\n",
    "unlab_x_2000_3 = np.moveaxis(unlab_x_2000,0,-1)[:,:,:,:,:]\n",
    "unlab_x_2000_3 = np.mean(unlab_x_2000_3, axis=0)\n",
    "#npad = ((1, 1), (1, 1), (0, 0))\n",
    "#lab_y_mini2 = np.pad(np.moveaxis(lab_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "#val_y_mini2 = np.pad(np.moveaxis(val_y_mini,0,-1), pad_width = npad, mode = 'constant')\n",
    "lab_y_1000_3 = np.moveaxis(lab_y_1000,0,-1)\n",
    "val_y_500_3 = np.moveaxis(val_y_500,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_x_1000_3.shape)\n",
    "print(lab_y_1000_3.shape)\n",
    "print(val_x_500_3.shape)\n",
    "print(val_y_500_3.shape)\n",
    "print(unlab_x_2000_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L: Old turning everything to 14x14\n",
    "#lab_x_mini = np.moveaxis(lab_x_mini,0,-1)[0,1:-1,1:-1,3,:]\n",
    "#lab_y_mini = np.moveaxis(lab_y_mini,0,-1)\n",
    "#unlab_x_mini = np.moveaxis(unlab_x_mini,0,-1)[0,1:-1,1:-1,3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/gan_processed_data/lab_x_mini.npy', lab_x_mini2)\n",
    "np.save('data/gan_processed_data/lab_y_mini.npy', lab_y_mini2)\n",
    "np.save('data/gan_processed_data/val_x_mini.npy', val_x_mini2)\n",
    "np.save('data/gan_processed_data/val_y_mini.npy', val_y_mini2)\n",
    "np.save('data/gan_processed_data/unlab_x_mini.npy', unlab_x_mini2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/gan_processed_data/lab_x_large.npy', lab_x_1000_2)\n",
    "np.save('data/gan_processed_data/lab_y_large.npy', lab_y_1000_2)\n",
    "np.save('data/gan_processed_data/val_x_large.npy', val_x_500_2)\n",
    "np.save('data/gan_processed_data/val_y_large.npy', val_y_500_2)\n",
    "np.save('data/gan_processed_data/unlab_x_large.npy', unlab_x_2000_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/gan_processed_data/lab_x_large2.npy', lab_x_1000_3)\n",
    "np.save('data/gan_processed_data/lab_y_large2.npy', lab_y_1000_3)\n",
    "np.save('data/gan_processed_data/val_x_large2.npy', val_x_500_3)\n",
    "np.save('data/gan_processed_data/val_y_large2.npy', val_y_500_3)\n",
    "np.save('data/gan_processed_data/unlab_x_large2.npy', unlab_x_2000_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('data/gan_processed_data/unlab_x_mini.npy')\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloop = tf.placeholder(tf.float32, shape=[20, 16, 16, 2], name='v_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.repeat(1, 10240), shape=[20, 16, 16, 2], name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = tf.compat.v1.image.central_crop(a, .875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(np.delete(np.delete(np.delete(a, 0, 1), 14, 1), 0, 2), 14, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.repeat(True, 7840), shape=[20, 14, 14, 2], name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.pad(np.tile(1,(14,14)), pad_width = 1, mode = \"constant\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(np.delete(np.delete(np.delete(test, 0, 0), 14, 0), 0, 1), 14, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_x_mini[1,1,:,:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
