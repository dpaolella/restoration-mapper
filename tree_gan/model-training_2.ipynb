{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model-training_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"lQF4P0_cQdFK","colab":{}},"source":["colab = True  \n","install = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6iITVTcjGrm8","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"8a61f14e-e2a9-4577-c60b-3df0846125a0","executionInfo":{"status":"ok","timestamp":1588792290537,"user_tz":240,"elapsed":2242699,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}}},"source":["# Mount Google Drive\n","%%capture\n","if colab:\n","    from google.colab import drive # import drive from google colab\n","\n","    ROOT = \"/content/drive\"     # default location for the drive\n","\n","    drive.mount(ROOT);           # we mount the google drive at /content/drive\n","    \n","    # Set working directory\n","    %cd /content/drive/My Drive/restoration-mapper/tree_gan"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GYrtC8obIAAr","outputId":"d9f6e19e-cd8e-4465-8a51-03866fbc1f4c","executionInfo":{"status":"ok","timestamp":1588792340685,"user_tz":240,"elapsed":2292682,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["if install:\n","    !pip install tensorflow==1.13.1"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.13.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n","\u001b[K     |████████████████████████████████| 92.5MB 63kB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.18.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.34.2)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n","Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n","\u001b[K     |████████████████████████████████| 368kB 44.1MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.28.1)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 44.4MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.3.3)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (46.1.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n","Collecting mock>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n","Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: tensorboard 2.2.1\n","    Uninstalling tensorboard-2.2.1:\n","      Successfully uninstalled tensorboard-2.2.1\n","  Found existing installation: tensorflow 2.2.0rc4\n","    Uninstalling tensorflow-2.2.0rc4:\n","      Successfully uninstalled tensorflow-2.2.0rc4\n","Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RZ1tigQMIehQ","outputId":"f854c9b9-bb6c-4bae-a5bc-7683eed589cd","executionInfo":{"status":"ok","timestamp":1588792352719,"user_tz":240,"elapsed":2304548,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["if install:\n","    !pip install -r requirements.txt "],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting matplotlib==2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/09/530909669df3503caf1558544054fdbe2922d1f88a0a27820ee652c4fc58/matplotlib-2.2.0-cp36-cp36m-manylinux1_x86_64.whl (12.5MB)\n","\u001b[K     |████████████████████████████████| 12.5MB 3.5MB/s \n","\u001b[?25hCollecting nibabel==2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/ec/c4d49fb2aecb80d1c61f89542fdc0ba9686b232bc24f490caeba69d231b6/nibabel-2.1.0.tar.gz (3.6MB)\n","\u001b[K     |████████████████████████████████| 3.6MB 30.9MB/s \n","\u001b[?25hCollecting numpy==1.14.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/7d/348c5d8d44443656e76285aa97b828b6dbd9c10e5b9c0f7f98eff0ff70e4/numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n","\u001b[K     |████████████████████████████████| 12.2MB 10.3MB/s \n","\u001b[?25hCollecting scipy==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n","\u001b[K     |████████████████████████████████| 31.2MB 158kB/s \n","\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.0)\n","Collecting array2gif\n","  Downloading https://files.pythonhosted.org/packages/42/f4/4dd66a8b65738ea8f2739a48d0e1a79e6a77c0cfb4b35a890eb9792dc861/array2gif-1.0.4-py3-none-any.whl\n","Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.0.1)\n","Collecting argparse\n","  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n","Collecting skimage\n","  Downloading https://files.pythonhosted.org/packages/3b/ee/edbfa69ba7b7d9726e634bfbeefd04b5a1764e9e74867ec916113eeaf4a1/skimage-0.0.tar.gz\n","\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TExx0ZMDa2vX","outputId":"9cdf2e5d-6e17-4818-933c-ca0c6cf6adcb","executionInfo":{"status":"ok","timestamp":1588792356715,"user_tz":240,"elapsed":2308402,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["if install:\n","    !pip install array2gif"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting array2gif\n","  Using cached https://files.pythonhosted.org/packages/42/f4/4dd66a8b65738ea8f2739a48d0e1a79e6a77c0cfb4b35a890eb9792dc861/array2gif-1.0.4-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from array2gif) (1.18.3)\n","Installing collected packages: array2gif\n","Successfully installed array2gif-1.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L7Zag0WcIPR_","outputId":"cb16cde0-5674-42ab-9966-6d3d7af11d0c","executionInfo":{"status":"ok","timestamp":1588792358504,"user_tz":240,"elapsed":2309992,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":258}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","import os\n","config=tf.ConfigProto()\n","config.gpu_options.allow_growth=True\n","config.allow_soft_placement=True\n","\n","import matplotlib\n","matplotlib.use('Agg')\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","#to make directories\n","import pathlib\n","\n","import sys\n","\n","from utils import *\n","\n","import argparse\n","\n","# P: allows for easy module reloads\n","from importlib import reload\n","\n","# P\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# P\n","import math"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["1.13.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fXpK-mkyKWCY","colab":{}},"source":["class params:\n","  dataset = 'acdc'\n","  #no of training images\n","  no_of_tr_imgs = 'tr3' # Options include: ['tr1', 'tr3', 'tr5', 'tr15', 'tr40']\n","  #combination of training images\n","  comb_tr_imgs = 'c1' # Options include: ['c1', 'c2', 'c3', 'c4', 'c5']\n","\n","  #learning rate of seg unet\n","  lr_seg = 0.00001\n","  # learning rate of generator\n","  lr_gen = 0.0001\n","  # learning rate of discriminator\n","  lr_disc = 0.0001\n","  # lat dim of z sample\n","  z_lat_dim = 100\n","\n","  # ra_en : 0 - disabled, 1 - enabled\n","  ra_en = 0\n","  # select gan type\n","  gan_type = 'lsgan' # Options include: ['lsgan', 'gan', 'wgan-gp','ngan']\n","  # beta value of Adam optimizer\n","  beta_val = 0.9\n","  # to enable the representation of labels with 1 hot encoding\n","  en_1hot = 1\n","\n","  # lamda factors\n","  # for segmenation loss term (lamda_dsc)\n","  lamda_dsc = 1\n","  # adversarial loss term (lamda_adv)\n","  lamda_adv = 1\n","  ### deformation field cGAN specific\n","  # for negative L1 loss on spatial transformation (per-pixel flow field/deformation field) term (lamda_l1_g)\n","  lamda_l1_g = 0.001\n","\n","  ### Intensity field cGAN specific\n","  # for negative L1 loss on transformation (additive intensity field) term (lamda_l1_i)\n","  lamda_l1_i = 0.001\n","\n","  #version of run\n","  ver = 0\n","\n","  #data aug - 0 - disabled, 1 - enabled\n","  data_aug_seg = 1 # Options include: [0,1]\n","\n","  # segmentation loss to optimize\n","  # 0 for weighted cross entropy, 1 for dice score loss\n","  dsc_loss = 0 # Options include: [0,1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8cqzpqbwnvoZ"},"source":["## Deformation Network"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eB16wUjnQdHp","colab":{}},"source":["ra_en_val=params.ra_en\n","if(params.ra_en==1):\n","    params.ra_en=True\n","else:\n","    params.ra_en=False\n","\n","import experiment_init.init_acdc as cfg\n","import experiment_init.data_cfg_acdc as data_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qMJmPLx-QdHz","outputId":"7b33ce7f-757f-4d73-aeb5-4b557aa593c9","executionInfo":{"status":"ok","timestamp":1588792361107,"user_tz":240,"elapsed":2312081,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["######################################\n","# class loaders\n","# ####################################\n","#  load dataloader object\n","from dataloaders import dataloaderObj\n","dt = dataloaderObj(cfg)\n","\n","\n","#print('set acdc orig img dataloader handle')\n","orig_img_dt=dt.load_acdc_imgs\n","\n","#  load model object\n","import models \n","model = models.modelObj(cfg)\n","#  load f1_utils object\n","from f1_utils import f1_utilsObj\n","f1_util = f1_utilsObj(cfg,dt)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["loss init\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KzKvy6cfQdIb","outputId":"472408f6-7e6a-4d25-c2fa-95552bc770e6","executionInfo":{"status":"ok","timestamp":1588792361108,"user_tz":240,"elapsed":2311924,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["######################################\n","#define save_dir for the model\n","save_dir = 'models/'\n","if not os.path.exists(save_dir[:-1]):\n","    os.makedirs(save_dir[:-1])\n","print('save_dir: ',save_dir)\n","######################################"],"execution_count":10,"outputs":[{"output_type":"stream","text":["save_dir:  models/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OLwxfnLoQdIk","outputId":"5e73724e-4379-4f9f-d2e4-b83af8c089bc","executionInfo":{"status":"ok","timestamp":1588792363422,"user_tz":240,"elapsed":2314100,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["######################################\n","# load train and val images\n","#train_list = data_list.train_data(params.no_of_tr_imgs,params.comb_tr_imgs)\n","#load train data cropped images directly\n","print('loading train imgs')\n","train_imgs,train_labels = dt.load_imgs(dataset= 'lab')\n","# P: switching dimensions before feeding into minibatch function\n","train_imgs = np.moveaxis(train_imgs,3,0)\n","train_labels = np.moveaxis(train_labels,2,0)\n","print(train_imgs.shape)\n","print(train_labels.shape)\n","#D: we might have to adjust this if our mini sample is too small compared to batch size\n","# if(params.no_of_tr_imgs=='tr1'):\n","#     train_imgs_copy=np.copy(train_imgs)\n","#     train_labels_copy=np.copy(train_labels)\n","#     while(train_imgs.shape[2]<cfg.batch_size):\n","#         train_imgs=np.concatenate((train_imgs,train_imgs_copy),axis=2)\n","#         train_labels=np.concatenate((train_labels,train_labels_copy),axis=2)\n","#     del train_imgs_copy,train_labels_copy\n","\n","#load both val data and its cropped images\n","print('loading val imgs')\n","val_imgs,val_labels = dt.load_imgs(dataset= 'val')\n","val_n = val_imgs.shape[3]\n","#L: change to (20,16,16,16)\n","val_imgs = np.moveaxis(val_imgs,3,0)\n","val_labels = np.moveaxis(val_labels,2,0)\n","print('val image dims after reshape')\n","print(val_imgs.shape)\n","print(val_labels.shape)\n","\n","# # load unlabeled images\n","#unl_list = data_list.unlabeled_data()\n","print('loading unlabeled imgs')\n","unlabeled_imgs=dt.load_imgs(dataset= 'unlab')\n","# P: switching dimensions before feeding into minibatch function\n","unlabeled_imgs = np.moveaxis(unlabeled_imgs,3,0)\n","print('unlabeled_imgs',unlabeled_imgs.shape)\n","\n","\n","# get test list\n","#print('get test imgs list')\n","#D: will have to add this back once test data created\n","#test_list = data_list.test_data()\n","#D: will have to figure out struct name in our case - it is used for computing the \n","# model performance, segmentation mask etc. in f1_util.pred_segs_acdc_test_subjs\n","#struct_name=cfg.struct_name\n","val_step_update=cfg.val_step_update\n","######################################"],"execution_count":11,"outputs":[{"output_type":"stream","text":["loading train imgs\n","image shape:  (16, 16, 16, 1000)\n","mask shape:  (14, 14, 1000)\n","(1000, 16, 16, 16)\n","(1000, 14, 14)\n","loading val imgs\n","image shape:  (16, 16, 16, 500)\n","mask shape:  (14, 14, 500)\n","val image dims after reshape\n","(500, 16, 16, 16)\n","(500, 14, 14)\n","loading unlabeled imgs\n","image shape:  (16, 16, 16, 2000)\n","unlabeled_imgs (2000, 16, 16, 16)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wIQPyXweQdIo","colab":{}},"source":["######################################\n","\n","def get_samples(labeled_imgs,unlabeled_imgs):\n","    # sample z vectors from Gaussian Distribution\n","    z_samples = np.random.normal(loc=0.0, scale=1.0, size=(cfg.batch_size, params.z_lat_dim)).astype(np.float32)\n","\n","    # sample Unlabeled data shuffled batch\n","    unld_img_batch=shuffle_minibatch([unlabeled_imgs],batch_size=int(cfg.batch_size),num_channels=cfg.num_channels,labels_present=0,axis=2)\n","\n","    # sample Labelled data shuffled batch\n","    ld_img_batch=shuffle_minibatch([labeled_imgs],batch_size=int(cfg.batch_size),num_channels=cfg.num_channels,labels_present=0,axis=2)\n","\n","    return z_samples,ld_img_batch,unld_img_batch\n","\n","def plt_func(sess,ae,save_dir,z_samples,ld_img_batch,unld_img_batch,index=0):\n","    # plot deformed images for an fixed input image and different per-pixel flow vectors generated from sampled z values\n","    ld_img_tmp=np.zeros_like(ld_img_batch)\n","    # select one 2D image from the batch and apply different z's sampled over this selected image\n","    for i in range(0,20):\n","        ld_img_tmp[i,:,:,0]=ld_img_batch[index,:,:,0]\n","\n","    flow_vec,y_geo_deformed,z_cost=sess.run([ae['flow_vec'],ae['y_trans'],ae['z_cost']], feed_dict={ae['x_l']: ld_img_tmp, ae['z']:z_samples,\\\n","                          ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: False})\n","\n","    f1_util.plot_deformed_imgs(ld_img_tmp,y_geo_deformed,flow_vec,save_dir,index=index)\n","\n","    # Plot gif of all the deformed images generated for the fixed input image\n","    f1_util.write_gif_func(ip_img=y_geo_deformed, imsize=(cfg.img_size_x,cfg.img_size_y),save_dir=save_dir,index=index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8iq2HxGrQdJL","colab":{}},"source":["######################################\n","# Define checkpoint file to save CNN architecture and learnt hyperparameters\n","checkpoint_filename='unet_'+str(params.dataset)\n","logs_path = str(save_dir)+'tensorflow_logs/'\n","best_model_dir=str(save_dir)+'best_model/'\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J07ehm8y9z3n","colab_type":"code","colab":{}},"source":["#L: set class weights so adapts to training data if it changes\n","ntrlabs = np.sum(np.unique(train_labels, return_counts = True)[1])\n","propnotree = (ntrlabs - np.unique(train_labels, return_counts = True)[1][1])/ntrlabs\n","proptree = 1-propnotree\n","classweight = tf.constant([[proptree, propnotree]],name='class_weights')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cSUvxMguQdJQ","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QamwiCTOQdJV","outputId":"d8f8d669-d953-4086-966a-9b7bab828a86","executionInfo":{"status":"ok","timestamp":1588792376474,"user_tz":240,"elapsed":2326407,"user":{"displayName":"Daniel Csonth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgSdHLnwnqx6y0K49d41peLXVaonB_cLI8fznPw7bc=s64","userId":"10232999259256545758"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["######################################\n","# Define deformation field generator model graph\n","ae = model.spatial_generator_cgan_unet(learn_rate_gen=params.lr_gen,learn_rate_disc=params.lr_disc,\\\n","                        beta1_val=params.beta_val,gan_type=params.gan_type,ra_en=params.ra_en,\\\n","                        learn_rate_seg=params.lr_seg,dsc_loss=params.dsc_loss,en_1hot=params.en_1hot,\\\n","                        lamda_dsc=params.lamda_dsc,lamda_adv=params.lamda_adv,lamda_l1_g=params.lamda_l1_g,\n","                        class_weights = classweight, num_channels = cfg.num_channels)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","Inputs\n","gen_c1_weights: (100, 256)\n","gen_c1_biases: (256,)\n","fcn_c1_weights: (256, 128)\n","fcn_c1_biases: (128,)\n","fcn_c2_weights: (128, 128)\n","fcn_c2_biases: (128,)\n","fcn_c3_weights: (128, 1)\n","fcn_c3_biases: (1,)\n","z: (20, 100)\n","x_l: (20, 16, 16, 16)\n","x: (?, 16, 16, 16)\n","x_unl: (?, 16, 16, 16)\n","Generator\n","gen_fcn_c1: (20, 256)\n","gen_fcn_relu_c1: (20, 256)\n","gen_fcn_reshaped: (20, 2, 2, 64)\n","gen_up5: (20, 4, 4, 64)\n","gen_c5: (20, 4, 4, 64)\n","gen_up4: (20, 8, 8, 64)\n","gen_c4: (20, 8, 8, 32)\n","gen_up3: (20, 16, 16, 32)\n","gen_c3: (20, 16, 16, 16)\n","conv_1a: (20, 16, 16, 32)\n","conv_1b: (20, 16, 16, 64)\n","gen_cat: (20, 16, 16, 80)\n","conv_1c: (20, 16, 16, 32)\n","conv_1d: (20, 16, 16, 16)\n","conv_1e: (20, 16, 16, 2)\n","\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n","y_trans: (20, 16, 16, 16)\n","Discriminator\n","cat_disc_c1: (?, 16, 16, 16)\n","cat_disc_c1: (?, 16, 16, 16)\n","disc_c1: (?, 8, 8, 32)\n","disc_c2: (?, 4, 4, 32)\n","disc_c3: (?, 2, 2, 64)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","flat_conv: (?, 256)\n","z_fcn_relu_c1: (?, 128)\n","z_fcn_relu_c2: (?, 128)\n","z_class: (?, 1)\n","UNet Downsampling\n","enc_c1_a: (?, 16, 16, 64)\n","enc_c1_b: (?, 16, 16, 64)\n","enc_c1_pool: (?, 8, 8, 64)\n","enc_c2_a: (?, 8, 8, 128)\n","enc_c2_b: (?, 8, 8, 128)\n","enc_c2_pool: (?, 4, 4, 128)\n","enc_c3_a: (?, 4, 4, 256)\n","enc_c3_b: (?, 4, 4, 256)\n","UNet Upsampling\n","dec_up5: (?, 8, 8, 256)\n","dec_dc5: (?, 8, 8, 128)\n","dec_cat_c5: (?, 8, 8, 256)\n","dec_c4_a: (?, 8, 8, 128)\n","dec_c4_b: (?, 8, 8, 128)\n","dec_up4: (?, 16, 16, 128)\n","dec_dc4: (?, 16, 16, 64)\n","dec_cat_c4: (?, 16, 16, 128)\n","dec_c1_a: (?, 16, 16, 64)\n","seg_c1_a: (?, 16, 16, 32)\n","seg_c1_b: (?, 16, 16, 16)\n","seg_c1_c: (?, 16, 16, 16)\n","seg_fin_layer: (?, 14, 14, 2)\n","y_pred: (?, 14, 14, 2)\n","y_pred_cls: (?, 14, 14)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CPU6xYgvQdJm","colab":{}},"source":["######################################\n","#  training parameters\n","start_epoch=0\n","#L: make this 10 for quick training to test\n","n_epochs = 1500\n","disp_step=400\n","print_step=2000\n","# no of iterations to train just the segmentation network using the labeled data without any cGAN generated data\n","seg_tr_limit=300\n","f1_val_prev=0.1\n","threshold_f1=0.000001\n","pathlib.Path(best_model_dir).mkdir(parents=True, exist_ok=True)\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5Ja6WdYEQdJ0","colab":{}},"source":["######################################\n","# define graph to compute deformed image given an per-pixel flow vector and input image\n","#L: change this to new function, deform net clip\n","df_ae= model.deform_netclip()\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T64RuzRgQdJ_","colab":{}},"source":["######################################\n","#writer for train summary\n","train_writer = tf.summary.FileWriter(logs_path)\n","#writer for dice score and val summary\n","dsc_writer = tf.summary.FileWriter(logs_path)\n","val_sum_writer = tf.summary.FileWriter(logs_path)\n","######################################\n","\n","######################################\n","# create a session and initialize variable to use the graph\n","sess = tf.Session(config=config)\n","sess.run(tf.global_variables_initializer())\n","# Save training data\n","saver = tf.train.Saver(max_to_keep=2)\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qfnJ_KT5nul0","scrolled":false,"outputId":"1a670c8d-21dc-4bd4-e77e-8ef9a7b59d7a","colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# Run for n_epochs\n","\n","# arrays to store metrics from every epoch\n","seg_cost_epoch = np.array([])\n","seg_acc_epoch = np.array([])\n","g_loss_epoch = np.array([])\n","d_loss_epoch = np.array([])\n","val_acc_epoch = np.array([])\n","val_f1_epoch = np.array([])\n","\n","\n","for epoch_i in range(start_epoch,n_epochs):\n","    \n","\n","    \n","    # sample Unlabeled shuffled batch\n","    unld_img_batches=shuffle_minibatch([unlabeled_imgs],batch_size=int(cfg.batch_size),labels_present=0)\n","    # P: already did this above\n","#     unld_img_batch = np.moveaxis(unld_img_batch,3,0).reshape(20,16,16,16)\n","    \n","    # sample Labelled shuffled batch\n","    ld_img_batches,ld_label_batches=shuffle_minibatch([train_imgs,train_labels],batch_size=cfg.batch_size)\n","    # P: already did this above\n","#     ld_img_batch = np.moveaxis(ld_img_batch,3,0)\n","#     ld_label_batch = np.moveaxis(ld_label_batch,2,0)\n","\n","    minibatches = math.floor(len(train_imgs)/cfg.batch_size+0.5)\n","    for b in range(minibatches):\n","        \n","        unld_img_batch = unld_img_batches[b]\n","        ld_img_batch = ld_img_batches[b]\n","        ld_label_batch = ld_label_batches[b]\n","\n","        # sample z's from Gaussian Distribution\n","        z_samples = np.random.normal(loc=0.0, scale=1.0, size=(cfg.batch_size, params.z_lat_dim)).astype(np.float32)\n","\n","        if(cfg.aug_en==1):\n","            # Apply affine transformations\n","            ld_img_batch,ld_label_batch=augmentation_function([ld_img_batch,ld_label_batch],dt)\n","            unld_img_batch=augmentation_function([unld_img_batch],dt,labels_present=0)\n","\n","        ld_img_batch_tmp=np.copy(ld_img_batch)\n","        # Compute 1 hot encoding of the segmentation mask labels\n","        ld_label_batch_1hot = sess.run(df_ae['y_tmp_1hot'],feed_dict={df_ae['y_tmp']:ld_label_batch})    \n","\n","        if(epoch_i>=seg_tr_limit):\n","            # sample the batch of images and apply deformation field generated by the Generator network on these which are used for the remaining 9500 epochs\n","            # Batch comprosed of both deformed image,label pairs and original affine transformed image, label pairs\n","            ld_label_batch_tmp=np.copy(ld_label_batch)\n","            ###########################\n","            ## use Deformation field cGAN to generate additional augmented image,label pairs from labeled samples\n","            flow_vec,ld_img_batch=sess.run([ae['flow_vec'],ae['y_trans']],\\\n","                                        feed_dict={ae['x_l']: ld_img_batch_tmp, ae['z']:z_samples, ae['train_phase']: False})\n","\n","            ld_label_batch=sess.run([df_ae['deform_y_1hot']],feed_dict={df_ae['y_tmp']:ld_label_batch,df_ae['flow_v']:flow_vec})\n","            ld_label_batch=ld_label_batch[0]\n","\n","            ###########################\n","            #shuffle the quantity/number of images chosen from deformation cGAN augmented images and rest are original images with conventional affine transformations\n","            no_orig=np.random.randint(5, high=15)\n","            ld_img_batch[0:no_orig] = ld_img_batch_tmp[0:no_orig]\n","            if(params.en_1hot==1):\n","                ld_label_batch[0:no_orig] = ld_label_batch_1hot[0:no_orig]\n","            #D: this else statement here baffles me - we will have to look into it at some later point - why would you turn images back from 1hot to\n","            # regular 1d classes with that argmax? When the ld_label batch assumes 1hot input? and especially in a scenario called 1hot == 0\n","            else:\n","                ld_label_batch = np.argmax(ld_label_batch,axis=3)\n","                ld_label_batch[0:no_orig] = ld_label_batch_tmp[0:no_orig]\n","\n","            #Pick equal number of images from each category\n","            # ld_img_batch[0:10]=ld_img_batch_tmp[0:10]\n","            # ld_label_batch[0:10]=ld_label_batch_1hot[0:10]\n","\n","        elif(epoch_i<seg_tr_limit):\n","            # sample only labeled data batches to optimize only Segmentation Network for initial 500 epochs\n","            ld_img_batch=ld_img_batch\n","            unld_img_batch=unld_img_batch\n","            ld_label_batch=ld_label_batch_1hot\n","\n","        if(epoch_i<seg_tr_limit):\n","            #Optimize only Segmentation Network for initial 500 epochs\n","            train_summary, seg_cost, _ =sess.run([ae['seg_summary'], ae['seg_cost'], ae['optimizer_unet_seg']], feed_dict={ae['x']: ld_img_batch, ae['y_l']: ld_label_batch,\\\n","                                       ae['select_mask']: False, ae['train_phase']: True})\n","\n","            if(b==minibatches-1):\n","                pred_sf_mask = f1_util.calc_pred_sf_mask_full(sess, ae, ld_img_batch)\n","                acc = np.mean(f1_util.calc_accuracy(np.argmax(pred_sf_mask, -1),np.argmax(ld_label_batch, -1)))\n","                seg_cost_epoch = np.append(seg_cost_epoch,seg_cost)\n","                seg_acc_epoch = np.append(seg_acc_epoch,acc)\n","                print(\"Epoch: \", epoch_i, \"Seg loss: \", np.mean(seg_cost_epoch), \"Seg acc: \", np.mean(seg_acc_epoch))\n","\n","\n","         #Optimize Generator (G), Discriminator (D) and Segmentation (S) networks for the remaining 9500 epochs       \n","        if(epoch_i>seg_tr_limit):   \n","\n","            # update both Generator and Segmentation Net parameters in the framework using total loss value\n","            train_summary, z_cost, cost_a1_seg, seg_cost, _ =sess.run([ae['train_summary'],\\\n","                                                                       ae['z_cost'],  ae['cost_a1_seg'], ae['seg_cost'], ae['optimizer_l2_both_gen_unet']],\\\n","                                                                       feed_dict={ae['x']: ld_img_batch,ae['x_l']: ld_img_batch,ae['y_l']: ld_label_batch,\\\n","                                       ae['z']:z_samples, ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: True})\n","            # update Discriminator Net (D) parameters in the setup using only discriminator loss\n","            train_summary,_ =sess.run([ae['train_summary'],ae['optimizer_disc']], feed_dict={ae['x']: ld_img_batch, ae['x_l']: ld_img_batch, ae['z']:z_samples,\\\n","                                  ae['y_l']: ld_label_batch,ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: True})\n","\n","            if(b==minibatches-1):\n","                pred_sf_mask = f1_util.calc_pred_sf_mask_full(sess, ae, ld_img_batch)\n","                acc = np.mean(f1_util.calc_accuracy(np.argmax(pred_sf_mask, -1),np.argmax(ld_label_batch, -1)))\n","                seg_cost_epoch = np.append(seg_cost_epoch,seg_cost)\n","                seg_acc_epoch = np.append(seg_acc_epoch,acc)\n","                g_loss_epoch = np.append(g_loss_epoch, cost_a1_seg)\n","                d_loss_epoch = np.append(d_loss_epoch, z_cost)\n","                print(\"Epoch: \", epoch_i, \"Seg loss: \", np.mean(seg_cost_epoch), \"Seg acc: \", np.mean(seg_acc_epoch), \"Disc loss: \", z_cost, \"Gen loss: \", cost_a1_seg,)\n","\n","\n","    if(epoch_i%val_step_update==0):\n","        train_writer.add_summary(train_summary, epoch_i)\n","        train_writer.flush()\n","\n","    if(epoch_i%val_step_update==0):\n","        ##Save the model with best DSC for Validation Image\n","        f1_arr=[]\n","\n","        # Compute segmentation mask and dice_score validation data\n","        val_pred_sf_mask = f1_util.calc_pred_sf_mask_full(sess, ae, val_imgs)\n","        f1_val = np.mean(f1_util.calc_f1_score(np.argmax(val_pred_sf_mask, -1),val_labels))\n","        val_f1_epoch = np.append(val_f1_epoch, f1_val)\n","        val_acc = np.mean(f1_util.calc_accuracy(np.argmax(val_pred_sf_mask, -1),val_labels))\n","        val_acc_epoch = np.append(val_acc_epoch, val_acc)\n","\n","        print(\"Epoch: \", epoch_i, \"F1 (val): \", f1_val, \"Acc (val): \", val_acc)\n","        \n","\n","\n","        # if (f1_val-f1_val_prev>threshold_f1 and epoch_i!=start_epoch):\n","        #     print(\"prev f1_val; present_f1_val\", f1_val_prev, f1_val)\n","        #     f1_val_prev = f1_val\n","        #     # to save the best model with maximum dice score over the entire n_epochs\n","        #     print(\"best model saved at epoch no. \", epoch_i)\n","        #     mp_best = str(best_model_dir) + str(checkpoint_filename) + '_best_model_epoch_' + str(epoch_i) + '.ckpt'\n","        #     saver.save(sess, mp_best)\n","\n","        # # calc. and save validation image dice summary\n","        # dsc_summary_msg = sess.run(ae['val_f1_summary'], feed_dict={ae['f1']:f1_val})\n","        # val_sum_writer.add_summary(dsc_summary_msg, epoch_i)\n","        # val_sum_writer.flush()\n","\n","    if ((epoch_i==n_epochs-1) and (epoch_i != start_epoch)):\n","        # model saved at last epoch\n","        mp = str(save_dir) + str(checkpoint_filename) + '_epochs_' + str(epoch_i) + '.ckpt'\n","        saver.save(sess, mp)\n","        try:\n","            mp_best\n","        except NameError:\n","            mp_best=mp\n","\n","sess.close()\n","######################################\n","# restore best model and predict segmentations on test subjects\n","saver_new = tf.train.Saver()\n","sess_new = tf.Session(config=config)\n","saver_new.restore(sess_new, mp_best)\n","print(\"best model chkpt name\",mp_best)\n","print(\"Model restored\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch:  0 Seg loss:  0.5505106449127197 Seg acc:  0.875\n","Epoch:  0 F1 (val):  0.406940711706098 Acc (val):  0.6451326530612245\n","Epoch:  1 Seg loss:  0.5158576965332031 Seg acc:  0.900765306122449\n","Epoch:  2 Seg loss:  0.4994940161705017 Seg acc:  0.8732993197278912\n","Epoch:  3 Seg loss:  0.49485625326633453 Seg acc:  0.8598214285714286\n","Epoch:  4 Seg loss:  0.49340368509292604 Seg acc:  0.8504081632653062\n","Epoch:  5 Seg loss:  0.4956253568331401 Seg acc:  0.8403486394557823\n","Epoch:  6 Seg loss:  0.4977575285094125 Seg acc:  0.8302842565597668\n","Epoch:  7 Seg loss:  0.48994044959545135 Seg acc:  0.8283482142857144\n","Epoch:  8 Seg loss:  0.47770214743084377 Seg acc:  0.8323412698412699\n","Epoch:  9 Seg loss:  0.4767565935850143 Seg acc:  0.8292857142857143\n","Epoch:  10 Seg loss:  0.47154048627073114 Seg acc:  0.8304267161410018\n","Epoch:  10 F1 (val):  0.6884069942856096 Acc (val):  0.7441734693877551\n","Epoch:  11 Seg loss:  0.4620581964651744 Seg acc:  0.8357993197278911\n","Epoch:  12 Seg loss:  0.46579010211504424 Seg acc:  0.8317503924646782\n","Epoch:  13 Seg loss:  0.4549835664885385 Seg acc:  0.8358053935860058\n","Epoch:  14 Seg loss:  0.45239018201828 Seg acc:  0.836360544217687\n","Epoch:  15 Seg loss:  0.44737835228443146 Seg acc:  0.8385522959183673\n","Epoch:  16 Seg loss:  0.44438864378368154 Seg acc:  0.8378451380552221\n","Epoch:  17 Seg loss:  0.4382881240712272 Seg acc:  0.8411281179138322\n","Epoch:  18 Seg loss:  0.43205914371892024 Seg acc:  0.8433673469387756\n","Epoch:  19 Seg loss:  0.4302052691578865 Seg acc:  0.8433928571428572\n","Epoch:  20 Seg loss:  0.42604407526197885 Seg acc:  0.8448129251700681\n","Epoch:  20 F1 (val):  0.7219697170532968 Acc (val):  0.7692244897959184\n","Epoch:  21 Seg loss:  0.4232624132524837 Seg acc:  0.8458603896103898\n","Epoch:  22 Seg loss:  0.41809371761653735 Seg acc:  0.8477817213842059\n","Epoch:  23 Seg loss:  0.41466820364197093 Seg acc:  0.8485969387755102\n","Epoch:  24 Seg loss:  0.4133098292350769 Seg acc:  0.8486938775510204\n","Epoch:  25 Seg loss:  0.41391906486107755 Seg acc:  0.8477335164835165\n","Epoch:  26 Seg loss:  0.4093387987878587 Seg acc:  0.8500094482237339\n","Epoch:  27 Seg loss:  0.4059056852545057 Seg acc:  0.8509566326530613\n","Epoch:  28 Seg loss:  0.40520662377620564 Seg acc:  0.8505541871921183\n","Epoch:  29 Seg loss:  0.4038412610689799 Seg acc:  0.851232993197279\n","Epoch:  30 Seg loss:  0.40090257410080204 Seg acc:  0.8521313364055301\n","Epoch:  30 F1 (val):  0.724503590368843 Acc (val):  0.7748979591836734\n","Epoch:  31 Seg loss:  0.39858481008559465 Seg acc:  0.8525191326530612\n","Epoch:  32 Seg loss:  0.39533813433213666 Seg acc:  0.8537414965986394\n","Epoch:  33 Seg loss:  0.39441046819967385 Seg acc:  0.8538790516206483\n","Epoch:  34 Seg loss:  0.392951637506485 Seg acc:  0.8545262390670554\n","Epoch:  35 Seg loss:  0.39228185680177474 Seg acc:  0.8545422335600907\n","Epoch:  36 Seg loss:  0.39288292543308156 Seg acc:  0.8534404302261446\n","Epoch:  37 Seg loss:  0.3915911481568688 Seg acc:  0.8538802363050484\n","Epoch:  38 Seg loss:  0.38885236703432524 Seg acc:  0.8549973835688122\n","Epoch:  39 Seg loss:  0.3876745529472828 Seg acc:  0.8550956632653062\n","Epoch:  40 Seg loss:  0.3852840153182425 Seg acc:  0.8562966650074665\n","Epoch:  40 F1 (val):  0.7168363060900994 Acc (val):  0.7707755102040816\n","Epoch:  41 Seg loss:  0.3832622298172542 Seg acc:  0.8573007774538387\n","Epoch:  42 Seg loss:  0.38229779587235563 Seg acc:  0.8574750830564785\n","Epoch:  43 Seg loss:  0.37878596952015703 Seg acc:  0.8590503246753247\n","Epoch:  44 Seg loss:  0.37665258083078595 Seg acc:  0.8600566893424036\n","Epoch:  45 Seg loss:  0.37583967946145846 Seg acc:  0.8605090949423249\n","Epoch:  46 Seg loss:  0.37538474449451936 Seg acc:  0.8610290924880593\n","Epoch:  47 Seg loss:  0.37502105937649804 Seg acc:  0.8615965136054422\n","Epoch:  48 Seg loss:  0.371502691081592 Seg acc:  0.8634735526863807\n","Epoch:  49 Seg loss:  0.3705621913075447 Seg acc:  0.8638469387755103\n","Epoch:  50 Seg loss:  0.368946871044589 Seg acc:  0.8648659463785515\n","Epoch:  50 F1 (val):  0.7369470351838388 Acc (val):  0.7762653061224489\n","Epoch:  51 Seg loss:  0.3665517204656051 Seg acc:  0.8664050235478806\n","Epoch:  52 Seg loss:  0.3665306087372438 Seg acc:  0.866572006160955\n","Epoch:  53 Seg loss:  0.3654563788462568 Seg acc:  0.8673705593348451\n","Epoch:  54 Seg loss:  0.36304103108969604 Seg acc:  0.868538961038961\n","Epoch:  55 Seg loss:  0.3609545124428613 Seg acc:  0.8696428571428572\n","Epoch:  56 Seg loss:  0.3604494385551988 Seg acc:  0.8695936269244541\n","Epoch:  57 Seg loss:  0.35907193042080976 Seg acc:  0.8701178747361014\n","Epoch:  58 Seg loss:  0.3568071690151247 Seg acc:  0.8712296783120027\n","Epoch:  59 Seg loss:  0.3545995973050594 Seg acc:  0.8721598639455782\n","Epoch:  60 Seg loss:  0.35350661077460305 Seg acc:  0.8728128136500501\n","Epoch:  60 F1 (val):  0.7372971410254153 Acc (val):  0.7770204081632653\n","Epoch:  61 Seg loss:  0.3524437433769626 Seg acc:  0.8733212639894667\n","Epoch:  62 Seg loss:  0.3503126324642272 Seg acc:  0.8742549400712666\n","Epoch:  63 Seg loss:  0.35101562389172614 Seg acc:  0.8744021045918368\n","Epoch:  64 Seg loss:  0.34904665442613453 Seg acc:  0.8752864992150706\n","Epoch:  65 Seg loss:  0.34740813053918607 Seg acc:  0.8756570810142238\n","Epoch:  66 Seg loss:  0.34702558326187416 Seg acc:  0.8757957660676211\n","Epoch:  67 Seg loss:  0.3444019880364923 Seg acc:  0.8770070528211285\n","Epoch:  68 Seg loss:  0.3428504784470019 Seg acc:  0.8775769003253475\n","Epoch:  69 Seg loss:  0.34110336537872044 Seg acc:  0.8782325072886298\n","Epoch:  70 Seg loss:  0.33902545714042553 Seg acc:  0.8791714573153204\n","Epoch:  70 F1 (val):  0.7533794762700609 Acc (val):  0.7821632653061225\n","Epoch:  71 Seg loss:  0.33780601082576645 Seg acc:  0.8796272675736962\n","Epoch:  72 Seg loss:  0.3359251083576516 Seg acc:  0.8804654738607772\n","Epoch:  73 Seg loss:  0.33509708457701914 Seg acc:  0.8806329288472146\n","Epoch:  74 Seg loss:  0.3333361272017161 Seg acc:  0.8816802721088434\n","Epoch:  75 Seg loss:  0.3315149418225414 Seg acc:  0.8824919441460793\n","Epoch:  76 Seg loss:  0.3298821021597107 Seg acc:  0.8832030214683275\n","Epoch:  77 Seg loss:  0.32886227602377915 Seg acc:  0.8835230245944532\n","Epoch:  78 Seg loss:  0.32734318794328954 Seg acc:  0.884190131748902\n","Epoch:  79 Seg loss:  0.32597546484321355 Seg acc:  0.8846843112244898\n","Epoch:  80 Seg loss:  0.32621326950597174 Seg acc:  0.8847064751826655\n","Epoch:  80 F1 (val):  0.7297670689281412 Acc (val):  0.7708673469387755\n","Epoch:  81 Seg loss:  0.3248326971036632 Seg acc:  0.885306744649079\n","Epoch:  82 Seg loss:  0.3235361674822957 Seg acc:  0.8857726825670027\n","Epoch:  83 Seg loss:  0.3220840784765425 Seg acc:  0.8863155976676383\n","Epoch:  84 Seg loss:  0.3207593905575135 Seg acc:  0.8868487394957982\n","Epoch:  85 Seg loss:  0.31962483948053316 Seg acc:  0.8871915045087801\n","Epoch:  86 Seg loss:  0.3200412440574032 Seg acc:  0.8873709828759089\n","Epoch:  87 Seg loss:  0.31979545849290764 Seg acc:  0.8874884044526901\n","Epoch:  88 Seg loss:  0.318830379274454 Seg acc:  0.8877407704654894\n","Epoch:  89 Seg loss:  0.3182069804933336 Seg acc:  0.8879846938775509\n","Epoch:  90 Seg loss:  0.3173518575482316 Seg acc:  0.8883662256111234\n","Epoch:  90 F1 (val):  0.7495590361493734 Acc (val):  0.7788673469387755\n","Epoch:  91 Seg loss:  0.31619002475686697 Seg acc:  0.8888087843833185\n","Epoch:  92 Seg loss:  0.3143269201760651 Seg acc:  0.8895792187842878\n","Epoch:  93 Seg loss:  0.3134286565349457 Seg acc:  0.8899261832392531\n","Epoch:  94 Seg loss:  0.3127123846819526 Seg acc:  0.8901315789473684\n","Epoch:  95 Seg loss:  0.31133653052772087 Seg acc:  0.8906063988095237\n","Epoch:  96 Seg loss:  0.3100496697978875 Seg acc:  0.8909767515253524\n","Epoch:  97 Seg loss:  0.309047274899726 Seg acc:  0.8913317367763431\n","Epoch:  98 Seg loss:  0.30786134618701355 Seg acc:  0.8918238507524221\n","Epoch:  99 Seg loss:  0.30699934929609296 Seg acc:  0.8921734693877551\n","Epoch:  100 Seg loss:  0.3063560519183036 Seg acc:  0.8924631238634068\n","Epoch:  100 F1 (val):  0.7436630220652323 Acc (val):  0.7768673469387755\n","Epoch:  101 Seg loss:  0.30511484412001627 Seg acc:  0.8929396758703482\n","Epoch:  102 Seg loss:  0.30453069276601363 Seg acc:  0.8931419655240738\n","Epoch:  103 Seg loss:  0.3038324282432978 Seg acc:  0.8933796114599685\n","Epoch:  104 Seg loss:  0.30298264296281907 Seg acc:  0.8937414965986393\n","Epoch:  105 Seg loss:  0.3021221090600176 Seg acc:  0.8941807855217556\n","Epoch:  106 Seg loss:  0.30111267814569387 Seg acc:  0.8945474918939537\n","Epoch:  107 Seg loss:  0.3002876092990239 Seg acc:  0.8949806311413453\n","Epoch:  108 Seg loss:  0.29955145826033497 Seg acc:  0.8952092304811832\n","Epoch:  109 Seg loss:  0.29867814210328186 Seg acc:  0.8954220779220777\n","Epoch:  110 Seg loss:  0.2981950600404997 Seg acc:  0.8958930869645153\n","Epoch:  110 F1 (val):  0.7321596761392859 Acc (val):  0.7720204081632653\n","Epoch:  111 Seg loss:  0.29724158932055744 Seg acc:  0.8961780247813411\n","Epoch:  112 Seg loss:  0.29701632788750976 Seg acc:  0.8962637709951237\n","Epoch:  113 Seg loss:  0.2960916744512424 Seg acc:  0.8965180809165773\n","Epoch:  114 Seg loss:  0.29566617465537526 Seg acc:  0.8966548358473825\n","Epoch:  115 Seg loss:  0.2946438130376668 Seg acc:  0.8970685256861365\n","Epoch:  116 Seg loss:  0.29483073300276047 Seg acc:  0.8970717774289203\n","Epoch:  117 Seg loss:  0.2937031786068011 Seg acc:  0.8974641127637496\n","Epoch:  118 Seg loss:  0.2936918701694793 Seg acc:  0.897528297033099\n","Epoch:  119 Seg loss:  0.29252030501763027 Seg acc:  0.8980590986394558\n","Epoch:  120 Seg loss:  0.2918376725567274 Seg acc:  0.898467279473773\n","Epoch:  120 F1 (val):  0.7426400484220821 Acc (val):  0.7756224489795919\n","Epoch:  121 Seg loss:  0.2905312680074426 Seg acc:  0.898921043827367\n","Epoch:  122 Seg loss:  0.2896913967239178 Seg acc:  0.8991558818649412\n","Epoch:  123 Seg loss:  0.28904308979549714 Seg acc:  0.8993951612903226\n","Epoch:  124 Seg loss:  0.2881265969276428 Seg acc:  0.8997204081632654\n","Epoch:  125 Seg loss:  0.28805444945418646 Seg acc:  0.8998137350178167\n","Epoch:  126 Seg loss:  0.28699508391496703 Seg acc:  0.9002249718785152\n","Epoch:  127 Seg loss:  0.2865163228707388 Seg acc:  0.9003766741071427\n","Epoch:  128 Seg loss:  0.28562257883622666 Seg acc:  0.9006743395032433\n","Epoch:  129 Seg loss:  0.2850404585783298 Seg acc:  0.9008320251177395\n","Epoch:  130 Seg loss:  0.2846361416896791 Seg acc:  0.9008646206574232\n","Epoch:  130 F1 (val):  0.7393402933116161 Acc (val):  0.7727551020408163\n","Epoch:  131 Seg loss:  0.28401536471915967 Seg acc:  0.9010551948051948\n","Epoch:  132 Seg loss:  0.28316995635964815 Seg acc:  0.9014097744360903\n","Epoch:  133 Seg loss:  0.28277247777180886 Seg acc:  0.9016315108132805\n","Epoch:  134 Seg loss:  0.28243401105757115 Seg acc:  0.9017063492063492\n","Epoch:  135 Seg loss:  0.2818908307920484 Seg acc:  0.9018776260504201\n","Epoch:  136 Seg loss:  0.28180237538623115 Seg acc:  0.9018974378072397\n","Epoch:  137 Seg loss:  0.281266946291578 Seg acc:  0.9020574534161491\n","Epoch:  138 Seg loss:  0.2801031396864987 Seg acc:  0.9025271619439142\n","Epoch:  139 Seg loss:  0.2791580752070461 Seg acc:  0.9029063411078718\n","Epoch:  140 Seg loss:  0.27879131183767997 Seg acc:  0.9030178028658272\n","Epoch:  140 F1 (val):  0.7391516713276544 Acc (val):  0.771734693877551\n","Epoch:  141 Seg loss:  0.27840729996981756 Seg acc:  0.9032265018683528\n","Epoch:  142 Seg loss:  0.2780714624307372 Seg acc:  0.9032895675752819\n","Epoch:  143 Seg loss:  0.27751810843538904 Seg acc:  0.903482851473923\n","Epoch:  144 Seg loss:  0.2767172680332743 Seg acc:  0.9037772695285012\n","Epoch:  145 Seg loss:  0.2760257520393966 Seg acc:  0.9041165781381046\n","Epoch:  146 Seg loss:  0.2751260363832623 Seg acc:  0.9044425933638762\n","Epoch:  147 Seg loss:  0.27484819227577867 Seg acc:  0.904521166574738\n","Epoch:  148 Seg loss:  0.2744452047768055 Seg acc:  0.9046911382002466\n","Epoch:  149 Seg loss:  0.2738633391757806 Seg acc:  0.904858843537415\n","Epoch:  150 Seg loss:  0.2734110313436843 Seg acc:  0.9050108122719288\n","Epoch:  150 F1 (val):  0.7370293358198282 Acc (val):  0.7710408163265307\n","Epoch:  151 Seg loss:  0.2730473249072307 Seg acc:  0.9051490332975295\n","Epoch:  152 Seg loss:  0.2723577111667278 Seg acc:  0.9054338402027478\n","Epoch:  153 Seg loss:  0.2718065053514846 Seg acc:  0.9056288099655445\n","Epoch:  154 Seg loss:  0.2714022035079618 Seg acc:  0.9057225148123765\n","Epoch:  155 Seg loss:  0.27085417590271205 Seg acc:  0.9059343929879643\n","Epoch:  156 Seg loss:  0.2703830227255821 Seg acc:  0.9060753282204601\n","Epoch:  157 Seg loss:  0.2703231919698323 Seg acc:  0.906114376130199\n","Epoch:  158 Seg loss:  0.26974334886426443 Seg acc:  0.9062716596072392\n","Epoch:  159 Seg loss:  0.2690284192096442 Seg acc:  0.9065513392857142\n","Epoch:  160 Seg loss:  0.2680783814154797 Seg acc:  0.906930536189631\n","Epoch:  160 F1 (val):  0.7258535274974139 Acc (val):  0.767734693877551\n","Epoch:  161 Seg loss:  0.26738077172158675 Seg acc:  0.9071806500377927\n","Epoch:  162 Seg loss:  0.2670505616379662 Seg acc:  0.9073650932765744\n","Epoch:  163 Seg loss:  0.2664204525147996 Seg acc:  0.9076359507217522\n","Epoch:  164 Seg loss:  0.26593022247155507 Seg acc:  0.9078293135435992\n","Epoch:  165 Seg loss:  0.2653518195252821 Seg acc:  0.9080387878042783\n","Epoch:  166 Seg loss:  0.26500014603851796 Seg acc:  0.9081174385922033\n","Epoch:  167 Seg loss:  0.2647089387866713 Seg acc:  0.9082650024295431\n","Epoch:  168 Seg loss:  0.26470662053872845 Seg acc:  0.9084198768264701\n","Epoch:  169 Seg loss:  0.2640737036571783 Seg acc:  0.9086464585834332\n","Epoch:  170 Seg loss:  0.263501849439409 Seg acc:  0.9088718820861676\n","Epoch:  170 F1 (val):  0.7377353929154562 Acc (val):  0.7718367346938776\n","Epoch:  171 Seg loss:  0.2631211123147676 Seg acc:  0.9089789985761745\n","Epoch:  172 Seg loss:  0.26249659698822597 Seg acc:  0.9092293853957767\n","Epoch:  173 Seg loss:  0.26202146880243016 Seg acc:  0.9094138517475955\n","Epoch:  174 Seg loss:  0.26151780911854333 Seg acc:  0.9096137026239065\n","Epoch:  175 Seg loss:  0.2610441977158189 Seg acc:  0.9097764958256029\n","Epoch:  176 Seg loss:  0.2605690463619717 Seg acc:  0.9099100657212036\n","Epoch:  177 Seg loss:  0.26016788126042717 Seg acc:  0.9100106053657417\n","Epoch:  178 Seg loss:  0.2599474870958808 Seg acc:  0.9101256983240223\n","Epoch:  179 Seg loss:  0.2593944276372592 Seg acc:  0.9103245464852607\n","Epoch:  180 Seg loss:  0.2587286112881497 Seg acc:  0.9105846205885669\n","Epoch:  180 F1 (val):  0.7346605200372716 Acc (val):  0.7696122448979592\n","Epoch:  181 Seg loss:  0.25822467422419854 Seg acc:  0.910710080735591\n","Epoch:  182 Seg loss:  0.2578103964934584 Seg acc:  0.9109094457455112\n","Epoch:  183 Seg loss:  0.2574182894566785 Seg acc:  0.9110692102928127\n","Epoch:  184 Seg loss:  0.25686795349056657 Seg acc:  0.9112617209045781\n","Epoch:  185 Seg loss:  0.25623185251669217 Seg acc:  0.9115001645819619\n","Epoch:  186 Seg loss:  0.25570381380976204 Seg acc:  0.9116746698679472\n","Epoch:  187 Seg loss:  0.25566308271694693 Seg acc:  0.9117319800260529\n","Epoch:  188 Seg loss:  0.25536996457311845 Seg acc:  0.9118048806824318\n","Epoch:  189 Seg loss:  0.25499787189458545 Seg acc:  0.9119495166487648\n","Epoch:  190 Seg loss:  0.25460411288351287 Seg acc:  0.9121327064857356\n","Epoch:  190 F1 (val):  0.7366244657840384 Acc (val):  0.7713877551020408\n","Epoch:  191 Seg loss:  0.25406566192395985 Seg acc:  0.9122754570578232\n","Epoch:  192 Seg loss:  0.25348503424404817 Seg acc:  0.9125145394945543\n","Epoch:  193 Seg loss:  0.25294432213011475 Seg acc:  0.9127235430254576\n","Epoch:  194 Seg loss:  0.2528242052365572 Seg acc:  0.9128349031920461\n","Epoch:  195 Seg loss:  0.2525562139189973 Seg acc:  0.9129503331945023\n","Epoch:  196 Seg loss:  0.2524188710802098 Seg acc:  0.9130904900031077\n","Epoch:  197 Seg loss:  0.2519353453559105 Seg acc:  0.913301381158524\n","Epoch:  198 Seg loss:  0.25170832864902726 Seg acc:  0.9134396472156702\n","Epoch:  199 Seg loss:  0.2513144034147263 Seg acc:  0.913594387755102\n","Epoch:  200 Seg loss:  0.2509815358552174 Seg acc:  0.913748857752056\n","Epoch:  200 F1 (val):  0.7360002130840995 Acc (val):  0.7694795918367346\n","Epoch:  201 Seg loss:  0.25065983551563603 Seg acc:  0.91390432410588\n","Epoch:  202 Seg loss:  0.25013036732309557 Seg acc:  0.9140771086759826\n","Epoch:  203 Seg loss:  0.24976354961593947 Seg acc:  0.9142919667867145\n","Epoch:  204 Seg loss:  0.24919596510689432 Seg acc:  0.9145345943255351\n","Epoch:  205 Seg loss:  0.24898375773314133 Seg acc:  0.914623786407767\n","Epoch:  206 Seg loss:  0.2486487344843178 Seg acc:  0.914735531893917\n","Epoch:  207 Seg loss:  0.24844025096927697 Seg acc:  0.9148143151491366\n","Epoch:  208 Seg loss:  0.24810321999793988 Seg acc:  0.9149216385118641\n","Epoch:  209 Seg loss:  0.24762034167846045 Seg acc:  0.9150850340136054\n","Epoch:  210 Seg loss:  0.24737604696886234 Seg acc:  0.9151658767772513\n","Epoch:  210 F1 (val):  0.7217921057841135 Acc (val):  0.7646428571428572\n","Epoch:  211 Seg loss:  0.24680499591917363 Seg acc:  0.9153735078937235\n","Epoch:  212 Seg loss:  0.24657201312237503 Seg acc:  0.9154402606112868\n","Epoch:  213 Seg loss:  0.24599750736884982 Seg acc:  0.9156387087545298\n","Epoch:  214 Seg loss:  0.245535290102626 Seg acc:  0.9157961556715709\n","Epoch:  215 Seg loss:  0.24501843253771463 Seg acc:  0.9160029289493576\n","Epoch:  216 Seg loss:  0.2445001534877285 Seg acc:  0.9161795824320512\n","Epoch:  217 Seg loss:  0.2440569398736735 Seg acc:  0.9164072739187419\n","Epoch:  218 Seg loss:  0.24376039724099582 Seg acc:  0.9165396980710093\n","Epoch:  219 Seg loss:  0.2433595400642265 Seg acc:  0.9166628014842302\n","Epoch:  220 Seg loss:  0.24284025325494654 Seg acc:  0.9168667466986796\n","Epoch:  220 F1 (val):  0.7262369472656567 Acc (val):  0.7663265306122449\n","Epoch:  221 Seg loss:  0.24252326209265906 Seg acc:  0.9169792241220814\n","Epoch:  222 Seg loss:  0.24219922454100554 Seg acc:  0.9170906927793541\n","Epoch:  223 Seg loss:  0.2417458721569606 Seg acc:  0.9172843021137026\n","Epoch:  224 Seg loss:  0.24135780387454564 Seg acc:  0.9174875283446712\n","Epoch:  225 Seg loss:  0.24111149413923247 Seg acc:  0.9175873668051291\n","Epoch:  226 Seg loss:  0.2408642084194175 Seg acc:  0.9177166681650634\n","Epoch:  227 Seg loss:  0.24050880635255262 Seg acc:  0.9179018976011457\n","Epoch:  228 Seg loss:  0.24003101930868156 Seg acc:  0.9180810533820515\n","Epoch:  229 Seg loss:  0.23970576304456462 Seg acc:  0.9181754658385093\n","Epoch:  230 Seg loss:  0.23929993627649365 Seg acc:  0.9183364254792827\n","Epoch:  230 F1 (val):  0.7348566782027702 Acc (val):  0.7681734693877551\n","Epoch:  231 Seg loss:  0.23894669610107766 Seg acc:  0.9184850017593245\n","Epoch:  232 Seg loss:  0.23848332061788044 Seg acc:  0.9186793816238942\n","Epoch:  233 Seg loss:  0.2381998611948429 Seg acc:  0.9188088697017269\n","Epoch:  234 Seg loss:  0.23771349496029792 Seg acc:  0.919001302648719\n","Epoch:  235 Seg loss:  0.23719477662974495 Seg acc:  0.919281822898651\n","Epoch:  236 Seg loss:  0.2367788893701155 Seg acc:  0.9194157409799363\n","Epoch:  237 Seg loss:  0.23619742322118342 Seg acc:  0.919648216429429\n","Epoch:  238 Seg loss:  0.23609626156016872 Seg acc:  0.9197730765946546\n","Epoch:  239 Seg loss:  0.23567421305924655 Seg acc:  0.9199277210884353\n","Epoch:  240 Seg loss:  0.23539739593677997 Seg acc:  0.9200641459903464\n","Epoch:  240 F1 (val):  0.7240686045822834 Acc (val):  0.7645102040816326\n","Epoch:  241 Seg loss:  0.2349790607478993 Seg acc:  0.9202352841963232\n","Epoch:  242 Seg loss:  0.23453915284739602 Seg acc:  0.9204102628705804\n","Epoch:  243 Seg loss:  0.2342064080790418 Seg acc:  0.9205691702910672\n","Epoch:  244 Seg loss:  0.23373947748724294 Seg acc:  0.920759058725531\n","Epoch:  245 Seg loss:  0.23361576430317832 Seg acc:  0.9209007383441181\n","Epoch:  246 Seg loss:  0.23331820937665368 Seg acc:  0.9210133851111295\n","Epoch:  247 Seg loss:  0.2331343600526452 Seg acc:  0.921106607965767\n","Epoch:  248 Seg loss:  0.2327811538155778 Seg acc:  0.9212441603147283\n","Epoch:  249 Seg loss:  0.23253031846880912 Seg acc:  0.9213357142857143\n","Epoch:  250 Seg loss:  0.23205456689771903 Seg acc:  0.921529189364989\n","Epoch:  250 F1 (val):  0.7282197029592066 Acc (val):  0.7639795918367347\n","Epoch:  251 Seg loss:  0.23167565565497156 Seg acc:  0.9217130304502754\n","Epoch:  252 Seg loss:  0.23162432621590234 Seg acc:  0.9217734129224813\n","Epoch:  253 Seg loss:  0.23134129177632295 Seg acc:  0.9219206974128235\n","Epoch:  254 Seg loss:  0.23103654799508114 Seg acc:  0.9220438175270108\n","Epoch:  255 Seg loss:  0.2308105675620027 Seg acc:  0.9221599968112244\n","Epoch:  256 Seg loss:  0.23039168945662242 Seg acc:  0.9223090208846184\n","Epoch:  257 Seg loss:  0.2299792037396006 Seg acc:  0.9224786426198386\n","Epoch:  258 Seg loss:  0.22962576844172128 Seg acc:  0.9226341501851706\n","Epoch:  259 Seg loss:  0.2293155355235705 Seg acc:  0.9227835557299843\n","Epoch:  260 Seg loss:  0.22882638260779253 Seg acc:  0.922958206271014\n","Epoch:  260 F1 (val):  0.7310734714137433 Acc (val):  0.7678367346938776\n","Epoch:  261 Seg loss:  0.22840181473671026 Seg acc:  0.9231169185231344\n","Epoch:  262 Seg loss:  0.2281380424084772 Seg acc:  0.9232346550787615\n","Epoch:  263 Seg loss:  0.22786687373776326 Seg acc:  0.9234114100185529\n","Epoch:  264 Seg loss:  0.2274913138657246 Seg acc:  0.9235964574509048\n","Epoch:  265 Seg loss:  0.2272754079921353 Seg acc:  0.9237129814331746\n","Epoch:  266 Seg loss:  0.22682434727859854 Seg acc:  0.9238897806313535\n","Epoch:  267 Seg loss:  0.22670891274934385 Seg acc:  0.9239795918367347\n","Epoch:  268 Seg loss:  0.2263725459243285 Seg acc:  0.924103823685608\n","Epoch:  269 Seg loss:  0.22592668864462112 Seg acc:  0.9242517006802721\n","Epoch:  270 Seg loss:  0.22556551202196917 Seg acc:  0.9243853076285864\n","Epoch:  270 F1 (val):  0.7336760506400062 Acc (val):  0.7688265306122449\n","Epoch:  271 Seg loss:  0.22521740296746 Seg acc:  0.9245113670468187\n","Epoch:  272 Seg loss:  0.2247440291793792 Seg acc:  0.9246953726545563\n","Epoch:  273 Seg loss:  0.22455648444321033 Seg acc:  0.9247830701623714\n","Epoch:  274 Seg loss:  0.22418254180388017 Seg acc:  0.9249183673469388\n","Epoch:  275 Seg loss:  0.22389164194464684 Seg acc:  0.9250480627033422\n","Epoch:  276 Seg loss:  0.2234742710844274 Seg acc:  0.9251841891991454\n","Epoch:  277 Seg loss:  0.2232272001050359 Seg acc:  0.9252606078402584\n","Epoch:  278 Seg loss:  0.22298277134750052 Seg acc:  0.9253977397410578\n","Epoch:  279 Seg loss:  0.222580014887665 Seg acc:  0.9255512026239067\n","Epoch:  280 Seg loss:  0.22213275661672138 Seg acc:  0.9256944948798026\n","Epoch:  280 F1 (val):  0.7298307076210468 Acc (val):  0.7643265306122449\n","Epoch:  281 Seg loss:  0.22171025580548226 Seg acc:  0.9258331524098999\n","Epoch:  282 Seg loss:  0.22138480889502346 Seg acc:  0.9259879570202639\n","Epoch:  283 Seg loss:  0.22102771253443101 Seg acc:  0.9261569416498995\n","Epoch:  284 Seg loss:  0.2207228179563556 Seg acc:  0.9262593984962406\n","Epoch:  285 Seg loss:  0.2204542865494748 Seg acc:  0.9263861138861138\n","Epoch:  286 Seg loss:  0.22021259092287734 Seg acc:  0.926495057953495\n","Epoch:  287 Seg loss:  0.21985838978758288 Seg acc:  0.92663779053288\n","Epoch:  288 Seg loss:  0.21952407448762015 Seg acc:  0.9267618812230775\n","Epoch:  289 Seg loss:  0.21911966998515459 Seg acc:  0.9269247009148487\n","Epoch:  290 Seg loss:  0.21874084781944955 Seg acc:  0.9270951679640929\n","Epoch:  290 F1 (val):  0.7267940833683106 Acc (val):  0.7645102040816326\n","Epoch:  291 Seg loss:  0.21848452989369221 Seg acc:  0.9271945764607215\n","Epoch:  292 Seg loss:  0.21821978286468127 Seg acc:  0.9273342272062409\n","Epoch:  293 Seg loss:  0.21789779929684944 Seg acc:  0.9274876787449674\n","Epoch:  294 Seg loss:  0.21757003462920754 Seg acc:  0.9276400899342788\n","Epoch:  295 Seg loss:  0.2172738629317767 Seg acc:  0.9277673400441258\n","Epoch:  296 Seg loss:  0.21697274648179912 Seg acc:  0.9279203600632172\n","Epoch:  297 Seg loss:  0.216728079238994 Seg acc:  0.928040679358992\n","Epoch:  298 Seg loss:  0.2164458014992966 Seg acc:  0.9281439833458468\n","Epoch:  299 Seg loss:  0.21613323303560417 Seg acc:  0.9283001700680271\n","Epoch:  300 F1 (val):  0.7202767844015123 Acc (val):  0.7619081632653061\n","Epoch:  301 Seg loss:  0.2167989410880792 Seg acc:  0.9281002101837413 Disc loss:  0.6264501 Gen loss:  0.99030393\n","Epoch:  302 Seg loss:  0.2169243757742525 Seg acc:  0.9280510879848629 Disc loss:  0.45259696 Gen loss:  0.55585015\n","Epoch:  303 Seg loss:  0.2168475287434685 Seg acc:  0.9280864821175997 Disc loss:  0.49645597 Gen loss:  0.47300097\n","Epoch:  304 Seg loss:  0.21676406717712157 Seg acc:  0.9281526919978518 Disc loss:  0.41704467 Gen loss:  0.51203346\n","Epoch:  305 Seg loss:  0.21668228143062748 Seg acc:  0.928226831716293 Disc loss:  0.44244546 Gen loss:  0.49580115\n","Epoch:  306 Seg loss:  0.21664183686663901 Seg acc:  0.9282846471922103 Disc loss:  0.472243 Gen loss:  0.5068883\n","Epoch:  307 Seg loss:  0.21687064416924984 Seg acc:  0.9281941766934785 Disc loss:  0.42343354 Gen loss:  0.62458825\n","Epoch:  308 Seg loss:  0.2169827652200089 Seg acc:  0.9281912602703417 Disc loss:  0.49875468 Gen loss:  0.57852197\n","Epoch:  309 Seg loss:  0.21738399895553065 Seg acc:  0.928067003500429 Disc loss:  0.37960356 Gen loss:  0.71231234\n","Epoch:  310 Seg loss:  0.2173154436532528 Seg acc:  0.9281484529295587 Disc loss:  0.41582224 Gen loss:  0.5476621\n","Epoch:  310 F1 (val):  0.7419719643176572 Acc (val):  0.7826938775510204\n","Epoch:  311 Seg loss:  0.21733345917854277 Seg acc:  0.9280915742502789 Disc loss:  0.5250137 Gen loss:  0.5351664\n","Epoch:  312 Seg loss:  0.21731149164052346 Seg acc:  0.9280497776033488 Disc loss:  0.34870732 Gen loss:  0.6078334\n","Epoch:  313 Seg loss:  0.21740126693115447 Seg acc:  0.9279740170828714 Disc loss:  0.39061782 Gen loss:  0.6192302\n","Epoch:  314 Seg loss:  0.21745834511461531 Seg acc:  0.9279068633822956 Disc loss:  0.4007246 Gen loss:  0.59070075\n","Epoch:  315 Seg loss:  0.217393602005073 Seg acc:  0.927942986718497 Disc loss:  0.4516795 Gen loss:  0.5527658\n","Epoch:  316 Seg loss:  0.2174960549071997 Seg acc:  0.9278747416688193 Disc loss:  0.48330516 Gen loss:  0.54438734\n","Epoch:  317 Seg loss:  0.2177356228420411 Seg acc:  0.9278053177106806 Disc loss:  0.4599859 Gen loss:  0.6427876\n","Epoch:  318 Seg loss:  0.2177603099752897 Seg acc:  0.9277571877807725 Disc loss:  0.42572594 Gen loss:  0.5770221\n","Epoch:  319 Seg loss:  0.21783952833062803 Seg acc:  0.9277445460942998 Disc loss:  0.41341466 Gen loss:  0.59645057\n","Epoch:  320 Seg loss:  0.21791267248336227 Seg acc:  0.9277582908163264 Disc loss:  0.46182823 Gen loss:  0.57838017\n","Epoch:  320 F1 (val):  0.7683980592793291 Acc (val):  0.7956020408163266\n","Epoch:  321 Seg loss:  0.2180057942078121 Seg acc:  0.927731419670672 Disc loss:  0.4321262 Gen loss:  0.5944507\n","Epoch:  322 Seg loss:  0.218129184837482 Seg acc:  0.9276587653695018 Disc loss:  0.40963513 Gen loss:  0.61066437\n","Epoch:  323 Seg loss:  0.21797042709109216 Seg acc:  0.9277397801225753 Disc loss:  0.4570772 Gen loss:  0.5107337\n","Epoch:  324 Seg loss:  0.2179992598377996 Seg acc:  0.9277470710506425 Disc loss:  0.40606123 Gen loss:  0.5925153\n","Epoch:  325 Seg loss:  0.21808545087392514 Seg acc:  0.9277307692307692 Disc loss:  0.32876927 Gen loss:  0.6961967\n","Epoch:  326 Seg loss:  0.21813373768165067 Seg acc:  0.927667616126205 Disc loss:  0.42230362 Gen loss:  0.59867287\n","Epoch:  327 Seg loss:  0.21823783932020177 Seg acc:  0.9276344941646382 Disc loss:  0.3460213 Gen loss:  0.59694254\n","Epoch:  328 Seg loss:  0.21840961929410696 Seg acc:  0.9275517981582877 Disc loss:  0.3908195 Gen loss:  0.6698423\n","Epoch:  329 Seg loss:  0.2185593839610239 Seg acc:  0.9274393648036722 Disc loss:  0.38464928 Gen loss:  0.6374819\n","Epoch:  330 Seg loss:  0.21859950041680626 Seg acc:  0.927404143475572 Disc loss:  0.3492583 Gen loss:  0.67676085\n","Epoch:  330 F1 (val):  0.7315240006640487 Acc (val):  0.7754897959183673\n","Epoch:  331 Seg loss:  0.21864854152739227 Seg acc:  0.927376071274431 Disc loss:  0.37976104 Gen loss:  0.68718517\n","Epoch:  332 Seg loss:  0.2185631200716079 Seg acc:  0.9274396053602164 Disc loss:  0.4626311 Gen loss:  0.54908955\n","Epoch:  333 Seg loss:  0.2186104884600496 Seg acc:  0.9273886131028987 Disc loss:  0.39397442 Gen loss:  0.5902276\n","Epoch:  334 Seg loss:  0.2186310107256481 Seg acc:  0.9273432726383967 Disc loss:  0.42967576 Gen loss:  0.5851919\n","Epoch:  335 Seg loss:  0.21871931417219675 Seg acc:  0.9273339932988119 Disc loss:  0.40147665 Gen loss:  0.66414475\n","Epoch:  336 Seg loss:  0.21877886317226858 Seg acc:  0.9273042699222546 Disc loss:  0.35190287 Gen loss:  0.63624597\n","Epoch:  337 Seg loss:  0.21893770473205018 Seg acc:  0.9272452007509234 Disc loss:  0.39331895 Gen loss:  0.6623383\n","Epoch:  338 Seg loss:  0.21897901698241573 Seg acc:  0.9271970474580364 Disc loss:  0.36483622 Gen loss:  0.6485167\n","Epoch:  339 Seg loss:  0.21887202483978244 Seg acc:  0.9272304497020047 Disc loss:  0.35753596 Gen loss:  0.62271726\n","Epoch:  340 Seg loss:  0.21913206943256014 Seg acc:  0.9271158463385354 Disc loss:  0.4883772 Gen loss:  0.67371666\n","Epoch:  340 F1 (val):  0.7436481259986483 Acc (val):  0.7821632653061225\n","Epoch:  341 Seg loss:  0.21926181055496166 Seg acc:  0.9270288467293074 Disc loss:  0.35849872 Gen loss:  0.6365863\n","Epoch:  342 Seg loss:  0.21911378755991223 Seg acc:  0.9270706528225325 Disc loss:  0.38117945 Gen loss:  0.6096963\n","Epoch:  343 Seg loss:  0.2189674955024316 Seg acc:  0.9271122151484502 Disc loss:  0.36484563 Gen loss:  0.64260316\n","Epoch:  344 Seg loss:  0.21894460048006717 Seg acc:  0.9271134907451352 Disc loss:  0.42808068 Gen loss:  0.54070294\n","Epoch:  345 Seg loss:  0.21890616734390672 Seg acc:  0.9271280686187519 Disc loss:  0.37833786 Gen loss:  0.63680387\n","Epoch:  346 Seg loss:  0.21901467130277197 Seg acc:  0.9270843163855136 Disc loss:  0.26004615 Gen loss:  0.8114507\n","Epoch:  347 Seg loss:  0.21915233931812841 Seg acc:  0.9270099394224548 Disc loss:  0.3618446 Gen loss:  0.636314\n","Epoch:  348 Seg loss:  0.2188881961307649 Seg acc:  0.9271251172882946 Disc loss:  0.35194135 Gen loss:  0.54354614\n","Epoch:  349 Seg loss:  0.21894526667359224 Seg acc:  0.9271241447868546 Disc loss:  0.42121616 Gen loss:  0.6457179\n","Epoch:  350 Seg loss:  0.21888611429503985 Seg acc:  0.9271275510204081 Disc loss:  0.31569093 Gen loss:  0.67930466\n","Epoch:  350 F1 (val):  0.7531120797217778 Acc (val):  0.7899693877551021\n","Epoch:  351 Seg loss:  0.21882451852077772 Seg acc:  0.927157102157102 Disc loss:  0.38301313 Gen loss:  0.6044518\n","Epoch:  352 Seg loss:  0.21873641113581305 Seg acc:  0.9272038787105751 Disc loss:  0.44282824 Gen loss:  0.58393383\n","Epoch:  353 Seg loss:  0.21875807995414598 Seg acc:  0.9271954674220964 Disc loss:  0.3545131 Gen loss:  0.6450548\n","Epoch:  354 Seg loss:  0.21845601303345066 Seg acc:  0.927294477112879 Disc loss:  0.43015337 Gen loss:  0.528008\n","Epoch:  355 Seg loss:  0.21836252621781657 Seg acc:  0.9273225064673756 Disc loss:  0.44073272 Gen loss:  0.6826458\n","Epoch:  356 Seg loss:  0.21836217114011222 Seg acc:  0.9273417794083926 Disc loss:  0.42819792 Gen loss:  0.5776743\n","Epoch:  357 Seg loss:  0.21849693718995034 Seg acc:  0.9272894872234607 Disc loss:  0.373181 Gen loss:  0.6636773\n","Epoch:  358 Seg loss:  0.21838502142599175 Seg acc:  0.9273244213886673 Disc loss:  0.48551297 Gen loss:  0.68864715\n","Epoch:  359 Seg loss:  0.21828078628001438 Seg acc:  0.9273456597123529 Disc loss:  0.42062533 Gen loss:  0.5208348\n","Epoch:  360 Seg loss:  0.2184231542257799 Seg acc:  0.927280328798186 Disc loss:  0.37919495 Gen loss:  0.74803597\n","Epoch:  360 F1 (val):  0.7539721304410724 Acc (val):  0.7815204081632653\n","Epoch:  361 Seg loss:  0.2185783728214182 Seg acc:  0.9271962801741196 Disc loss:  0.25486454 Gen loss:  0.74884486\n","Epoch:  362 Seg loss:  0.21860055471075832 Seg acc:  0.9271740049611006 Disc loss:  0.340014 Gen loss:  0.72969526\n","Epoch:  363 Seg loss:  0.2185602508095342 Seg acc:  0.9271785573733626 Disc loss:  0.36225915 Gen loss:  0.7118411\n","Epoch:  364 Seg loss:  0.21857550734101416 Seg acc:  0.9271466416236824 Disc loss:  0.412539 Gen loss:  0.6851697\n","Epoch:  365 Seg loss:  0.21837913404180578 Seg acc:  0.9272155437517473 Disc loss:  0.36713684 Gen loss:  0.6067495\n","Epoch:  366 Seg loss:  0.21832470451303518 Seg acc:  0.927243643358983 Disc loss:  0.31018254 Gen loss:  0.6783358\n","Epoch:  367 Seg loss:  0.21825456414219468 Seg acc:  0.9272854918534171 Disc loss:  0.3149059 Gen loss:  0.71920294\n","Epoch:  368 Seg loss:  0.21814888794703977 Seg acc:  0.9273395907275954 Disc loss:  0.4475854 Gen loss:  0.6537394\n","Epoch:  369 Seg loss:  0.21812386982405413 Seg acc:  0.9273450030418672 Disc loss:  0.30635327 Gen loss:  0.7653265\n","Epoch:  370 Seg loss:  0.21811124305870083 Seg acc:  0.9273455598455598 Disc loss:  0.42275164 Gen loss:  0.63191944\n","Epoch:  370 F1 (val):  0.7694614709217986 Acc (val):  0.7976530612244898\n","Epoch:  371 Seg loss:  0.21797705297119854 Seg acc:  0.9273839320094615 Disc loss:  0.46871263 Gen loss:  0.50668055\n","Epoch:  372 Seg loss:  0.21807439959738203 Seg acc:  0.9273130623217027 Disc loss:  0.46026903 Gen loss:  0.67602\n","Epoch:  373 Seg loss:  0.2179964260864833 Seg acc:  0.9273390053072166 Disc loss:  0.37639612 Gen loss:  0.66983867\n","Epoch:  374 Seg loss:  0.21797365239039462 Seg acc:  0.9273252482811306 Disc loss:  0.40701464 Gen loss:  0.6754597\n","Epoch:  375 Seg loss:  0.21788314924637478 Seg acc:  0.9273789115646258 Disc loss:  0.35947278 Gen loss:  0.6896101\n","Epoch:  376 Seg loss:  0.21765224502838038 Seg acc:  0.9274560356057318 Disc loss:  0.36305287 Gen loss:  0.66362\n","Epoch:  377 Seg loss:  0.21758494684250348 Seg acc:  0.9274806474313863 Disc loss:  0.323848 Gen loss:  0.6504772\n","Epoch:  378 Seg loss:  0.21752964261741864 Seg acc:  0.927497705431379 Disc loss:  0.29951882 Gen loss:  0.7032446\n","Epoch:  379 Seg loss:  0.21741596908163585 Seg acc:  0.9275301545420279 Disc loss:  0.34042323 Gen loss:  0.6080467\n","Epoch:  380 Seg loss:  0.21738540241984944 Seg acc:  0.9275436358754028 Disc loss:  0.42513555 Gen loss:  0.7111688\n","Epoch:  380 F1 (val):  0.7596298179789442 Acc (val):  0.7877142857142857\n","Epoch:  381 Seg loss:  0.21746930955197868 Seg acc:  0.927482725373614 Disc loss:  0.2741609 Gen loss:  0.8102119\n","Epoch:  382 Seg loss:  0.2173770090945416 Seg acc:  0.9274955924778288 Disc loss:  0.2370624 Gen loss:  0.7536228\n","Epoch:  383 Seg loss:  0.21742559185460716 Seg acc:  0.9274797516917994 Disc loss:  0.26260105 Gen loss:  0.7840202\n","Epoch:  384 Seg loss:  0.2174022772233002 Seg acc:  0.9274606717687076 Disc loss:  0.40195388 Gen loss:  0.66032565\n","Epoch:  385 Seg loss:  0.21755532190784233 Seg acc:  0.9273866949377153 Disc loss:  0.32620287 Gen loss:  0.7764236\n","Epoch:  386 Seg loss:  0.21738705558248753 Seg acc:  0.9274551919213282 Disc loss:  0.3824347 Gen loss:  0.6655291\n","Epoch:  387 Seg loss:  0.21722339800238918 Seg acc:  0.9275516795865633 Disc loss:  0.2734599 Gen loss:  0.73110473\n","Epoch:  388 Seg loss:  0.21718738594850928 Seg acc:  0.9275628550389229 Disc loss:  0.22695954 Gen loss:  0.67300427\n","Epoch:  389 Seg loss:  0.21715521151654824 Seg acc:  0.9276047951314202 Disc loss:  0.32496595 Gen loss:  0.74690247\n","Epoch:  390 Seg loss:  0.2171376798588496 Seg acc:  0.9276079277864993 Disc loss:  0.23864451 Gen loss:  0.81173414\n","Epoch:  390 F1 (val):  0.7657817949409459 Acc (val):  0.7843673469387755\n","Epoch:  391 Seg loss:  0.21713268926457677 Seg acc:  0.9276045200688972 Disc loss:  0.3199212 Gen loss:  0.92128944\n","Epoch:  392 Seg loss:  0.2171571890500431 Seg acc:  0.927559480424823 Disc loss:  0.30345872 Gen loss:  0.77425957\n","Epoch:  393 Seg loss:  0.21712905196970655 Seg acc:  0.9275711429610012 Disc loss:  0.22000289 Gen loss:  0.87205946\n","Epoch:  394 Seg loss:  0.21713848379889722 Seg acc:  0.92755749507925 Disc loss:  0.3354887 Gen loss:  0.7088109\n","Epoch:  395 Seg loss:  0.21710969720837436 Seg acc:  0.9275464996125032 Disc loss:  0.27299565 Gen loss:  0.75304306\n","Epoch:  396 Seg loss:  0.21706362224814266 Seg acc:  0.9275381364667079 Disc loss:  0.29129052 Gen loss:  0.7214885\n","Epoch:  397 Seg loss:  0.217088350327099 Seg acc:  0.9275137510923765 Disc loss:  0.3887141 Gen loss:  0.70848465\n","Epoch:  398 Seg loss:  0.21693448847786864 Seg acc:  0.9275587119269819 Disc loss:  0.23688114 Gen loss:  0.68259096\n","Epoch:  399 Seg loss:  0.2168766205434811 Seg acc:  0.9275708403662217 Disc loss:  0.31071234 Gen loss:  0.77828014\n","Epoch:  400 Seg loss:  0.21685850283131003 Seg acc:  0.9275803571428571 Disc loss:  0.32256627 Gen loss:  0.7851972\n","Epoch:  400 F1 (val):  0.7653733289841345 Acc (val):  0.7945204081632653\n","Epoch:  401 Seg loss:  0.21666278169859673 Seg acc:  0.927653442923304 Disc loss:  0.31514162 Gen loss:  0.70196766\n","Epoch:  402 Seg loss:  0.21652632560673638 Seg acc:  0.9277153771956544 Disc loss:  0.31681544 Gen loss:  0.7623062\n","Epoch:  403 Seg loss:  0.21637704774330627 Seg acc:  0.9277789031245252 Disc loss:  0.26896557 Gen loss:  0.7312754\n","Epoch:  404 Seg loss:  0.2163905224181933 Seg acc:  0.9277897049909073 Disc loss:  0.33307123 Gen loss:  0.5995436\n","Epoch:  405 Seg loss:  0.21620946554122147 Seg acc:  0.9278514739229025 Disc loss:  0.26420945 Gen loss:  0.66624635\n","Epoch:  406 Seg loss:  0.21603689260156872 Seg acc:  0.927887177038303 Disc loss:  0.27082497 Gen loss:  0.7542942\n","Epoch:  407 Seg loss:  0.21610171362501399 Seg acc:  0.9278562653562653 Disc loss:  0.28617772 Gen loss:  0.82668954\n","Epoch:  408 Seg loss:  0.2160054199653221 Seg acc:  0.927890531212485 Disc loss:  0.2551902 Gen loss:  0.7219264\n","Epoch:  409 Seg loss:  0.21583656604176338 Seg acc:  0.9279701611696024 Disc loss:  0.32823098 Gen loss:  0.62114024\n","Epoch:  410 Seg loss:  0.21579012312903637 Seg acc:  0.9280046042807367 Disc loss:  0.2838819 Gen loss:  0.715033\n","Epoch:  410 F1 (val):  0.7604237399803523 Acc (val):  0.7985918367346939\n","Epoch:  411 Seg loss:  0.2157041828381464 Seg acc:  0.9280593624311039 Disc loss:  0.339012 Gen loss:  0.68964636\n","Epoch:  412 Seg loss:  0.2155211383603441 Seg acc:  0.92813366851595 Disc loss:  0.21405588 Gen loss:  0.71768194\n","Epoch:  413 Seg loss:  0.21538959239715524 Seg acc:  0.9281816721846124 Disc loss:  0.30733645 Gen loss:  0.84064543\n","Epoch:  414 Seg loss:  0.21526732844647 Seg acc:  0.9282189687469191 Disc loss:  0.18580124 Gen loss:  0.8238767\n","Epoch:  415 Seg loss:  0.2150505485125335 Seg acc:  0.928304647160069 Disc loss:  0.2692228 Gen loss:  0.6914474\n","Epoch:  416 Seg loss:  0.2148780258050045 Seg acc:  0.9283580259026688 Disc loss:  0.3365811 Gen loss:  0.6864029\n","Epoch:  417 Seg loss:  0.21472455252060216 Seg acc:  0.9284148191650761 Disc loss:  0.3252245 Gen loss:  0.629528\n","Epoch:  418 Seg loss:  0.21457185608704696 Seg acc:  0.928459745142076 Disc loss:  0.31712842 Gen loss:  0.7472694\n","Epoch:  419 Seg loss:  0.2144770551303975 Seg acc:  0.9285026301690126 Disc loss:  0.27119797 Gen loss:  0.86376166\n","Epoch:  420 Seg loss:  0.21443379019342718 Seg acc:  0.9285222303206996 Disc loss:  0.25511098 Gen loss:  0.8108674\n","Epoch:  420 F1 (val):  0.7528195098099933 Acc (val):  0.7845714285714286\n","Epoch:  421 Seg loss:  0.21443318883554385 Seg acc:  0.9284962916282903 Disc loss:  0.28556907 Gen loss:  0.81405985\n","Epoch:  422 Seg loss:  0.2143523735281133 Seg acc:  0.9285031192571814 Disc loss:  0.3082438 Gen loss:  0.6181858\n","Epoch:  423 Seg loss:  0.21418087323577129 Seg acc:  0.9285569546967722 Disc loss:  0.26948228 Gen loss:  0.82728386\n","Epoch:  424 Seg loss:  0.21402398987128488 Seg acc:  0.9286249759337697 Disc loss:  0.270559 Gen loss:  0.7655139\n","Epoch:  425 Seg loss:  0.21397767331670312 Seg acc:  0.9286500600240095 Disc loss:  0.2800787 Gen loss:  0.6926098\n","Epoch:  426 Seg loss:  0.2140640638628756 Seg acc:  0.9286175385647216 Disc loss:  0.21431808 Gen loss:  0.9324545\n","Epoch:  427 Seg loss:  0.2138597728462074 Seg acc:  0.9286789657314917 Disc loss:  0.19395944 Gen loss:  0.7467011\n","Epoch:  428 Seg loss:  0.21387603365441907 Seg acc:  0.9286650057219148 Disc loss:  0.22808933 Gen loss:  0.9123807\n","Epoch:  429 Seg loss:  0.2138465613578305 Seg acc:  0.9286802483231054 Disc loss:  0.27476943 Gen loss:  0.7934871\n","Epoch:  430 Seg loss:  0.21362637724987296 Seg acc:  0.9287482202183198 Disc loss:  0.23706147 Gen loss:  0.8051357\n","Epoch:  430 F1 (val):  0.7468569034556887 Acc (val):  0.7858673469387755\n","Epoch:  431 Seg loss:  0.21346086778801168 Seg acc:  0.9288052227851696 Disc loss:  0.20960125 Gen loss:  0.71053946\n","Epoch:  432 Seg loss:  0.21341592135528722 Seg acc:  0.9288283021541951 Disc loss:  0.36910298 Gen loss:  0.90189976\n","Epoch:  433 Seg loss:  0.21331812565238592 Seg acc:  0.9288683602771364 Disc loss:  0.22491388 Gen loss:  0.8094183\n","Epoch:  434 Seg loss:  0.21328827584256774 Seg acc:  0.9288653249318161 Disc loss:  0.2548535 Gen loss:  0.7837845\n","Epoch:  435 Seg loss:  0.21325808958760623 Seg acc:  0.9288775510204081 Disc loss:  0.19591185 Gen loss:  0.88205874\n","Epoch:  436 Seg loss:  0.213154335702778 Seg acc:  0.9288996676652314 Disc loss:  0.22716315 Gen loss:  0.8747227\n","Epoch:  437 Seg loss:  0.21300160731412726 Seg acc:  0.9289356932704431 Disc loss:  0.1792573 Gen loss:  0.7893567\n","Epoch:  438 Seg loss:  0.2128946510011747 Seg acc:  0.9289494222346474 Disc loss:  0.2299192 Gen loss:  0.73095316\n","Epoch:  439 Seg loss:  0.21278797388755652 Seg acc:  0.9289805215936033 Disc loss:  0.2853909 Gen loss:  0.75491357\n","Epoch:  440 Seg loss:  0.2126944267952984 Seg acc:  0.9290126391465678 Disc loss:  0.17589983 Gen loss:  0.86558735\n","Epoch:  440 F1 (val):  0.7558607957275048 Acc (val):  0.7928061224489796\n","Epoch:  441 Seg loss:  0.21269344445529167 Seg acc:  0.9290388264149197 Disc loss:  0.18285316 Gen loss:  0.8464989\n","Epoch:  442 Seg loss:  0.2125734883735622 Seg acc:  0.9290793240373073 Disc loss:  0.2700754 Gen loss:  0.8144796\n","Epoch:  443 Seg loss:  0.2125141639809038 Seg acc:  0.9290799050997375 Disc loss:  0.25129744 Gen loss:  0.7349938\n","Epoch:  444 Seg loss:  0.2124464945817316 Seg acc:  0.9291155313476742 Disc loss:  0.2318484 Gen loss:  0.81179374\n","Epoch:  445 Seg loss:  0.2122626470046097 Seg acc:  0.9291813804173356 Disc loss:  0.2287394 Gen loss:  0.6456709\n","Epoch:  446 Seg loss:  0.21211177665289208 Seg acc:  0.9292526539763888 Disc loss:  0.3411997 Gen loss:  0.8020705\n","Epoch:  447 Seg loss:  0.21202539724258235 Seg acc:  0.9292556955668174 Disc loss:  0.26799172 Gen loss:  0.796992\n","Epoch:  448 Seg loss:  0.21196969894559256 Seg acc:  0.929278653425656 Disc loss:  0.21848454 Gen loss:  0.9074123\n","Epoch:  449 Seg loss:  0.21192941321827521 Seg acc:  0.9292946911504022 Disc loss:  0.26395267 Gen loss:  0.8044347\n","Epoch:  450 Seg loss:  0.21192066616482205 Seg acc:  0.9292687074829932 Disc loss:  0.19351776 Gen loss:  0.96736896\n","Epoch:  450 F1 (val):  0.7681802416317862 Acc (val):  0.7937244897959184\n","Epoch:  451 Seg loss:  0.2118172817378245 Seg acc:  0.929293746323363 Disc loss:  0.2762493 Gen loss:  0.6818593\n","Epoch:  452 Seg loss:  0.2119885812291002 Seg acc:  0.9292080549033773 Disc loss:  0.2727909 Gen loss:  1.0130496\n","Epoch:  453 Seg loss:  0.21195594686809227 Seg acc:  0.9292376222011984 Disc loss:  0.15885252 Gen loss:  1.0722713\n","Epoch:  454 Seg loss:  0.21185173865588225 Seg acc:  0.9292726782342893 Disc loss:  0.20830882 Gen loss:  0.92773986\n","Epoch:  455 Seg loss:  0.21179811083353484 Seg acc:  0.9292784256559766 Disc loss:  0.27512604 Gen loss:  0.84511995\n","Epoch:  456 Seg loss:  0.2116080123212254 Seg acc:  0.929360790368779 Disc loss:  0.34665278 Gen loss:  0.6133159\n","Epoch:  457 Seg loss:  0.21159228301022193 Seg acc:  0.9293384093243424 Disc loss:  0.18852115 Gen loss:  0.9257072\n","Epoch:  458 Seg loss:  0.21155507215115701 Seg acc:  0.9293551154086088 Disc loss:  0.22174028 Gen loss:  0.9222189\n","Epoch:  459 Seg loss:  0.21147609828344358 Seg acc:  0.9293623004757459 Disc loss:  0.23110308 Gen loss:  0.83398\n","Epoch:  460 Seg loss:  0.21133776336260465 Seg acc:  0.9294171472937001 Disc loss:  0.22092718 Gen loss:  0.8076273\n","Epoch:  460 F1 (val):  0.7595334275816323 Acc (val):  0.7898469387755102\n","Epoch:  461 Seg loss:  0.21126485924658703 Seg acc:  0.9294302536632875 Disc loss:  0.17811877 Gen loss:  0.8189717\n","Epoch:  462 Seg loss:  0.21101345718552023 Seg acc:  0.9295211591129959 Disc loss:  0.30243927 Gen loss:  0.7060389\n","Epoch:  463 Seg loss:  0.21083402839644166 Seg acc:  0.929573654515802 Disc loss:  0.13143367 Gen loss:  0.8823631\n","Epoch:  464 Seg loss:  0.2106184696553853 Seg acc:  0.9296627595003519 Disc loss:  0.22209348 Gen loss:  0.82966334\n","Epoch:  465 Seg loss:  0.21049768070700348 Seg acc:  0.9296927803379417 Disc loss:  0.2978393 Gen loss:  0.69529253\n","Epoch:  466 Seg loss:  0.21040103851533754 Seg acc:  0.9297363580625383 Disc loss:  0.19730473 Gen loss:  0.81752044\n","Epoch:  467 Seg loss:  0.21034821349433452 Seg acc:  0.9297606301621293 Disc loss:  0.3455453 Gen loss:  0.7840237\n","Epoch:  468 Seg loss:  0.21021583127096677 Seg acc:  0.9298125981161696 Disc loss:  0.18423301 Gen loss:  0.8020451\n","Epoch:  469 Seg loss:  0.21013845149070215 Seg acc:  0.9298420434271791 Disc loss:  0.21257116 Gen loss:  0.8037993\n","Epoch:  470 Seg loss:  0.2099817322765259 Seg acc:  0.9299012158054711 Disc loss:  0.28771394 Gen loss:  0.7046983\n","Epoch:  470 F1 (val):  0.7067503141553331 Acc (val):  0.769265306122449\n","Epoch:  471 Seg loss:  0.20996728765673445 Seg acc:  0.9298994757138526 Disc loss:  0.15752782 Gen loss:  0.957777\n","Epoch:  472 Seg loss:  0.20993773022003598 Seg acc:  0.9299171999308199 Disc loss:  0.18320087 Gen loss:  0.85381997\n","Epoch:  473 Seg loss:  0.2098613450536012 Seg acc:  0.929946714415153 Disc loss:  0.13965422 Gen loss:  0.86314726\n","Epoch:  474 Seg loss:  0.20965753637159926 Seg acc:  0.9300315379316285 Disc loss:  0.2170693 Gen loss:  0.790138\n","Epoch:  475 Seg loss:  0.20968356505820626 Seg acc:  0.9300295381310419 Disc loss:  0.18869609 Gen loss:  0.9002661\n","Epoch:  476 Seg loss:  0.20950051190472452 Seg acc:  0.9301068641742412 Disc loss:  0.3216309 Gen loss:  0.7116394\n","Epoch:  477 Seg loss:  0.20940853139889315 Seg acc:  0.9301421512001027 Disc loss:  0.1735656 Gen loss:  0.9317728\n","Epoch:  478 Seg loss:  0.2092759829934172 Seg acc:  0.9301868969345061 Disc loss:  0.2917633 Gen loss:  0.87702304\n","Epoch:  479 Seg loss:  0.2089732391662538 Seg acc:  0.9303022879297856 Disc loss:  0.12198782 Gen loss:  0.79091465\n","Epoch:  480 Seg loss:  0.20894840247929097 Seg acc:  0.9303194090136055 Disc loss:  0.24440971 Gen loss:  0.9642419\n","Epoch:  480 F1 (val):  0.765820737505524 Acc (val):  0.7894183673469388\n","Epoch:  481 Seg loss:  0.20887828220200885 Seg acc:  0.9303216088930376 Disc loss:  0.1653083 Gen loss:  0.94908863\n","Epoch:  482 Seg loss:  0.20888505038260424 Seg acc:  0.9303026293504955 Disc loss:  0.2701505 Gen loss:  0.9154422\n","Epoch:  483 Seg loss:  0.2088184478243439 Seg acc:  0.9303243968394812 Disc loss:  0.2538274 Gen loss:  0.68884975\n","Epoch:  484 Seg loss:  0.20868887321269217 Seg acc:  0.9303834963737562 Disc loss:  0.22803633 Gen loss:  0.7857343\n","Epoch:  485 Seg loss:  0.20868025228534778 Seg acc:  0.9304123711340206 Disc loss:  0.21041407 Gen loss:  0.90781385\n","Epoch:  486 Seg loss:  0.20855418500708944 Seg acc:  0.9304490005878895 Disc loss:  0.20768021 Gen loss:  0.91076374\n","Epoch:  487 Seg loss:  0.20834159907680272 Seg acc:  0.9305300046096469 Disc loss:  0.13623495 Gen loss:  0.8743966\n","Epoch:  488 Seg loss:  0.20820037041958728 Seg acc:  0.9305991761458683 Disc loss:  0.34832874 Gen loss:  0.77587885\n","Epoch:  489 Seg loss:  0.20795980705317788 Seg acc:  0.9306868452902634 Disc loss:  0.2526738 Gen loss:  0.7512089\n","Epoch:  490 Seg loss:  0.2077613916476162 Seg acc:  0.9307772802998752 Disc loss:  0.2558622 Gen loss:  0.8426354\n","Epoch:  490 F1 (val):  0.7567384644318048 Acc (val):  0.7909489795918367\n","Epoch:  491 Seg loss:  0.2077106882774417 Seg acc:  0.930790972193358 Disc loss:  0.18070012 Gen loss:  0.8545862\n","Epoch:  492 Seg loss:  0.20753115942565406 Seg acc:  0.9308409034345446 Disc loss:  0.24737169 Gen loss:  0.9018941\n","Epoch:  493 Seg loss:  0.2074411035067653 Seg acc:  0.9308590677650371 Disc loss:  0.22729908 Gen loss:  0.85028684\n","Epoch:  494 Seg loss:  0.20718685460657726 Seg acc:  0.9309458398744115 Disc loss:  0.14858665 Gen loss:  0.89345336\n","Epoch:  495 Seg loss:  0.20700491630669796 Seg acc:  0.9310018552875697 Disc loss:  0.2615724 Gen loss:  0.7147024\n","Epoch:  496 Seg loss:  0.20681457601547723 Seg acc:  0.9310653596115867 Disc loss:  0.2868558 Gen loss:  0.80372846\n","Epoch:  497 Seg loss:  0.20678583889360158 Seg acc:  0.9310798464254918 Disc loss:  0.2704399 Gen loss:  0.8928376\n","Epoch:  498 Seg loss:  0.2067476632036119 Seg acc:  0.9310978608310794 Disc loss:  0.18104978 Gen loss:  0.82791215\n","Epoch:  499 Seg loss:  0.2065721000274102 Seg acc:  0.9311638583289028 Disc loss:  0.116763785 Gen loss:  0.87949806\n","Epoch:  500 Seg loss:  0.20652013416588305 Seg acc:  0.9311852040816327 Disc loss:  0.12743843 Gen loss:  0.99201465\n","Epoch:  500 F1 (val):  0.7333836084833567 Acc (val):  0.7766428571428572\n","Epoch:  501 Seg loss:  0.20627844119500258 Seg acc:  0.931271640392684 Disc loss:  0.24016255 Gen loss:  0.75361305\n","Epoch:  502 Seg loss:  0.206217736658822 Seg acc:  0.931270326855842 Disc loss:  0.108275525 Gen loss:  0.96567214\n","Epoch:  503 Seg loss:  0.20613798656117607 Seg acc:  0.9313040126587415 Disc loss:  0.12614383 Gen loss:  0.9879065\n","Epoch:  504 Seg loss:  0.20615205964043973 Seg acc:  0.9313026401036605 Disc loss:  0.161136 Gen loss:  0.9358429\n","Epoch:  505 Seg loss:  0.20609090098060004 Seg acc:  0.9313184481713478 Disc loss:  0.14065239 Gen loss:  0.9371993\n","Epoch:  506 Seg loss:  0.20603106247459946 Seg acc:  0.9313346979107849 Disc loss:  0.11517186 Gen loss:  0.88273937\n","Epoch:  507 Seg loss:  0.20595591008310488 Seg acc:  0.93137251942197 Disc loss:  0.119491324 Gen loss:  1.0364234\n","Epoch:  508 Seg loss:  0.2060301485141431 Seg acc:  0.9313469186887353 Disc loss:  0.28821266 Gen loss:  1.0248077\n","Epoch:  509 Seg loss:  0.20601649219371482 Seg acc:  0.9313920853213584 Disc loss:  0.1862225 Gen loss:  0.8419272\n","Epoch:  510 Seg loss:  0.20586596844243069 Seg acc:  0.9314625850340136 Disc loss:  0.1736526 Gen loss:  0.9738555\n","Epoch:  510 F1 (val):  0.7425804893837298 Acc (val):  0.7802244897959184\n","Epoch:  511 Seg loss:  0.20578512657295234 Seg acc:  0.9314873796876871 Disc loss:  0.17742382 Gen loss:  0.8428821\n","Epoch:  512 Seg loss:  0.20562024605169427 Seg acc:  0.9315364915497448 Disc loss:  0.1519739 Gen loss:  0.86080354\n","Epoch:  513 Seg loss:  0.20548295671189273 Seg acc:  0.9315824282929546 Disc loss:  0.23918302 Gen loss:  0.7676914\n","Epoch:  514 Seg loss:  0.20527596757221778 Seg acc:  0.9316693798141824 Disc loss:  0.15266985 Gen loss:  0.74907976\n","Epoch:  515 Seg loss:  0.20507831279803249 Seg acc:  0.9317465821279968 Disc loss:  0.2649777 Gen loss:  0.7569279\n","Epoch:  516 Seg loss:  0.20502905788752 Seg acc:  0.9317557546274323 Disc loss:  0.17747492 Gen loss:  0.8251896\n","Epoch:  517 Seg loss:  0.2048141035598989 Seg acc:  0.9318403860577114 Disc loss:  0.18872486 Gen loss:  0.7442703\n","Epoch:  518 Seg loss:  0.20469810823733742 Seg acc:  0.9318611614529981 Disc loss:  0.16940734 Gen loss:  0.78002197\n","Epoch:  519 Seg loss:  0.2046468122243192 Seg acc:  0.9318631787975306 Disc loss:  0.16696857 Gen loss:  0.9558878\n","Epoch:  520 Seg loss:  0.20466428715735674 Seg acc:  0.9318759811616955 Disc loss:  0.14900948 Gen loss:  0.98311406\n","Epoch:  520 F1 (val):  0.7425373540649596 Acc (val):  0.7809489795918367\n","Epoch:  521 Seg loss:  0.20462747237103457 Seg acc:  0.9318862861843394 Disc loss:  0.19150274 Gen loss:  1.0602571\n","Epoch:  522 Seg loss:  0.2044521474575631 Seg acc:  0.9319576393775901 Disc loss:  0.16339634 Gen loss:  0.91975856\n","Epoch:  523 Seg loss:  0.2044157458764873 Seg acc:  0.9319706754594762 Disc loss:  0.20563081 Gen loss:  0.8468691\n","Epoch:  524 Seg loss:  0.20445632109887726 Seg acc:  0.93194520174482 Disc loss:  0.08124398 Gen loss:  1.0945684\n","Epoch:  525 Seg loss:  0.2043197575921104 Seg acc:  0.9320038872691935 Disc loss:  0.12998037 Gen loss:  1.0017742\n","Epoch:  526 Seg loss:  0.20426417261135443 Seg acc:  0.9320085163342904 Disc loss:  0.20725435 Gen loss:  0.95684433\n","Epoch:  527 Seg loss:  0.2041010655307227 Seg acc:  0.9320658908724779 Disc loss:  0.18324052 Gen loss:  0.8321619\n","Epoch:  528 Seg loss:  0.2038977900802186 Seg acc:  0.9321525200989487 Disc loss:  0.17389512 Gen loss:  0.8834497\n","Epoch:  529 Seg loss:  0.20370789544061813 Seg acc:  0.9322176034875198 Disc loss:  0.19608629 Gen loss:  0.8127284\n","Epoch:  530 Seg loss:  0.2035690257431201 Seg acc:  0.9322631882941856 Disc loss:  0.12011142 Gen loss:  0.82051635\n","Epoch:  530 F1 (val):  0.7699322470448142 Acc (val):  0.7998979591836735\n","Epoch:  531 Seg loss:  0.20348945073432617 Seg acc:  0.9322768938083709 Disc loss:  0.1317735 Gen loss:  0.90770507\n","Epoch:  532 Seg loss:  0.20339993828613506 Seg acc:  0.9322991790701243 Disc loss:  0.17371237 Gen loss:  0.81836665\n","Epoch:  533 Seg loss:  0.20324791769373038 Seg acc:  0.9323496190220928 Disc loss:  0.16574088 Gen loss:  0.799169\n","Epoch:  534 Seg loss:  0.20310892739545985 Seg acc:  0.9324151570740656 Disc loss:  0.20360738 Gen loss:  0.90810084\n","Epoch:  535 Seg loss:  0.20319206508520607 Seg acc:  0.9324032042723631 Disc loss:  0.19766784 Gen loss:  0.95169175\n","Epoch:  536 Seg loss:  0.20306033755082692 Seg acc:  0.9324493603411513 Disc loss:  0.21788041 Gen loss:  0.7569751\n","Epoch:  537 Seg loss:  0.20300854248596525 Seg acc:  0.9324492646220499 Disc loss:  0.10118109 Gen loss:  0.9608388\n","Epoch:  538 Seg loss:  0.2029810166159527 Seg acc:  0.9324534367650406 Disc loss:  0.22446084 Gen loss:  0.9582164\n","Epoch:  539 Seg loss:  0.20288922536793358 Seg acc:  0.9324646927416608 Disc loss:  0.23310682 Gen loss:  0.85379684\n","Epoch:  540 Seg loss:  0.20277134069689998 Seg acc:  0.9325094482237339 Disc loss:  0.19196206 Gen loss:  0.9336278\n","Epoch:  540 F1 (val):  0.7686935627420439 Acc (val):  0.8000510204081632\n","Epoch:  541 Seg loss:  0.20266634055074173 Seg acc:  0.9325417782639858 Disc loss:  0.16019145 Gen loss:  0.96709466\n","Epoch:  542 Seg loss:  0.20248662045301108 Seg acc:  0.9325975224037955 Disc loss:  0.12981011 Gen loss:  0.91580945\n","Epoch:  543 Seg loss:  0.202259237498978 Seg acc:  0.9326770210846769 Disc loss:  0.108338624 Gen loss:  0.9151011\n","Epoch:  544 Seg loss:  0.20205950446646004 Seg acc:  0.9327571653661465 Disc loss:  0.15699805 Gen loss:  1.007013\n","Epoch:  545 Seg loss:  0.20187275870404112 Seg acc:  0.9328253136116832 Disc loss:  0.18539964 Gen loss:  0.79813224\n","Epoch:  546 Seg loss:  0.20186347835066118 Seg acc:  0.9328408836061896 Disc loss:  0.09188103 Gen loss:  1.0750495\n","Epoch:  547 Seg loss:  0.20169101949692644 Seg acc:  0.9328993023169047 Disc loss:  0.12753507 Gen loss:  0.75496715\n","Epoch:  548 Seg loss:  0.20165301693508225 Seg acc:  0.9329146804707283 Disc loss:  0.10623634 Gen loss:  0.9479979\n","Epoch:  549 Seg loss:  0.2015003702435337 Seg acc:  0.9329657819411917 Disc loss:  0.17690367 Gen loss:  0.82670665\n","Epoch:  550 Seg loss:  0.20140399557622995 Seg acc:  0.9329911873840443 Disc loss:  0.20422289 Gen loss:  0.81210715\n","Epoch:  550 F1 (val):  0.7574875064067859 Acc (val):  0.7895510204081633\n","Epoch:  551 Seg loss:  0.20133597441626114 Seg acc:  0.9330137227304715 Disc loss:  0.11872951 Gen loss:  0.9197607\n","Epoch:  552 Seg loss:  0.20125954561745343 Seg acc:  0.9330324792960661 Disc loss:  0.120654136 Gen loss:  0.9306781\n","Epoch:  553 Seg loss:  0.20117273868800933 Seg acc:  0.9330640845850093 Disc loss:  0.12752752 Gen loss:  0.9041128\n","Epoch:  554 Seg loss:  0.20100694598919217 Seg acc:  0.9331222832093126 Disc loss:  0.116986886 Gen loss:  0.86414456\n","Epoch:  555 Seg loss:  0.20076241861055563 Seg acc:  0.9332064717779003 Disc loss:  0.13284348 Gen loss:  0.91451406\n","Epoch:  556 Seg loss:  0.20081105399474825 Seg acc:  0.93318299442079 Disc loss:  0.103215784 Gen loss:  1.0416341\n","Epoch:  557 Seg loss:  0.20073953910297723 Seg acc:  0.9332154764958047 Disc loss:  0.101127036 Gen loss:  1.0696402\n","Epoch:  558 Seg loss:  0.20051016159168708 Seg acc:  0.93330636017848 Disc loss:  0.17211656 Gen loss:  0.8655161\n","Epoch:  559 Seg loss:  0.20042457779426268 Seg acc:  0.9333353108685334 Disc loss:  0.071573004 Gen loss:  1.0692582\n","Epoch:  560 Seg loss:  0.20027931007955754 Seg acc:  0.9333846574344022 Disc loss:  0.1147248 Gen loss:  1.0504569\n","Epoch:  560 F1 (val):  0.7491571726408444 Acc (val):  0.7856224489795919\n","Epoch:  561 Seg loss:  0.20028388306312594 Seg acc:  0.933371530430354 Disc loss:  0.14657256 Gen loss:  0.9730756\n","Epoch:  562 Seg loss:  0.2001149072262104 Seg acc:  0.9334360701576004 Disc loss:  0.19605428 Gen loss:  0.6907408\n","Epoch:  563 Seg loss:  0.2001650680260167 Seg acc:  0.9334065864356399 Disc loss:  0.10923259 Gen loss:  1.0017508\n","Epoch:  564 Seg loss:  0.20014273159945686 Seg acc:  0.9334219858156029 Disc loss:  0.1766338 Gen loss:  0.8879148\n","Epoch:  565 Seg loss:  0.19999853045539517 Seg acc:  0.9334689362470652 Disc loss:  0.27741727 Gen loss:  0.8647052\n","Epoch:  566 Seg loss:  0.19987388694981384 Seg acc:  0.9334976923631645 Disc loss:  0.11054189 Gen loss:  0.8821077\n","Epoch:  567 Seg loss:  0.19979636326458508 Seg acc:  0.9335263470467552 Disc loss:  0.103662536 Gen loss:  0.939264\n","Epoch:  568 Seg loss:  0.1997061966275665 Seg acc:  0.9335481639839033 Disc loss:  0.14673583 Gen loss:  0.9623817\n","Epoch:  569 Seg loss:  0.1996534767909293 Seg acc:  0.9335703525698503 Disc loss:  0.16051109 Gen loss:  0.87530327\n","Epoch:  570 Seg loss:  0.19947509471523134 Seg acc:  0.9336345327604726 Disc loss:  0.17820516 Gen loss:  0.90025234\n","Epoch:  570 F1 (val):  0.7723126403693185 Acc (val):  0.7996428571428571\n","Epoch:  571 Seg loss:  0.19946913670376595 Seg acc:  0.9336368347689338 Disc loss:  0.12283707 Gen loss:  0.9638052\n","Epoch:  572 Seg loss:  0.1994073827184372 Seg acc:  0.9336426965891251 Disc loss:  0.09924077 Gen loss:  0.9538839\n","Epoch:  573 Seg loss:  0.199396752419168 Seg acc:  0.9336476475406916 Disc loss:  0.18217361 Gen loss:  1.069173\n","Epoch:  574 Seg loss:  0.19927498548963343 Seg acc:  0.9337010239635922 Disc loss:  0.13110252 Gen loss:  1.0652332\n","Epoch:  575 Seg loss:  0.19921513740135277 Seg acc:  0.933737799467613 Disc loss:  0.118788645 Gen loss:  0.94647205\n","Epoch:  576 Seg loss:  0.19896867329953238 Seg acc:  0.9338355654761905 Disc loss:  0.21059866 Gen loss:  0.9665539\n","Epoch:  577 Seg loss:  0.19881961162184306 Seg acc:  0.9338901071693843 Disc loss:  0.13934498 Gen loss:  0.7816313\n","Epoch:  578 Seg loss:  0.19874104859378305 Seg acc:  0.933917096250265 Disc loss:  0.10529828 Gen loss:  0.8467409\n","Epoch:  579 Seg loss:  0.19852988969450994 Seg acc:  0.9339774770011632 Disc loss:  0.1550208 Gen loss:  0.889193\n","Epoch:  580 Seg loss:  0.19845587356208727 Seg acc:  0.9339826706544688 Disc loss:  0.13017067 Gen loss:  0.92752194\n","Epoch:  580 F1 (val):  0.7761272381858875 Acc (val):  0.8009081632653061\n","Epoch:  581 Seg loss:  0.19832198148057917 Seg acc:  0.9340269240226211 Disc loss:  0.07464169 Gen loss:  1.006692\n","Epoch:  582 Seg loss:  0.19817870332382595 Seg acc:  0.9340811066694719 Disc loss:  0.14241159 Gen loss:  0.82725656\n","Epoch:  583 Seg loss:  0.1980558051751674 Seg acc:  0.9341263520845732 Disc loss:  0.119087934 Gen loss:  0.9019334\n","Epoch:  584 Seg loss:  0.19814534885298513 Seg acc:  0.9340705374615601 Disc loss:  0.114450246 Gen loss:  1.1313686\n","Epoch:  585 Seg loss:  0.19807511093652147 Seg acc:  0.9340829408686552 Disc loss:  0.088802636 Gen loss:  0.9238672\n","Epoch:  586 Seg loss:  0.19800461291529417 Seg acc:  0.9341079264470294 Disc loss:  0.10295616 Gen loss:  0.9164865\n","Epoch:  587 Seg loss:  0.19788608022884574 Seg acc:  0.9341515140979731 Disc loss:  0.15723297 Gen loss:  0.8557508\n","Epoch:  588 Seg loss:  0.19790891412848316 Seg acc:  0.9341476641677079 Disc loss:  0.16152523 Gen loss:  0.88223714\n","Epoch:  589 Seg loss:  0.1977841311872815 Seg acc:  0.9341914694570527 Disc loss:  0.0854339 Gen loss:  0.8836791\n","Epoch:  590 Seg loss:  0.19780203557999457 Seg acc:  0.934167243168454 Disc loss:  0.10629413 Gen loss:  1.0929027\n","Epoch:  590 F1 (val):  0.7638499680667079 Acc (val):  0.7989795918367347\n","Epoch:  591 Seg loss:  0.1976311699804013 Seg acc:  0.9342160468248214 Disc loss:  0.08174163 Gen loss:  0.8653702\n","Epoch:  592 Seg loss:  0.19741706227304767 Seg acc:  0.9342931260341975 Disc loss:  0.1305638 Gen loss:  0.8930591\n","Epoch:  593 Seg loss:  0.1971939792432532 Seg acc:  0.9343652132016381 Disc loss:  0.15752184 Gen loss:  0.8590527\n","Epoch:  594 Seg loss:  0.19706240170306027 Seg acc:  0.9344164433450148 Disc loss:  0.06727732 Gen loss:  1.0422535\n","Epoch:  595 Seg loss:  0.19699045434093276 Seg acc:  0.9344340593380209 Disc loss:  0.07050982 Gen loss:  1.0710415\n","Epoch:  596 Seg loss:  0.19687895658707258 Seg acc:  0.9344610327352417 Disc loss:  0.08550514 Gen loss:  1.0002179\n","Epoch:  597 Seg loss:  0.19673599770146 Seg acc:  0.9345084264861723 Disc loss:  0.1512388 Gen loss:  0.98838824\n","Epoch:  598 Seg loss:  0.19660807790290752 Seg acc:  0.9345513958091598 Disc loss:  0.07099446 Gen loss:  1.0254012\n","Epoch:  599 Seg loss:  0.1965255404765001 Seg acc:  0.9345652618309428 Disc loss:  0.06428228 Gen loss:  0.9874952\n","Epoch:  600 Seg loss:  0.19631425001348057 Seg acc:  0.9346475340136056 Disc loss:  0.12625447 Gen loss:  0.77844954\n","Epoch:  600 F1 (val):  0.7471614336979783 Acc (val):  0.7804795918367347\n","Epoch:  601 Seg loss:  0.19609280973796836 Seg acc:  0.934727410098815 Disc loss:  0.10665714 Gen loss:  0.87946737\n","Epoch:  602 Seg loss:  0.19605709317192882 Seg acc:  0.9347303206997085 Disc loss:  0.089108616 Gen loss:  1.0458081\n","Epoch:  603 Seg loss:  0.19600496305829257 Seg acc:  0.9347488746742478 Disc loss:  0.17244907 Gen loss:  0.9954523\n","Epoch:  604 Seg loss:  0.19582249840339092 Seg acc:  0.9348159379645898 Disc loss:  0.10718886 Gen loss:  1.0644171\n","Epoch:  605 Seg loss:  0.19572149723891386 Seg acc:  0.9348477820880419 Disc loss:  0.10811496 Gen loss:  0.92447513\n","Epoch:  606 Seg loss:  0.1955625309175489 Seg acc:  0.9348993062571563 Disc loss:  0.072518975 Gen loss:  0.93747073\n","Epoch:  607 Seg loss:  0.1954025563644322 Seg acc:  0.9349531822613724 Disc loss:  0.092204325 Gen loss:  0.9458116\n","Epoch:  608 Seg loss:  0.19533556009226136 Seg acc:  0.9349842239527391 Disc loss:  0.17964154 Gen loss:  0.9731222\n","Epoch:  609 Seg loss:  0.19526825586489857 Seg acc:  0.9349984082302872 Disc loss:  0.07857294 Gen loss:  1.0575503\n","Epoch:  610 Seg loss:  0.19526537454641257 Seg acc:  0.9349799263967882 Disc loss:  0.114231214 Gen loss:  0.9585934\n","Epoch:  610 F1 (val):  0.7651142545902342 Acc (val):  0.7906632653061224\n","Epoch:  611 Seg loss:  0.1951001214256751 Seg acc:  0.9350333177460837 Disc loss:  0.07589105 Gen loss:  1.0121363\n","Epoch:  612 Seg loss:  0.19501994990323687 Seg acc:  0.9350498532746431 Disc loss:  0.12857345 Gen loss:  0.98054075\n","Epoch:  613 Seg loss:  0.19481530280549803 Seg acc:  0.9351245963311915 Disc loss:  0.19158071 Gen loss:  0.9845003\n","Epoch:  614 Seg loss:  0.19475893907443126 Seg acc:  0.935129711493718 Disc loss:  0.08949675 Gen loss:  1.0608304\n","Epoch:  615 Seg loss:  0.19467738363437537 Seg acc:  0.9351563796250206 Disc loss:  0.07415457 Gen loss:  0.94243777\n","Epoch:  616 Seg loss:  0.1946468404857079 Seg acc:  0.935177991651206 Disc loss:  0.09887533 Gen loss:  1.055685\n","Epoch:  617 Seg loss:  0.19454818909061014 Seg acc:  0.9352102834650878 Disc loss:  0.061376188 Gen loss:  1.0914786\n","Epoch:  618 Seg loss:  0.19440106064394647 Seg acc:  0.935246185852982 Disc loss:  0.12666036 Gen loss:  0.8775106\n","Epoch:  619 Seg loss:  0.19439775732034434 Seg acc:  0.9352283966898554 Disc loss:  0.13476388 Gen loss:  0.9584258\n","Epoch:  620 Seg loss:  0.19424578984298052 Seg acc:  0.9352876069782753 Disc loss:  0.17391157 Gen loss:  0.7265026\n","Epoch:  620 F1 (val):  0.7584812331213147 Acc (val):  0.7902755102040816\n","Epoch:  621 Seg loss:  0.19418093383960103 Seg acc:  0.9352948667389661 Disc loss:  0.1636436 Gen loss:  0.74056613\n","Epoch:  622 Seg loss:  0.19401075620910938 Seg acc:  0.935371415447208 Disc loss:  0.06260431 Gen loss:  1.0579705\n","Epoch:  623 Seg loss:  0.1938863458653036 Seg acc:  0.9354174173682315 Disc loss:  0.10604015 Gen loss:  0.9202079\n","Epoch:  624 Seg loss:  0.1938128552148835 Seg acc:  0.9354481456043956 Disc loss:  0.049520448 Gen loss:  1.0441787\n","Epoch:  625 Seg loss:  0.1937134426176548 Seg acc:  0.9354755102040816 Disc loss:  0.15164104 Gen loss:  0.9722148\n","Epoch:  626 Seg loss:  0.19358760225006377 Seg acc:  0.9355186803155766 Disc loss:  0.10335022 Gen loss:  1.1283473\n","Epoch:  627 Seg loss:  0.1935449010340506 Seg acc:  0.9355263157894735 Disc loss:  0.056829378 Gen loss:  1.1120435\n","Epoch:  628 Seg loss:  0.19338157645480078 Seg acc:  0.935583891199792 Disc loss:  0.055559676 Gen loss:  0.9642539\n","Epoch:  629 Seg loss:  0.1932825043690887 Seg acc:  0.9356157327795982 Disc loss:  0.08638263 Gen loss:  0.98492634\n","Epoch:  630 Seg loss:  0.1930295761615511 Seg acc:  0.9357102364755425 Disc loss:  0.070911855 Gen loss:  0.8953214\n","Epoch:  630 F1 (val):  0.7572193549462134 Acc (val):  0.7900612244897959\n","Epoch:  631 Seg loss:  0.19287524547091378 Seg acc:  0.9357733109091496 Disc loss:  0.035420034 Gen loss:  1.0373646\n","Epoch:  632 Seg loss:  0.19272921356973768 Seg acc:  0.9358264983208473 Disc loss:  0.05596941 Gen loss:  0.9855927\n","Epoch:  633 Seg loss:  0.19259853847030592 Seg acc:  0.9358662185253247 Disc loss:  0.21087852 Gen loss:  0.6695807\n","Epoch:  634 Seg loss:  0.1924709339599692 Seg acc:  0.9359122513358655 Disc loss:  0.06925762 Gen loss:  1.0604093\n","Epoch:  635 Seg loss:  0.19231315858251466 Seg acc:  0.9359685842841073 Disc loss:  0.059265964 Gen loss:  0.9787179\n","Epoch:  636 Seg loss:  0.19212520011237957 Seg acc:  0.9360387787190346 Disc loss:  0.09309858 Gen loss:  0.9528353\n","Epoch:  637 Seg loss:  0.19205810266489887 Seg acc:  0.9360739115112293 Disc loss:  0.09790913 Gen loss:  0.975241\n","Epoch:  638 Seg loss:  0.19210996877016692 Seg acc:  0.9360553547437782 Disc loss:  0.108798295 Gen loss:  1.065734\n","Epoch:  639 Seg loss:  0.19201145547329912 Seg acc:  0.9361019290345245 Disc loss:  0.08398683 Gen loss:  1.0114399\n","Epoch:  640 Seg loss:  0.19186300728470088 Seg acc:  0.9361567283163265 Disc loss:  0.07835928 Gen loss:  1.0715106\n","Epoch:  640 F1 (val):  0.7463703061099871 Acc (val):  0.7889693877551021\n","Epoch:  641 Seg loss:  0.19178129046829925 Seg acc:  0.9361922538126014 Disc loss:  0.09070842 Gen loss:  0.9664185\n","Epoch:  642 Seg loss:  0.19162218048704377 Seg acc:  0.9362447549113102 Disc loss:  0.13255596 Gen loss:  0.8416817\n","Epoch:  643 Seg loss:  0.1915361998866357 Seg acc:  0.9362804297457707 Disc loss:  0.06956042 Gen loss:  0.9549326\n","Epoch:  644 Seg loss:  0.19152225205755752 Seg acc:  0.9362926226391177 Disc loss:  0.08066619 Gen loss:  0.99575806\n","Epoch:  645 Seg loss:  0.19140345048765803 Seg acc:  0.9363162474292042 Disc loss:  0.07925803 Gen loss:  0.9182538\n","Epoch:  646 Seg loss:  0.19128916234977475 Seg acc:  0.9363409837619256 Disc loss:  0.0689537 Gen loss:  1.1137432\n","Epoch:  647 Seg loss:  0.19113479646777812 Seg acc:  0.9364070434974607 Disc loss:  0.067967586 Gen loss:  1.0404651\n","Epoch:  648 Seg loss:  0.19099227834584903 Seg acc:  0.9364520345175106 Disc loss:  0.16154204 Gen loss:  0.8374148\n","Epoch:  649 Seg loss:  0.19090885528935858 Seg acc:  0.9364642621301216 Disc loss:  0.12279405 Gen loss:  1.1071954\n","Epoch:  650 Seg loss:  0.19071010105884992 Seg acc:  0.936533359497645 Disc loss:  0.11987607 Gen loss:  0.84888273\n","Epoch:  650 F1 (val):  0.755738317406015 Acc (val):  0.7918979591836734\n","Epoch:  651 Seg loss:  0.1905578569195787 Seg acc:  0.9365850026646603 Disc loss:  0.07651234 Gen loss:  0.9905713\n","Epoch:  652 Seg loss:  0.1904407537490861 Seg acc:  0.9366196632027044 Disc loss:  0.08014997 Gen loss:  0.8591437\n","Epoch:  653 Seg loss:  0.19035880925963886 Seg acc:  0.9366342938400475 Disc loss:  0.077437975 Gen loss:  1.0401124\n","Epoch:  654 Seg loss:  0.19022623838042266 Seg acc:  0.9366777444922924 Disc loss:  0.15671718 Gen loss:  0.9650594\n","Epoch:  655 Seg loss:  0.19018492194759937 Seg acc:  0.936683284000623 Disc loss:  0.066201456 Gen loss:  1.0261258\n","Epoch:  656 Seg loss:  0.19001245717858758 Seg acc:  0.9367463601294177 Disc loss:  0.17905661 Gen loss:  0.8718896\n","Epoch:  657 Seg loss:  0.18985859621061038 Seg acc:  0.9368076911129749 Disc loss:  0.11137548 Gen loss:  0.885129\n","Epoch:  658 Seg loss:  0.1898231790103811 Seg acc:  0.9368370448483345 Disc loss:  0.060106717 Gen loss:  1.0082792\n","Epoch:  659 Seg loss:  0.18969560192957272 Seg acc:  0.9368887615744326 Disc loss:  0.102436475 Gen loss:  0.96841085\n","Epoch:  660 Seg loss:  0.18950069114340073 Seg acc:  0.9369546227581943 Disc loss:  0.06827016 Gen loss:  0.98129475\n","Epoch:  660 F1 (val):  0.7601513756832163 Acc (val):  0.7926632653061224\n","Epoch:  661 Seg loss:  0.18938886651518486 Seg acc:  0.9369897959183674 Disc loss:  0.06679049 Gen loss:  1.0220212\n","Epoch:  662 Seg loss:  0.18927108673125595 Seg acc:  0.9370406621863248 Disc loss:  0.053621955 Gen loss:  1.007494\n","Epoch:  663 Seg loss:  0.18912216230979692 Seg acc:  0.9370986856280974 Disc loss:  0.2095564 Gen loss:  1.0766718\n","Epoch:  664 Seg loss:  0.1890179608353829 Seg acc:  0.9371334829112367 Disc loss:  0.098896086 Gen loss:  0.97262263\n","Epoch:  665 Seg loss:  0.1890216273808838 Seg acc:  0.9371324996163879 Disc loss:  0.09310827 Gen loss:  1.0761448\n","Epoch:  666 Seg loss:  0.1889738967759652 Seg acc:  0.9371422442851014 Disc loss:  0.120810166 Gen loss:  0.9148705\n","Epoch:  667 Seg loss:  0.18890427427179274 Seg acc:  0.9371657283603095 Disc loss:  0.06351204 Gen loss:  1.0500952\n","Epoch:  668 Seg loss:  0.18890712948928692 Seg acc:  0.9371501894170842 Disc loss:  0.080559514 Gen loss:  1.0856218\n","Epoch:  669 Seg loss:  0.18881231638838178 Seg acc:  0.9371861749183977 Disc loss:  0.11629434 Gen loss:  0.8809068\n","Epoch:  670 Seg loss:  0.18860208592864114 Seg acc:  0.9372567011879379 Disc loss:  0.07831424 Gen loss:  0.8924762\n","Epoch:  670 F1 (val):  0.7688254433565931 Acc (val):  0.7987551020408163\n","Epoch:  671 Seg loss:  0.1884355579631961 Seg acc:  0.9373171325161956 Disc loss:  0.07303393 Gen loss:  0.8903052\n","Epoch:  672 Seg loss:  0.18838142756084425 Seg acc:  0.9373394223760934 Disc loss:  0.08391836 Gen loss:  1.0431869\n","Epoch:  673 Seg loss:  0.18830696228243976 Seg acc:  0.9373790823907572 Disc loss:  0.106995896 Gen loss:  0.9050096\n","Epoch:  674 Seg loss:  0.1882604598413147 Seg acc:  0.9373947798704051 Disc loss:  0.062565275 Gen loss:  1.0461682\n","Epoch:  675 Seg loss:  0.1881632416391814 Seg acc:  0.9374195011337869 Disc loss:  0.08935106 Gen loss:  1.002666\n","Epoch:  676 Seg loss:  0.18813797161975027 Seg acc:  0.9374256581330757 Disc loss:  0.078923464 Gen loss:  1.0354403\n","Epoch:  677 Seg loss:  0.18797964268698297 Seg acc:  0.9374853043137491 Disc loss:  0.10697542 Gen loss:  0.8961622\n","Epoch:  678 Seg loss:  0.18778347881692173 Seg acc:  0.9375556859912106 Disc loss:  0.12287027 Gen loss:  0.88691616\n","Epoch:  679 Seg loss:  0.1878074902947855 Seg acc:  0.9375405758768898 Disc loss:  0.09897985 Gen loss:  1.137843\n","Epoch:  680 Seg loss:  0.1877140877823181 Seg acc:  0.9375765306122449 Disc loss:  0.100815594 Gen loss:  1.0261672\n","Epoch:  680 F1 (val):  0.7513860301992062 Acc (val):  0.7856224489795919\n","Epoch:  681 Seg loss:  0.1876399451360741 Seg acc:  0.9376056369684439 Disc loss:  0.072240114 Gen loss:  0.9882998\n","Epoch:  682 Seg loss:  0.1875843823207351 Seg acc:  0.9376140852235322 Disc loss:  0.111559406 Gen loss:  1.0021945\n","Epoch:  683 Seg loss:  0.18741291650675923 Seg acc:  0.9376800280873695 Disc loss:  0.14804396 Gen loss:  0.8189494\n","Epoch:  684 Seg loss:  0.1873680557129763 Seg acc:  0.937685732187612 Disc loss:  0.12010582 Gen loss:  1.0940876\n","Epoch:  685 Seg loss:  0.18718918183968014 Seg acc:  0.9377506330999553 Disc loss:  0.047685605 Gen loss:  0.9682714\n","Epoch:  686 Seg loss:  0.18704323167030212 Seg acc:  0.937808651157256 Disc loss:  0.07594988 Gen loss:  0.9671608\n","Epoch:  687 Seg loss:  0.18701218763228067 Seg acc:  0.937821198348335 Disc loss:  0.07903932 Gen loss:  1.0714986\n","Epoch:  688 Seg loss:  0.18685812985546194 Seg acc:  0.9378744957285239 Disc loss:  0.09024606 Gen loss:  0.8958341\n","Epoch:  689 Seg loss:  0.1867413967793756 Seg acc:  0.9379091259145166 Disc loss:  0.13006324 Gen loss:  0.9257122\n","Epoch:  690 Seg loss:  0.18666404005733953 Seg acc:  0.9379188849452825 Disc loss:  0.09834076 Gen loss:  1.1091608\n","Epoch:  690 F1 (val):  0.7611360939862961 Acc (val):  0.7924183673469388\n","Epoch:  691 Seg loss:  0.18654409938865218 Seg acc:  0.9379607342213294 Disc loss:  0.047430646 Gen loss:  1.0350549\n","Epoch:  692 Seg loss:  0.1864515180737211 Seg acc:  0.93799177185325 Disc loss:  0.09470138 Gen loss:  0.98645884\n","Epoch:  693 Seg loss:  0.18635805984918688 Seg acc:  0.9380208793474099 Disc loss:  0.055623464 Gen loss:  1.0047762\n","Epoch:  694 Seg loss:  0.18621538019596842 Seg acc:  0.9380737958007409 Disc loss:  0.086771086 Gen loss:  0.80367\n","Epoch:  695 Seg loss:  0.18610087855578328 Seg acc:  0.9381126119512553 Disc loss:  0.06465052 Gen loss:  1.0280299\n","Epoch:  696 Seg loss:  0.18596855781694083 Seg acc:  0.9381531491907107 Disc loss:  0.08457008 Gen loss:  0.9651725\n","Epoch:  697 Seg loss:  0.1857888076662846 Seg acc:  0.9382155301144848 Disc loss:  0.08028765 Gen loss:  0.8868497\n","Epoch:  698 Seg loss:  0.1856422425810792 Seg acc:  0.9382664025495583 Disc loss:  0.052806914 Gen loss:  0.968313\n","Epoch:  699 Seg loss:  0.1855650376395265 Seg acc:  0.9382959621616888 Disc loss:  0.023521539 Gen loss:  1.0587475\n","Epoch:  700 Seg loss:  0.18552396218691553 Seg acc:  0.9383002915451895 Disc loss:  0.07371171 Gen loss:  1.0190122\n","Epoch:  700 F1 (val):  0.7644167011268521 Acc (val):  0.7958163265306123\n","Epoch:  701 Seg loss:  0.18538259034617652 Seg acc:  0.938346822323794 Disc loss:  0.08843465 Gen loss:  0.9939349\n","Epoch:  702 Seg loss:  0.18522963714268473 Seg acc:  0.9383954008954009 Disc loss:  0.03794431 Gen loss:  0.9850745\n","Epoch:  703 Seg loss:  0.18504317961686198 Seg acc:  0.9384652509652508 Disc loss:  0.046536557 Gen loss:  0.9952452\n","Epoch:  704 Seg loss:  0.1850663628905419 Seg acc:  0.9384457618274582 Disc loss:  0.07232964 Gen loss:  1.1649581\n","Epoch:  705 Seg loss:  0.184929397644092 Seg acc:  0.9384900130264872 Disc loss:  0.04376102 Gen loss:  0.97392726\n","Epoch:  706 Seg loss:  0.18489175229188234 Seg acc:  0.9384987281031394 Disc loss:  0.039283067 Gen loss:  1.0768505\n","Epoch:  707 Seg loss:  0.18471249764656075 Seg acc:  0.9385619028375142 Disc loss:  0.0514348 Gen loss:  0.920918\n","Epoch:  708 Seg loss:  0.18463174402608543 Seg acc:  0.9385928312002768 Disc loss:  0.033389367 Gen loss:  1.0198898\n","Epoch:  709 Seg loss:  0.18456299678121701 Seg acc:  0.9386269105667655 Disc loss:  0.062055625 Gen loss:  1.0944436\n","Epoch:  710 Seg loss:  0.1844538889458062 Seg acc:  0.9386691578039666 Disc loss:  0.08670238 Gen loss:  0.96525234\n","Epoch:  710 F1 (val):  0.7525838616722753 Acc (val):  0.7908367346938775\n","Epoch:  711 Seg loss:  0.18446254201558238 Seg acc:  0.938653520479922 Disc loss:  0.09159968 Gen loss:  0.9515616\n","Epoch:  712 Seg loss:  0.1843856082276933 Seg acc:  0.9386733977298785 Disc loss:  0.07316293 Gen loss:  1.005665\n","Epoch:  713 Seg loss:  0.18428388623021763 Seg acc:  0.938712539714343 Disc loss:  0.12679261 Gen loss:  0.92441463\n","Epoch:  714 Seg loss:  0.1842018488836305 Seg acc:  0.9387387097696223 Disc loss:  0.05923695 Gen loss:  0.98994523\n","Epoch:  715 Seg loss:  0.18406389571882628 Seg acc:  0.9387915655772799 Disc loss:  0.115315735 Gen loss:  1.0316468\n","Epoch:  716 Seg loss:  0.18399435216717047 Seg acc:  0.9388157707216966 Disc loss:  0.08620669 Gen loss:  1.0096428\n","Epoch:  717 Seg loss:  0.1839086522336535 Seg acc:  0.9388544957732048 Disc loss:  0.039059103 Gen loss:  1.0742692\n","Epoch:  718 Seg loss:  0.1838499679383212 Seg acc:  0.9388792564379513 Disc loss:  0.075060695 Gen loss:  0.9663964\n","Epoch:  719 Seg loss:  0.18382834141066243 Seg acc:  0.9388801765490618 Disc loss:  0.14208981 Gen loss:  0.89222527\n","Epoch:  720 Seg loss:  0.18364493276199534 Seg acc:  0.9389477040816326 Disc loss:  0.08664216 Gen loss:  0.87854254\n","Epoch:  720 F1 (val):  0.7474575557580794 Acc (val):  0.7866632653061224\n","Epoch:  721 Seg loss:  0.18357819887640242 Seg acc:  0.9389729400775567 Disc loss:  0.08499783 Gen loss:  1.0004768\n","Epoch:  722 Seg loss:  0.1834547775552461 Seg acc:  0.9390108259370231 Disc loss:  0.08379638 Gen loss:  0.9516187\n","Epoch:  723 Seg loss:  0.18337231388580916 Seg acc:  0.9390256725096677 Disc loss:  0.06005855 Gen loss:  1.0417833\n","Epoch:  724 Seg loss:  0.18322791970587074 Seg acc:  0.9390700755440298 Disc loss:  0.08721366 Gen loss:  0.95782197\n","Epoch:  725 Seg loss:  0.18312063642103096 Seg acc:  0.9391034482758621 Disc loss:  0.08751984 Gen loss:  0.8576884\n","Epoch:  726 Seg loss:  0.1830178810227098 Seg acc:  0.9391251335244842 Disc loss:  0.08730172 Gen loss:  0.93977326\n","Epoch:  727 Seg loss:  0.18290684991701925 Seg acc:  0.9391629003733544 Disc loss:  0.041577652 Gen loss:  1.0454409\n","Epoch:  728 Seg loss:  0.18277739247839367 Seg acc:  0.9392072213500786 Disc loss:  0.05926067 Gen loss:  0.9159174\n","Epoch:  729 Seg loss:  0.18272851198523296 Seg acc:  0.9392020800089583 Disc loss:  0.06361494 Gen loss:  1.0274378\n","Epoch:  730 Seg loss:  0.18268136283612416 Seg acc:  0.9392109309477217 Disc loss:  0.036413606 Gen loss:  1.116267\n","Epoch:  730 F1 (val):  0.7557251563527311 Acc (val):  0.793\n","Epoch:  731 Seg loss:  0.18259908116732185 Seg acc:  0.9392389513945114 Disc loss:  0.06625878 Gen loss:  1.0426335\n","Epoch:  732 Seg loss:  0.18247730531755205 Seg acc:  0.939265849782536 Disc loss:  0.098217085 Gen loss:  0.9013612\n","Epoch:  733 Seg loss:  0.18254559181661104 Seg acc:  0.9392676170058748 Disc loss:  0.09663461 Gen loss:  1.0180066\n","Epoch:  734 Seg loss:  0.18251316438680287 Seg acc:  0.9392732024689986 Disc loss:  0.078691676 Gen loss:  1.0860085\n","Epoch:  735 Seg loss:  0.1823675059086206 Seg acc:  0.9393266694432876 Disc loss:  0.08354426 Gen loss:  1.1127044\n","Epoch:  736 Seg loss:  0.18223486979917178 Seg acc:  0.9393692463398403 Disc loss:  0.050565824 Gen loss:  0.984403\n","Epoch:  737 Seg loss:  0.18219455851689284 Seg acc:  0.9393684407277159 Disc loss:  0.049969926 Gen loss:  1.1316897\n","Epoch:  738 Seg loss:  0.18207602621356483 Seg acc:  0.9394132653061223 Disc loss:  0.07005708 Gen loss:  1.0390215\n","Epoch:  739 Seg loss:  0.18200669390765675 Seg acc:  0.9394251746706802 Disc loss:  0.053288497 Gen loss:  1.1326905\n","Epoch:  740 Seg loss:  0.18190540012374923 Seg acc:  0.9394601489244346 Disc loss:  0.15060166 Gen loss:  1.0170193\n","Epoch:  740 F1 (val):  0.7561179698529423 Acc (val):  0.7903877551020408\n","Epoch:  741 Seg loss:  0.18180005998927573 Seg acc:  0.9394895205045583 Disc loss:  0.06786308 Gen loss:  1.0896137\n","Epoch:  742 Seg loss:  0.1817774612637302 Seg acc:  0.939501278948237 Disc loss:  0.1265795 Gen loss:  1.0207179\n","Epoch:  743 Seg loss:  0.1816976265091835 Seg acc:  0.9395284560661411 Disc loss:  0.08685676 Gen loss:  0.86465526\n","Epoch:  744 Seg loss:  0.18168912941630008 Seg acc:  0.9395322443493528 Disc loss:  0.12000136 Gen loss:  1.1434213\n","Epoch:  745 Seg loss:  0.18164804335308554 Seg acc:  0.9395408163265306 Disc loss:  0.026872598 Gen loss:  1.1019832\n","Epoch:  746 Seg loss:  0.18152031875227956 Seg acc:  0.9395866389451223 Disc loss:  0.07646194 Gen loss:  1.0345994\n","Epoch:  747 Seg loss:  0.18148528641386524 Seg acc:  0.9395937491462449 Disc loss:  0.1249114 Gen loss:  0.9368486\n","Epoch:  748 Seg loss:  0.18153222699594848 Seg acc:  0.9395827649241515 Disc loss:  0.04832967 Gen loss:  1.0633636\n","Epoch:  749 Seg loss:  0.1814163848409586 Seg acc:  0.9396256232800195 Disc loss:  0.056214966 Gen loss:  0.98821586\n","Epoch:  750 Seg loss:  0.18141218973696233 Seg acc:  0.9396357142857142 Disc loss:  0.17165732 Gen loss:  0.96081764\n","Epoch:  750 F1 (val):  0.7708829513649715 Acc (val):  0.8020918367346939\n","Epoch:  751 Seg loss:  0.18135509017443213 Seg acc:  0.9396580070110601 Disc loss:  0.033390563 Gen loss:  1.0231678\n","Epoch:  752 Seg loss:  0.18124996962402254 Seg acc:  0.939688382001737 Disc loss:  0.14469288 Gen loss:  0.97073793\n","Epoch:  753 Seg loss:  0.18114891068967848 Seg acc:  0.9397247743719003 Disc loss:  0.07404744 Gen loss:  1.1584609\n","Epoch:  754 Seg loss:  0.1810416117340168 Seg acc:  0.9397610702105776 Disc loss:  0.042734254 Gen loss:  1.0084906\n","Epoch:  755 Seg loss:  0.18088781152241276 Seg acc:  0.9398175429112042 Disc loss:  0.06735727 Gen loss:  1.0300103\n","Epoch:  756 Seg loss:  0.1807435322799301 Seg acc:  0.9398704918475327 Disc loss:  0.053446494 Gen loss:  0.89739966\n","Epoch:  757 Seg loss:  0.1807089555600124 Seg acc:  0.9398757851885802 Disc loss:  0.09826709 Gen loss:  1.0828844\n","Epoch:  758 Seg loss:  0.18069184917003153 Seg acc:  0.9398733239997846 Disc loss:  0.064148016 Gen loss:  0.9861622\n","Epoch:  759 Seg loss:  0.18062594169272264 Seg acc:  0.9398843134091581 Disc loss:  0.077966824 Gen loss:  1.1169847\n","Epoch:  760 Seg loss:  0.1804084910790583 Seg acc:  0.9399603920515576 Disc loss:  0.081982896 Gen loss:  0.8952629\n","Epoch:  760 F1 (val):  0.7637223127546796 Acc (val):  0.7933163265306122\n","Epoch:  761 Seg loss:  0.1802067611777712 Seg acc:  0.9400329185550699 Disc loss:  0.096392475 Gen loss:  0.85093296\n","Epoch:  762 Seg loss:  0.18011849116670883 Seg acc:  0.9400667550484761 Disc loss:  0.06902276 Gen loss:  0.9556641\n","Epoch:  763 Seg loss:  0.18000384975044084 Seg acc:  0.9400978281220745 Disc loss:  0.10207619 Gen loss:  0.9708533\n","Epoch:  764 Seg loss:  0.17989883378040805 Seg acc:  0.9401291537557431 Disc loss:  0.066207856 Gen loss:  1.0323875\n","Epoch:  765 Seg loss:  0.1798711113217804 Seg acc:  0.9401293850873683 Disc loss:  0.097174704 Gen loss:  0.9497094\n","Epoch:  766 Seg loss:  0.17979835344638978 Seg acc:  0.9401499307294721 Disc loss:  0.0966006 Gen loss:  0.8887455\n","Epoch:  767 Seg loss:  0.17962584134817822 Seg acc:  0.9402123300428384 Disc loss:  0.10642195 Gen loss:  0.92588365\n","Epoch:  768 Seg loss:  0.1795638378074121 Seg acc:  0.9402423469387755 Disc loss:  0.109170236 Gen loss:  0.9089483\n","Epoch:  769 Seg loss:  0.17940013371989305 Seg acc:  0.9402951752872801 Disc loss:  0.09264782 Gen loss:  0.97044665\n","Epoch:  770 Seg loss:  0.17931896488533006 Seg acc:  0.9403220249138616 Disc loss:  0.06506349 Gen loss:  0.9569567\n","Epoch:  770 F1 (val):  0.7532215615977296 Acc (val):  0.7911326530612245\n","Epoch:  771 Seg loss:  0.17924537117098943 Seg acc:  0.940340863972048 Disc loss:  0.07610887 Gen loss:  0.9841373\n","Epoch:  772 Seg loss:  0.17922667866341113 Seg acc:  0.9403322274505658 Disc loss:  0.13944742 Gen loss:  0.99955857\n","Epoch:  773 Seg loss:  0.17918798628890883 Seg acc:  0.940344404255881 Disc loss:  0.036325503 Gen loss:  1.0076112\n","Epoch:  774 Seg loss:  0.17910972978553955 Seg acc:  0.9403670964509835 Disc loss:  0.10735345 Gen loss:  1.0272154\n","Epoch:  775 Seg loss:  0.17897197823130315 Seg acc:  0.940411784068466 Disc loss:  0.044456515 Gen loss:  0.9456339\n","Epoch:  776 Seg loss:  0.17891360001072057 Seg acc:  0.9404224963181148 Disc loss:  0.09940134 Gen loss:  0.91629624\n","Epoch:  777 Seg loss:  0.17880109711723974 Seg acc:  0.9404637144433063 Disc loss:  0.06934588 Gen loss:  1.04047\n","Epoch:  778 Seg loss:  0.17870853937788841 Seg acc:  0.940498924505535 Disc loss:  0.05253147 Gen loss:  1.0145519\n","Epoch:  779 Seg loss:  0.17854849212541124 Seg acc:  0.9405563123837468 Disc loss:  0.05779676 Gen loss:  0.8986012\n","Epoch:  780 Seg loss:  0.17843864309625365 Seg acc:  0.9405981815803245 Disc loss:  0.13725317 Gen loss:  0.98054713\n","Epoch:  780 F1 (val):  0.7729504612641704 Acc (val):  0.8026938775510204\n","Epoch:  781 Seg loss:  0.17841771147458332 Seg acc:  0.9405916015573963 Disc loss:  0.06276195 Gen loss:  1.1200662\n","Epoch:  782 Seg loss:  0.1784002960580961 Seg acc:  0.9405958035388069 Disc loss:  0.04327976 Gen loss:  1.1292944\n","Epoch:  783 Seg loss:  0.17835357170290342 Seg acc:  0.9406100946125577 Disc loss:  0.08583586 Gen loss:  0.9680057\n","Epoch:  784 Seg loss:  0.17823431367881368 Seg acc:  0.940640943877551 Disc loss:  0.026575528 Gen loss:  0.99923086\n","Epoch:  785 Seg loss:  0.17814223959709807 Seg acc:  0.9406730144287014 Disc loss:  0.0421397 Gen loss:  1.0837927\n","Epoch:  786 Seg loss:  0.1780352401440494 Seg acc:  0.9407040297034845 Disc loss:  0.049451575 Gen loss:  1.0219225\n","Epoch:  787 Seg loss:  0.17793888850267764 Seg acc:  0.9407339937245546 Disc loss:  0.13821046 Gen loss:  0.8995955\n","Epoch:  788 Seg loss:  0.1778877585781218 Seg acc:  0.9407635579612555 Disc loss:  0.07988277 Gen loss:  0.9798717\n","Epoch:  789 Seg loss:  0.1778245535862121 Seg acc:  0.9407943405499083 Disc loss:  0.06785206 Gen loss:  1.1189657\n","Epoch:  790 Seg loss:  0.1778248246782754 Seg acc:  0.9407882330147248 Disc loss:  0.050824635 Gen loss:  1.039716\n","Epoch:  790 F1 (val):  0.7365367057579573 Acc (val):  0.7816020408163266\n","Epoch:  791 Seg loss:  0.17769368947748515 Seg acc:  0.9408366443922702 Disc loss:  0.07931397 Gen loss:  1.0370775\n","Epoch:  792 Seg loss:  0.17761038819262098 Seg acc:  0.9408588435374149 Disc loss:  0.054911546 Gen loss:  0.9764916\n","Epoch:  793 Seg loss:  0.17744877076417695 Seg acc:  0.9409173379313895 Disc loss:  0.095864445 Gen loss:  0.86198956\n","Epoch:  794 Seg loss:  0.17740727899835376 Seg acc:  0.9409384156685344 Disc loss:  0.035786428 Gen loss:  1.0916369\n","Epoch:  795 Seg loss:  0.1772696619677656 Seg acc:  0.940990886920806 Disc loss:  0.042989567 Gen loss:  1.0108862\n","Epoch:  796 Seg loss:  0.1771826347284693 Seg acc:  0.9410172674597477 Disc loss:  0.046734862 Gen loss:  1.0565386\n","Epoch:  797 Seg loss:  0.17716459100516782 Seg acc:  0.9410080531585282 Disc loss:  0.06091167 Gen loss:  1.1790377\n","Epoch:  798 Seg loss:  0.17710986037628917 Seg acc:  0.9410138867577106 Disc loss:  0.050390817 Gen loss:  0.955486\n","Epoch:  799 Seg loss:  0.17700981433376278 Seg acc:  0.9410509948660315 Disc loss:  0.05168701 Gen loss:  0.99640024\n","Epoch:  800 Seg loss:  0.1769403842254542 Seg acc:  0.9410666454081632 Disc loss:  0.09957342 Gen loss:  1.0578104\n","Epoch:  800 F1 (val):  0.7556044499551808 Acc (val):  0.7890714285714285\n","Epoch:  801 Seg loss:  0.17685513696708854 Seg acc:  0.9410997732426304 Disc loss:  0.05612383 Gen loss:  1.0365905\n","Epoch:  802 Seg loss:  0.1766970382603884 Seg acc:  0.9411480864166115 Disc loss:  0.08377776 Gen loss:  0.901955\n","Epoch:  803 Seg loss:  0.17664547585377732 Seg acc:  0.9411648283223626 Disc loss:  0.0986218 Gen loss:  1.0141801\n","Epoch:  804 Seg loss:  0.1765634022185125 Seg acc:  0.9411907300233527 Disc loss:  0.02899857 Gen loss:  1.072903\n","Epoch:  805 Seg loss:  0.17646534637033198 Seg acc:  0.9412292432500952 Disc loss:  0.14248028 Gen loss:  1.018175\n","Epoch:  806 Seg loss:  0.17640226622511893 Seg acc:  0.9412394920747456 Disc loss:  0.043538794 Gen loss:  1.0633879\n","Epoch:  807 Seg loss:  0.17644366038249992 Seg acc:  0.9412196848999823 Disc loss:  0.033483684 Gen loss:  1.2082171\n","Epoch:  808 Seg loss:  0.1763025330411879 Seg acc:  0.9412681223479491 Disc loss:  0.057949778 Gen loss:  1.0541515\n","Epoch:  809 Seg loss:  0.176239250154132 Seg acc:  0.9412940516132288 Disc loss:  0.041203298 Gen loss:  1.0582893\n","Epoch:  810 Seg loss:  0.17619602050816202 Seg acc:  0.9413010204081633 Disc loss:  0.058736898 Gen loss:  1.0109617\n","Epoch:  810 F1 (val):  0.7662005034347876 Acc (val):  0.7971224489795918\n","Epoch:  811 Seg loss:  0.17618406792521552 Seg acc:  0.9412963335765874 Disc loss:  0.05076631 Gen loss:  1.0695415\n","Epoch:  812 Seg loss:  0.1761691084319698 Seg acc:  0.9413010832411782 Disc loss:  0.046976086 Gen loss:  1.1463186\n","Epoch:  813 Seg loss:  0.1761004537650983 Seg acc:  0.9413306097346689 Disc loss:  0.068389036 Gen loss:  1.0749913\n","Epoch:  814 Seg loss:  0.1760418672064269 Seg acc:  0.9413403199117485 Disc loss:  0.049117923 Gen loss:  0.9938994\n","Epoch:  815 Seg loss:  0.17593716768536466 Seg acc:  0.9413725428821836 Disc loss:  0.06394045 Gen loss:  0.9932845\n","Epoch:  816 Seg loss:  0.17580881949249363 Seg acc:  0.9414206307523009 Disc loss:  0.019570624 Gen loss:  1.0463985\n","Epoch:  817 Seg loss:  0.1756936815091964 Seg acc:  0.9414632927834535 Disc loss:  0.064102635 Gen loss:  1.0090828\n","Epoch:  818 Seg loss:  0.175592742065619 Seg acc:  0.9414989895713788 Disc loss:  0.07636364 Gen loss:  1.0544407\n","Epoch:  819 Seg loss:  0.17548685559777769 Seg acc:  0.9415180907527846 Disc loss:  0.043075763 Gen loss:  1.0420277\n","Epoch:  820 Seg loss:  0.17545515896479894 Seg acc:  0.9415181682429068 Disc loss:  0.06928356 Gen loss:  1.016349\n","Epoch:  820 F1 (val):  0.7642457432327132 Acc (val):  0.797265306122449\n","Epoch:  821 Seg loss:  0.17537887042998615 Seg acc:  0.9415381316960401 Disc loss:  0.047696166 Gen loss:  1.0570098\n","Epoch:  822 Seg loss:  0.17532599453390588 Seg acc:  0.9415384949600277 Disc loss:  0.018474504 Gen loss:  1.0546592\n","Epoch:  823 Seg loss:  0.17532491798706146 Seg acc:  0.9415447466957622 Disc loss:  0.047871325 Gen loss:  1.0900887\n","Epoch:  824 Seg loss:  0.17524648143783284 Seg acc:  0.9415627476718843 Disc loss:  0.028937813 Gen loss:  1.1135101\n","Epoch:  825 Seg loss:  0.17510019974952395 Seg acc:  0.9416156462585034 Disc loss:  0.06424126 Gen loss:  0.86250293\n","Epoch:  826 Seg loss:  0.175052930948214 Seg acc:  0.9416288852102586 Disc loss:  0.02492359 Gen loss:  1.148265\n","Epoch:  827 Seg loss:  0.17498957408770036 Seg acc:  0.9416408582780149 Disc loss:  0.03565044 Gen loss:  1.0527022\n","Epoch:  828 Seg loss:  0.17499126906727175 Seg acc:  0.9416349329586906 Disc loss:  0.047525875 Gen loss:  1.0724506\n","Epoch:  829 Seg loss:  0.17488109095491675 Seg acc:  0.9416742571576279 Disc loss:  0.060998842 Gen loss:  0.95991004\n","Epoch:  830 Seg loss:  0.17487545114638936 Seg acc:  0.9416698426358495 Disc loss:  0.043450214 Gen loss:  1.108326\n","Epoch:  830 F1 (val):  0.7639835951281638 Acc (val):  0.7966122448979592\n","Epoch:  831 Seg loss:  0.1748730865980708 Seg acc:  0.9416678945946609 Disc loss:  0.037723243 Gen loss:  1.0847094\n","Epoch:  832 Seg loss:  0.17475070594586073 Seg acc:  0.9417137828689169 Disc loss:  0.055294186 Gen loss:  1.0495781\n","Epoch:  833 Seg loss:  0.17472318608854928 Seg acc:  0.9417114805105716 Disc loss:  0.06738892 Gen loss:  1.0595727\n","Epoch:  834 Seg loss:  0.17464597971453297 Seg acc:  0.9417339597709589 Disc loss:  0.042872805 Gen loss:  0.92643195\n","Epoch:  835 Seg loss:  0.1745531673983721 Seg acc:  0.941763411951607 Disc loss:  0.04133907 Gen loss:  0.93765247\n","Epoch:  836 Seg loss:  0.17449859877977003 Seg acc:  0.9417732643296554 Disc loss:  0.08241433 Gen loss:  1.1101906\n","Epoch:  837 Seg loss:  0.1744491520850711 Seg acc:  0.9417882744495648 Disc loss:  0.07357392 Gen loss:  0.993091\n","Epoch:  838 Seg loss:  0.1743840147454407 Seg acc:  0.9418154254541912 Disc loss:  0.07200213 Gen loss:  0.9968525\n","Epoch:  839 Seg loss:  0.17435120172905197 Seg acc:  0.9418157549074458 Disc loss:  0.05952812 Gen loss:  1.0374755\n","Epoch:  840 Seg loss:  0.17429323940360475 Seg acc:  0.9418415937803695 Disc loss:  0.0603189 Gen loss:  1.0424459\n","Epoch:  840 F1 (val):  0.7690610918320187 Acc (val):  0.7994591836734694\n","Epoch:  841 Seg loss:  0.17416849753439567 Seg acc:  0.9418822344633453 Disc loss:  0.061763242 Gen loss:  0.96093374\n","Epoch:  842 Seg loss:  0.17405925372354422 Seg acc:  0.9419200518687285 Disc loss:  0.020669594 Gen loss:  1.0601279\n","Epoch:  843 Seg loss:  0.17403162432302005 Seg acc:  0.9419229791560754 Disc loss:  0.11613269 Gen loss:  1.0678668\n","Epoch:  844 Seg loss:  0.17401976556565737 Seg acc:  0.9419113913337847 Disc loss:  0.06650767 Gen loss:  1.0490733\n","Epoch:  845 Seg loss:  0.17392423354618294 Seg acc:  0.9419339451757033 Disc loss:  0.04541166 Gen loss:  0.96992314\n","Epoch:  846 Seg loss:  0.1738534318723542 Seg acc:  0.9419751411202779 Disc loss:  0.085209034 Gen loss:  0.9242264\n","Epoch:  847 Seg loss:  0.1738947338920522 Seg acc:  0.9419581114618221 Disc loss:  0.05154863 Gen loss:  1.1438239\n","Epoch:  848 Seg loss:  0.17395224699636325 Seg acc:  0.9419471385252214 Disc loss:  0.045456525 Gen loss:  1.2418866\n","Epoch:  849 Seg loss:  0.17384698886744618 Seg acc:  0.9419818634167448 Disc loss:  0.048001472 Gen loss:  0.97687227\n","Epoch:  850 Seg loss:  0.17375564670518917 Seg acc:  0.9420135054021608 Disc loss:  0.09519082 Gen loss:  0.942228\n","Epoch:  850 F1 (val):  0.7333566257048272 Acc (val):  0.7809081632653061\n","Epoch:  851 Seg loss:  0.17369813962974154 Seg acc:  0.9420306841890692 Disc loss:  0.025177782 Gen loss:  1.1053123\n","Epoch:  852 Seg loss:  0.173630292520819 Seg acc:  0.9420589010251987 Disc loss:  0.052280143 Gen loss:  1.097336\n","Epoch:  853 Seg loss:  0.17348966530250148 Seg acc:  0.9421100796707897 Disc loss:  0.07146104 Gen loss:  0.95729715\n","Epoch:  854 Seg loss:  0.17346542604503154 Seg acc:  0.9421214094537111 Disc loss:  0.08135623 Gen loss:  1.0011142\n","Epoch:  855 Seg loss:  0.1733212100984583 Seg acc:  0.942172395273899 Disc loss:  0.06941524 Gen loss:  0.95050865\n","Epoch:  856 Seg loss:  0.17324394749903999 Seg acc:  0.942196142475682 Disc loss:  0.096078 Gen loss:  0.9301095\n","Epoch:  857 Seg loss:  0.17325251491016588 Seg acc:  0.9421847093563213 Disc loss:  0.05106892 Gen loss:  1.1217427\n","Epoch:  858 Seg loss:  0.17316105310308003 Seg acc:  0.9422152252509395 Disc loss:  0.03726656 Gen loss:  1.070394\n","Epoch:  859 Seg loss:  0.17304200514049484 Seg acc:  0.9422596279489677 Disc loss:  0.05433782 Gen loss:  1.01759\n","Epoch:  860 Seg loss:  0.17298538146627157 Seg acc:  0.942281976744186 Disc loss:  0.05851447 Gen loss:  1.0274992\n","Epoch:  860 F1 (val):  0.768335020065845 Acc (val):  0.8002040816326531\n","Epoch:  861 Seg loss:  0.1728984206097155 Seg acc:  0.9423101993410604 Disc loss:  0.09192961 Gen loss:  0.98659897\n","Epoch:  862 Seg loss:  0.17284598515915692 Seg acc:  0.9423173445712392 Disc loss:  0.043230236 Gen loss:  0.97516006\n","Epoch:  863 Seg loss:  0.1727777716972426 Seg acc:  0.9423339324142171 Disc loss:  0.06004248 Gen loss:  1.0533181\n","Epoch:  864 Seg loss:  0.1727123698111865 Seg acc:  0.9423555012282692 Disc loss:  0.03921477 Gen loss:  1.0367689\n","Epoch:  865 Seg loss:  0.17266254465316416 Seg acc:  0.9423658133773741 Disc loss:  0.049816754 Gen loss:  1.0231318\n","Epoch:  866 Seg loss:  0.1725859150320601 Seg acc:  0.9423864118395627 Disc loss:  0.15519443 Gen loss:  0.9657923\n","Epoch:  867 Seg loss:  0.1725493519187764 Seg acc:  0.9423999011369254 Disc loss:  0.047341917 Gen loss:  1.0923942\n","Epoch:  868 Seg loss:  0.1724863391056279 Seg acc:  0.9424124776638767 Disc loss:  0.11074051 Gen loss:  0.92628896\n","Epoch:  869 Seg loss:  0.17237476198382085 Seg acc:  0.9424593715506917 Disc loss:  0.051380366 Gen loss:  1.0409653\n","Epoch:  870 Seg loss:  0.17236558753654532 Seg acc:  0.9424715575885527 Disc loss:  0.03913899 Gen loss:  1.1164782\n","Epoch:  870 F1 (val):  0.7818213100838068 Acc (val):  0.798704081632653\n","Epoch:  871 Seg loss:  0.17226770572176142 Seg acc:  0.9425039246467818 Disc loss:  0.04886375 Gen loss:  1.0439582\n","Epoch:  872 Seg loss:  0.17218202320696496 Seg acc:  0.9425373876614868 Disc loss:  0.13302216 Gen loss:  0.88346547\n","Epoch:  873 Seg loss:  0.17215598070565163 Seg acc:  0.9425409682773453 Disc loss:  0.07097012 Gen loss:  0.98834765\n","Epoch:  874 Seg loss:  0.17203160884067656 Seg acc:  0.9425856956054733 Disc loss:  0.036473453 Gen loss:  0.95686644\n","Epoch:  875 Seg loss:  0.17193664082246168 Seg acc:  0.9426239067055394 Disc loss:  0.023872506 Gen loss:  1.0264406\n","Epoch:  876 Seg loss:  0.17191897898903924 Seg acc:  0.9426247553816047 Disc loss:  0.041646738 Gen loss:  1.0937123\n","Epoch:  877 Seg loss:  0.17189464818987837 Seg acc:  0.9426197845158588 Disc loss:  0.036929764 Gen loss:  1.0663558\n","Epoch:  878 Seg loss:  0.17186160051053845 Seg acc:  0.9426215076007625 Disc loss:  0.030496787 Gen loss:  1.0992812\n","Epoch:  879 Seg loss:  0.17192646601492703 Seg acc:  0.9425965266652737 Disc loss:  0.028498784 Gen loss:  1.1414347\n","Epoch:  880 Seg loss:  0.17186411248139022 Seg acc:  0.9426174049165121 Disc loss:  0.0659715 Gen loss:  1.0031378\n","Epoch:  880 F1 (val):  0.7675393775228893 Acc (val):  0.7995918367346939\n","Epoch:  881 Seg loss:  0.17178864090171697 Seg acc:  0.9426498181565475 Disc loss:  0.035659656 Gen loss:  1.0482867\n","Epoch:  882 Seg loss:  0.1716854747135589 Seg acc:  0.9426876532926096 Disc loss:  0.061663702 Gen loss:  0.949144\n","Epoch:  883 Seg loss:  0.17164498558932428 Seg acc:  0.9427028682367624 Disc loss:  0.043530352 Gen loss:  1.1094505\n","Epoch:  884 Seg loss:  0.17161316769956125 Seg acc:  0.9427128543725182 Disc loss:  0.05403915 Gen loss:  1.0333593\n","Epoch:  885 Seg loss:  0.17159112090571116 Seg acc:  0.9427179176755448 Disc loss:  0.035392404 Gen loss:  1.0319247\n","Epoch:  886 Seg loss:  0.17154671956257345 Seg acc:  0.9427376537522458 Disc loss:  0.042677402 Gen loss:  1.0094638\n","Epoch:  887 Seg loss:  0.17156836601959072 Seg acc:  0.9427343372523755 Disc loss:  0.03585142 Gen loss:  1.199146\n","Epoch:  888 Seg loss:  0.17144719421284627 Seg acc:  0.9427815889869462 Disc loss:  0.041496214 Gen loss:  0.9393694\n","Epoch:  889 Seg loss:  0.1713477633746825 Seg acc:  0.9428140997681413 Disc loss:  0.049081903 Gen loss:  1.0131328\n","Epoch:  890 Seg loss:  0.1712646290471547 Seg acc:  0.9428436711763357 Disc loss:  0.054457497 Gen loss:  1.0104673\n","Epoch:  890 F1 (val):  0.764879298171925 Acc (val):  0.7971632653061225\n","Epoch:  891 Seg loss:  0.17112003941001114 Seg acc:  0.9428940768226483 Disc loss:  0.031444002 Gen loss:  0.9780831\n","Epoch:  892 Seg loss:  0.17101288131008033 Seg acc:  0.9429343598425918 Disc loss:  0.054775454 Gen loss:  1.0033482\n","Epoch:  893 Seg loss:  0.1710002085617415 Seg acc:  0.9429437004365017 Disc loss:  0.066394 Gen loss:  1.0749127\n","Epoch:  894 Seg loss:  0.1709535217946041 Seg acc:  0.9429535908323061 Disc loss:  0.10902062 Gen loss:  1.133145\n","Epoch:  895 Seg loss:  0.17088303301462582 Seg acc:  0.9429817010603124 Disc loss:  0.018849457 Gen loss:  1.100387\n","Epoch:  896 Seg loss:  0.1707568390141075 Seg acc:  0.9430234147230321 Disc loss:  0.09941882 Gen loss:  0.79520667\n","Epoch:  897 Seg loss:  0.17076732111739673 Seg acc:  0.9430129911496372 Disc loss:  0.03726455 Gen loss:  1.1377804\n","Epoch:  898 Seg loss:  0.17073459654241668 Seg acc:  0.9430199195491115 Disc loss:  0.04981134 Gen loss:  1.0541753\n","Epoch:  899 Seg loss:  0.17075169951477823 Seg acc:  0.9430129281968628 Disc loss:  0.030520447 Gen loss:  1.0935668\n","Epoch:  900 Seg loss:  0.17071595751369992 Seg acc:  0.9430187074829933 Disc loss:  0.028307207 Gen loss:  1.0551777\n","Epoch:  900 F1 (val):  0.7614353332990569 Acc (val):  0.7940408163265306\n","Epoch:  901 Seg loss:  0.17070643577133446 Seg acc:  0.9430196606944665 Disc loss:  0.046604685 Gen loss:  1.0396031\n","Epoch:  902 Seg loss:  0.17065891223503257 Seg acc:  0.9430429544323273 Disc loss:  0.0622682 Gen loss:  1.062158\n","Epoch:  903 Seg loss:  0.17072034192169425 Seg acc:  0.9430040454720094 Disc loss:  0.067121975 Gen loss:  1.0756338\n","Epoch:  904 Seg loss:  0.17062620852200264 Seg acc:  0.9430408501896335 Disc loss:  0.053063504 Gen loss:  0.9837542\n","Epoch:  905 Seg loss:  0.17056683495187958 Seg acc:  0.943061506370504 Disc loss:  0.025754405 Gen loss:  1.0631956\n","Epoch:  906 Seg loss:  0.17048273966431815 Seg acc:  0.9430798643960897 Disc loss:  0.04340361 Gen loss:  1.0108421\n","Epoch:  907 Seg loss:  0.17044883027319543 Seg acc:  0.9430835564655851 Disc loss:  0.016858215 Gen loss:  1.1221094\n","Epoch:  908 Seg loss:  0.17039869919969056 Seg acc:  0.9430990402769037 Disc loss:  0.032199595 Gen loss:  1.1523063\n","Epoch:  909 Seg loss:  0.17040540429387482 Seg acc:  0.9430923194360252 Disc loss:  0.053616166 Gen loss:  1.0811617\n","Epoch:  910 Seg loss:  0.17039703357309757 Seg acc:  0.9430875756896164 Disc loss:  0.04752437 Gen loss:  1.0546389\n","Epoch:  910 F1 (val):  0.7682763422554121 Acc (val):  0.8008979591836735\n","Epoch:  911 Seg loss:  0.1703371759790326 Seg acc:  0.9431100047044064 Disc loss:  0.05954995 Gen loss:  0.912952\n","Epoch:  912 Seg loss:  0.17030780374178695 Seg acc:  0.943110566595059 Disc loss:  0.036661197 Gen loss:  1.0716625\n","Epoch:  913 Seg loss:  0.17018238976312167 Seg acc:  0.9431549947470774 Disc loss:  0.03171299 Gen loss:  0.969367\n","Epoch:  914 Seg loss:  0.17013008087767487 Seg acc:  0.9431700196489974 Disc loss:  0.02192793 Gen loss:  1.0798727\n","Epoch:  915 Seg loss:  0.1700895955552006 Seg acc:  0.9431844541095127 Disc loss:  0.05130408 Gen loss:  1.0761094\n","Epoch:  916 Seg loss:  0.17001714136687363 Seg acc:  0.9432066549327155 Disc loss:  0.074179344 Gen loss:  0.9580729\n","Epoch:  917 Seg loss:  0.1698946769053732 Seg acc:  0.9432493935414951 Disc loss:  0.07923732 Gen loss:  1.0233157\n","Epoch:  918 Seg loss:  0.16982353489423954 Seg acc:  0.9432709194789027 Disc loss:  0.039109528 Gen loss:  1.0778048\n","Epoch:  919 Seg loss:  0.169875815974891 Seg acc:  0.943250205414048 Disc loss:  0.03243158 Gen loss:  1.1270303\n","Epoch:  920 Seg loss:  0.16984706666199084 Seg acc:  0.9432478371783497 Disc loss:  0.06571386 Gen loss:  1.0161377\n","Epoch:  920 F1 (val):  0.7781771712609081 Acc (val):  0.8015102040816326\n","Epoch:  921 Seg loss:  0.16987966285183234 Seg acc:  0.943229132043697 Disc loss:  0.037832353 Gen loss:  1.1461928\n","Epoch:  922 Seg loss:  0.16988012116450874 Seg acc:  0.9432312187347824 Disc loss:  0.10440898 Gen loss:  1.0505987\n","Epoch:  923 Seg loss:  0.16992119564306504 Seg acc:  0.9432073208481658 Disc loss:  0.03108027 Gen loss:  1.1809196\n","Epoch:  924 Seg loss:  0.16983152419788303 Seg acc:  0.9432295807933564 Disc loss:  0.045359008 Gen loss:  1.0207368\n","Epoch:  925 Seg loss:  0.16986449077524043 Seg acc:  0.9432164920022063 Disc loss:  0.061818548 Gen loss:  1.1142728\n","Epoch:  926 Seg loss:  0.16983901397335155 Seg acc:  0.9432006765989333 Disc loss:  0.038558133 Gen loss:  1.1756551\n","Epoch:  927 Seg loss:  0.169838991882169 Seg acc:  0.9432027827312155 Disc loss:  0.025888879 Gen loss:  1.1273524\n","Epoch:  928 Seg loss:  0.16970776134082277 Seg acc:  0.9432513414848698 Disc loss:  0.031015307 Gen loss:  1.0102383\n","Epoch:  929 Seg loss:  0.16965074481320266 Seg acc:  0.9432682168669405 Disc loss:  0.039943915 Gen loss:  1.1089603\n","Epoch:  930 Seg loss:  0.16957359645836134 Seg acc:  0.9432960280886549 Disc loss:  0.110808805 Gen loss:  0.9188369\n","Epoch:  930 F1 (val):  0.764846772927388 Acc (val):  0.7927448979591837\n","Epoch:  931 Seg loss:  0.1695053743420591 Seg acc:  0.9433237795655318 Disc loss:  0.044671334 Gen loss:  1.0011154\n","Epoch:  932 Seg loss:  0.16943198087717137 Seg acc:  0.9433457234825261 Disc loss:  0.04126014 Gen loss:  1.0501014\n","Epoch:  933 Seg loss:  0.16945836965557656 Seg acc:  0.9433383642846206 Disc loss:  0.067796275 Gen loss:  1.096777\n","Epoch:  934 Seg loss:  0.16938135080733507 Seg acc:  0.9433648887820654 Disc loss:  0.05474763 Gen loss:  1.016973\n","Epoch:  935 Seg loss:  0.16933900776274383 Seg acc:  0.9433766233766234 Disc loss:  0.045218926 Gen loss:  1.0821879\n","Epoch:  936 Seg loss:  0.16934667822395444 Seg acc:  0.9433651665794525 Disc loss:  0.025540365 Gen loss:  1.118835\n","Epoch:  937 Seg loss:  0.16930096754913587 Seg acc:  0.9433752423061007 Disc loss:  0.106112465 Gen loss:  0.99331367\n","Epoch:  938 Seg loss:  0.1691971589177688 Seg acc:  0.9434244593359733 Disc loss:  0.098278806 Gen loss:  0.8682791\n","Epoch:  939 Seg loss:  0.16916646309498465 Seg acc:  0.9434341787833345 Disc loss:  0.055640593 Gen loss:  1.038822\n","Epoch:  940 Seg loss:  0.16906572289645988 Seg acc:  0.9434702019105514 Disc loss:  0.050129965 Gen loss:  0.9565411\n","Epoch:  940 F1 (val):  0.7679634553028645 Acc (val):  0.7991224489795918\n","Epoch:  941 Seg loss:  0.16905739670075026 Seg acc:  0.9434681949294065 Disc loss:  0.034460578 Gen loss:  1.109997\n","Epoch:  942 Seg loss:  0.16912691134785454 Seg acc:  0.9434434442566836 Disc loss:  0.025213115 Gen loss:  1.1891611\n","Epoch:  943 Seg loss:  0.1690276271563857 Seg acc:  0.9434823186963015 Disc loss:  0.052052952 Gen loss:  1.0189016\n","Epoch:  944 Seg loss:  0.1689830575300128 Seg acc:  0.9434986812521619 Disc loss:  0.0433715 Gen loss:  0.9814526\n","Epoch:  945 Seg loss:  0.1689704051034318 Seg acc:  0.9434936831875608 Disc loss:  0.03628353 Gen loss:  1.1456892\n","Epoch:  946 Seg loss:  0.16892088577348197 Seg acc:  0.9435032575398025 Disc loss:  0.03808584 Gen loss:  1.0770074\n","Epoch:  947 Seg loss:  0.16879176033942034 Seg acc:  0.9435454065469905 Disc loss:  0.03643474 Gen loss:  1.0253731\n","Epoch:  948 Seg loss:  0.16879444042189273 Seg acc:  0.943528803926634 Disc loss:  0.016429624 Gen loss:  1.170154\n","Epoch:  949 Seg loss:  0.16869455695474173 Seg acc:  0.9435630416550183 Disc loss:  0.037531458 Gen loss:  0.9662711\n","Epoch:  950 Seg loss:  0.16862257627476201 Seg acc:  0.9435835123523094 Disc loss:  0.068456054 Gen loss:  0.97941107\n","Epoch:  950 F1 (val):  0.7613791176786615 Acc (val):  0.7975102040816326\n","Epoch:  951 Seg loss:  0.1685480301285047 Seg acc:  0.9436085001824074 Disc loss:  0.014759872 Gen loss:  1.0394769\n","Epoch:  952 Seg loss:  0.16857225793225988 Seg acc:  0.9435897573315041 Disc loss:  0.035968 Gen loss:  1.1170301\n","Epoch:  953 Seg loss:  0.16859461019423488 Seg acc:  0.9435772105274429 Disc loss:  0.026034256 Gen loss:  1.1676869\n","Epoch:  954 Seg loss:  0.16852612636663192 Seg acc:  0.9436005219697942 Disc loss:  0.04792788 Gen loss:  1.0275294\n","Epoch:  955 Seg loss:  0.16840325127604433 Seg acc:  0.9436432845389464 Disc loss:  0.028401913 Gen loss:  1.0173373\n","Epoch:  956 Seg loss:  0.16829219152059827 Seg acc:  0.9436814213132952 Disc loss:  0.038961664 Gen loss:  0.9725502\n","Epoch:  957 Seg loss:  0.16823960278015837 Seg acc:  0.9436976201138761 Disc loss:  0.053581335 Gen loss:  1.0684472\n","Epoch:  958 Seg loss:  0.1681492816606276 Seg acc:  0.9437284308295343 Disc loss:  0.03206957 Gen loss:  1.0064844\n","Epoch:  959 Seg loss:  0.16803462915433273 Seg acc:  0.9437655614904982 Disc loss:  0.031998377 Gen loss:  0.98991454\n","Epoch:  960 Seg loss:  0.16799315566701503 Seg acc:  0.9437699298469387 Disc loss:  0.035072915 Gen loss:  1.0324818\n","Epoch:  960 F1 (val):  0.761751453842367 Acc (val):  0.7968061224489796\n","Epoch:  961 Seg loss:  0.16793395606965664 Seg acc:  0.9437822527554206 Disc loss:  0.03442449 Gen loss:  1.0663519\n","Epoch:  962 Seg loss:  0.16782391706593877 Seg acc:  0.9438218634647206 Disc loss:  0.04493765 Gen loss:  1.0231224\n","Epoch:  963 Seg loss:  0.16778783430756375 Seg acc:  0.9438343717549326 Disc loss:  0.13248631 Gen loss:  1.002402\n","Epoch:  964 Seg loss:  0.16772015492675277 Seg acc:  0.9438619379286985 Disc loss:  0.02250901 Gen loss:  1.1461651\n","Epoch:  965 Seg loss:  0.16763145448977773 Seg acc:  0.9438931479327483 Disc loss:  0.035249837 Gen loss:  1.0093126\n","Epoch:  966 Seg loss:  0.1675923608540409 Seg acc:  0.9439021105336545 Disc loss:  0.030447787 Gen loss:  1.006594\n","Epoch:  967 Seg loss:  0.1675688209803823 Seg acc:  0.9439060422514404 Disc loss:  0.044127338 Gen loss:  1.1010174\n","Epoch:  968 Seg loss:  0.16756120649905307 Seg acc:  0.9439123376623377 Disc loss:  0.03106782 Gen loss:  1.1151811\n","Epoch:  969 Seg loss:  0.16756824401117884 Seg acc:  0.9438886080748089 Disc loss:  0.032584418 Gen loss:  1.0734192\n","Epoch:  970 Seg loss:  0.1675867823691866 Seg acc:  0.943875447086051 Disc loss:  0.030476224 Gen loss:  1.1200938\n","Epoch:  970 F1 (val):  0.7667998748893927 Acc (val):  0.796\n","Epoch:  971 Seg loss:  0.1674971450312038 Seg acc:  0.9438983059753253 Disc loss:  0.015331253 Gen loss:  0.9988844\n","Epoch:  972 Seg loss:  0.16739069054347805 Seg acc:  0.9439392269253382 Disc loss:  0.044463553 Gen loss:  1.0059747\n","Epoch:  973 Seg loss:  0.16730067631055554 Seg acc:  0.943979015038698 Disc loss:  0.033606865 Gen loss:  0.98460495\n","Epoch:  974 Seg loss:  0.16721155148886863 Seg acc:  0.944008244981771 Disc loss:  0.044529162 Gen loss:  0.9652919\n","Epoch:  975 Seg loss:  0.1671295472081655 Seg acc:  0.9440376766091052 Disc loss:  0.031696998 Gen loss:  1.0835025\n","Epoch:  976 Seg loss:  0.16710577321383857 Seg acc:  0.9440385580461693 Disc loss:  0.054787308 Gen loss:  0.9570058\n","Epoch:  977 Seg loss:  0.1671011202140285 Seg acc:  0.9440310822384226 Disc loss:  0.06152392 Gen loss:  1.0744349\n","Epoch:  978 Seg loss:  0.16701291107090765 Seg acc:  0.9440572701473229 Disc loss:  0.029806262 Gen loss:  1.0340697\n","Epoch:  979 Seg loss:  0.166944179202566 Seg acc:  0.9440774113526922 Disc loss:  0.061341204 Gen loss:  0.9748569\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S4UPXypC9z35","colab_type":"code","colab":{}},"source":["matplotlib.use('TkAgg')\n","import itertools"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNnpurUG9z37","colab_type":"code","colab":{}},"source":["fig, ax1 = plt.subplots()\n","\n","ax1.set_xlabel('epoch')\n","ax1.set_ylabel('Batch Segmentation Cost', color='red')\n","ln1 = ax1.plot(seg_cost_epoch, color='red', label='SegCost')\n","ax1.tick_params(axis='y', labelcolor='red')\n","\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","ax2.set_ylabel('F1 Validation Score', color='blue')  # we already handled the x-label with ax1\n","ln2 = ax2.plot(seg_acc_epoch, color ='green', label = 'Seg Acc')\n","#D: this is a hacky way of getting the validation data scores that are only calculated every few epochs to show properly on this graph\n","# it basically just repeats every value val_step_update number of times, so that it is flat for val_step_update epochs and thus works on the same x axis\n","ln3 = ax2.plot([i for b in map(lambda x:[x] if not isinstance(x, list) else x, [list(itertools.repeat(x,val_step_update)) for x in val_f1_epoch]) for i in b], color='blue', label = \"F1score\")\n","ln4 = ax2.plot([i for b in map(lambda x:[x] if not isinstance(x, list) else x, [list(itertools.repeat(x,val_step_update)) for x in val_acc_epoch]) for i in b], color=\"orange\", label = \"Valid. Acc.\")\n","ax2.tick_params(axis='y', labelcolor='blue')\n","\n","fig.legend(loc=\"center right\", bbox_to_anchor=(1,0.9), bbox_transform=ax1.transAxes)\n","\n","plt.title(\"Tree GAN Training (10k Epochs)\")\n","\n","fig.tight_layout()  # otherwise the right y-label is slightly clipped\n","#plt.savefig('../data/FES Team/Naive CCN Training.png')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"95D2tMnc9z39","colab_type":"code","outputId":"b902a5d2-597c-4edb-af68-54bafa2c89a6","colab":{}},"source":["np.argmax(val_acc_epoch)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["196"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"mvmrUpv39z3_","colab_type":"code","outputId":"3fc1d4cf-035f-4803-adf1-7e212eb7aaa9","colab":{}},"source":["val_acc_epoch[196]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7835"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"pEZuOgF-9z4B","colab_type":"code","outputId":"a91200dd-c081-415f-bf85-7a8b9ff34f86","colab":{}},"source":["fig2, ax1 = plt.subplots()\n","\n","ax1.set_xlabel('epoch')\n","ax1.set_ylabel('Batch Generator Loss', color='red')\n","ln1 = ax1.plot(g_loss_epoch, color='red', label='Gen Loss')\n","ax1.tick_params(axis='y', labelcolor='red')\n","\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","ax2.set_ylabel('Batch Discrimanotr Loss', color='blue')  # we already handled the x-label with ax1\n","ln2 = ax2.plot(d_loss_epoch, color ='green', label = 'Disc Loss')\n","ax2.tick_params(axis='y', labelcolor='blue')\n","\n","fig2.legend(loc=\"center right\", bbox_to_anchor=(1,0.9), bbox_transform=ax1.transAxes)\n","\n","plt.title(\"GAN Losses\")\n","\n","fig2.tight_layout()  # otherwise the right y-label is slightly clipped\n","#plt.savefig('../data/FES Team/Naive CCN Training.png')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'epoch')"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Batch Generator Loss')"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'Batch Discrimanotr Loss')"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x151287bd0>"]},"metadata":{"tags":[]},"execution_count":23},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'GAN Losses')"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ITKtK1haQdKG","colab":{}},"source":["import tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vOCLtLtAQdKJ","colab":{}},"source":["logs_path"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W6loZeYJQdKW","colab":{}},"source":["%load_ext tensorboard.notebook\n","%tensorboard --logdir data_aug_seg/models/dabes/tflogs/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qkl7qlDNQdKZ","colab":{}},"source":["#########################\n","# To compute inference on test images on the model that yields best dice score on validation images\n","f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir,orig_img_dt,test_list,struct_name)\n","#########################\n","# To plot the generated augmented images from the trained deformation cGAN\n","for j in range(0,5):\n","    z_samples,ld_img_batch,unld_img_batch=get_samples(train_imgs,unlabeled_imgs)\n","    save_dir_tmp=str(save_dir)+'/ep_best_model/'\n","    plt_func(sess_new,ae,save_dir_tmp,z_samples,ld_img_batch,unld_img_batch,index=j)\n","######################################\n","#D: we will have to put back some of these validation data references\n","# To compute inference on validation images on the best model\n","#save_dir_tmp=str(save_dir)+'/val_imgs/'\n","#f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir_tmp,orig_img_dt,val_list,struct_name)\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cI-JoaIAOeFl"},"source":["# Train Additive Intensity Field GAN"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gGmOCYBzOi3g","colab":{}},"source":["ra_en_val=params.ra_en\n","if(params.ra_en==1):\n","    params.ra_en=True\n","else:\n","    params.ra_en=False\n","\n","\n","if params.dataset == 'acdc':\n","    #print('load acdc configs')\n","    import experiment_init.init_acdc as cfg\n","    import experiment_init.data_cfg_acdc as data_list\n","else:\n","    raise ValueError(params.dataset)\n","\n","######################################\n","# class loaders\n","# ####################################\n","#  load dataloader object\n","from dataloaders import dataloaderObj\n","dt = dataloaderObj(cfg)\n","\n","if params.dataset == 'acdc':\n","    #print('set acdc orig img dataloader handle')\n","    orig_img_dt=dt.load_acdc_imgs\n","\n","#  load model object\n","from models import modelObj\n","model = modelObj(cfg)\n","#  load f1_utils object\n","from f1_utils import f1_utilsObj\n","f1_util = f1_utilsObj(cfg,dt)\n","\n","######################################\n","#define save_dir for the model\n","save_dir=str(cfg.base_dir)+'/models/'+str(params.dataset)+'/tr_intensity_cgan_unet/ra_en_'+str(ra_en_val)+'_gantype_'+str(params.gan_type)+'/'\n","\n","if(params.data_aug_seg==0):\n","    save_dir=str(save_dir)+'no_data_aug/'\n","    cfg.aug_en=params.data_aug_seg\n","else:\n","    save_dir=str(save_dir)+'with_data_aug/'\n","\n","save_dir=str(save_dir)+'lamda_dsc_'+str(params.lamda_dsc)+'_lamda_adv_'+str(params.lamda_adv)+'_lamda_i_'+str(params.lamda_l1_i)+'/'+\\\n","         str(params.no_of_tr_imgs)+'/'+str(params.comb_tr_imgs)+'_v'+str(params.ver)+\\\n","         '/unet_model_beta1_'+str(params.beta_val)+'_lr_seg_'+str(params.lr_seg)+'_lr_gen_'+str(params.lr_gen)+'_lr_disc_'+str(params.lr_disc)+'/'\n","\n","print('save_dir',save_dir)\n","######################################\n","\n","######################################\n","# load train and val images\n","train_list = data_list.train_data(params.no_of_tr_imgs,params.comb_tr_imgs)\n","# load train data cropped images directly\n","print('loading train imgs')\n","train_imgs,train_labels = dt.load_img_labels(train_list)\n","\n","if(params.no_of_tr_imgs=='tr1'):\n","    train_imgs_copy=np.copy(train_imgs)\n","    train_labels_copy=np.copy(train_labels)\n","    while(train_imgs.shape[2]<cfg.batch_size):\n","        train_imgs=np.concatenate((train_imgs,train_imgs_copy),axis=2)\n","        train_labels=np.concatenate((train_labels,train_labels_copy),axis=2)\n","    del train_imgs_copy,train_labels_copy\n","\n","val_list = data_list.val_data()\n","# load both val data and its cropped images\n","print('loading val imgs')\n","val_label_orig,val_img_crop,val_label_crop,pixel_val_list=load_val_imgs(val_list,dt,orig_img_dt)\n","\n","# load unlabeled images\n","unl_list = data_list.unlabeled_data()\n","print('loading unlabeled imgs')\n","unlabeled_imgs=dt.load_img_labels(unl_list,label_present=0)\n","\n","# get test list\n","print('get test imgs list')\n","test_list = data_list.test_data()\n","struct_name=cfg.struct_name\n","val_step_update=cfg.val_step_update\n","######################################\n","\n","######################################\n","\n","def get_samples(labeled_imgs,unlabeled_imgs):\n","    # sample z vectors from Gaussian Distribution\n","    z_samples = np.random.normal(loc=0.0, scale=1.0, size=(cfg.batch_size, params.z_lat_dim)).astype(np.float32)\n","\n","    #sample Unlabeled data shuffled batch\n","    unld_img_batch=shuffle_minibatch([unlabeled_imgs],batch_size=int(cfg.batch_size),num_channels=cfg.num_channels,labels_present=0,axis=2)\n","\n","    #sample Labelled data shuffled batch\n","    ld_img_batch=shuffle_minibatch([labeled_imgs],batch_size=int(cfg.batch_size),num_channels=cfg.num_channels,labels_present=0,axis=2)\n","\n","    return z_samples,ld_img_batch,unld_img_batch\n","\n","def plt_func(sess,ae,save_dir,z_samples,ld_img_batch,unld_img_batch,index=0):\n","    # plot intensity transformed images for an fixed input image and different sampled z values\n","    ld_img_tmp=np.zeros_like(ld_img_batch)\n","    # select one 2D image from the batch and apply different z's sampled over this selected image\n","    for i in range(0,20):\n","        ld_img_tmp[i,:,:,0]=ld_img_batch[index,:,:,0]\n","\n","    int_vec,y_int_deformed,z_cost=sess.run([ae['int_c1'],ae['y_int'],ae['z_cost']], feed_dict={ae['x']: ld_img_tmp, ae['z']:z_samples,\\\n","                          ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: False})\n","\n","    f1_util.plot_intensity_transformed_imgs(ld_img_tmp,y_int_deformed,int_vec,save_dir,index=index)\n","\n","    # Plot gif of all the transformed images generated for the fixed input image\n","    #f1_util.write_gif_func(ip_img=y_int_deformed, imsize=(cfg.img_size_x,cfg.img_size_y),save_dir=save_dir,index=index)\n","\n","######################################\n","# Define checkpoint file to save CNN architecture and learnt hyperparameters\n","checkpoint_filename='unet_'+str(params.dataset)\n","logs_path = str(save_dir)+'tensorflow_logs/'\n","best_model_dir=str(save_dir)+'best_model/'\n","######################################\n","\n","######################################\n","# Define additive intensity field generator model graph\n","ae = model.intensity_transform_cgan_unet(learn_rate_gen=params.lr_gen,learn_rate_disc=params.lr_disc,\\\n","                        beta1_val=params.beta_val,gan_type=params.gan_type,ra_en=params.ra_en,\\\n","                        learn_rate_seg=params.lr_seg,dsc_loss=params.dsc_loss,en_1hot=params.en_1hot,\\\n","                        lamda_dsc=params.lamda_dsc,lamda_adv=params.lamda_adv,lamda_l1_i=params.lamda_l1_i)\n","\n","######################################\n","#  training parameters\n","start_epoch=0\n","n_epochs = 10000\n","disp_step=400\n","print_step=2000\n","# no of iterations to train just the segmentation network using the labeled data without any cGAN generated data\n","seg_tr_limit=400\n","mean_f1_val_prev=0.1\n","threshold_f1=0.00001\n","pathlib.Path(best_model_dir).mkdir(parents=True, exist_ok=True)\n","######################################\n","\n","######################################\n","# define graph to compute 1 hot encoding for an input label\n","df_ae= model.deform_net()\n","######################################\n","\n","######################################\n","#writer for train summary\n","train_writer = tf.summary.FileWriter(logs_path)\n","#writer for dice score and val summary\n","dsc_writer = tf.summary.FileWriter(logs_path)\n","val_sum_writer = tf.summary.FileWriter(logs_path)\n","######################################\n","\n","######################################\n","# create a session and initialize variable to use the graph\n","sess = tf.Session(config=config)\n","sess.run(tf.global_variables_initializer())\n","# Save training data\n","saver = tf.train.Saver(max_to_keep=2)\n","######################################\n","\n","# Run for n_epochs\n","for epoch_i in range(start_epoch,n_epochs):\n","\n","    # sample z's from Gaussian Distribution\n","    z_samples = np.random.normal(loc=0.0, scale=1.0, size=(cfg.batch_size, params.z_lat_dim)).astype(np.float32)\n","\n","    # sample Unlabeled shuffled batch\n","    unld_img_batch=shuffle_minibatch([unlabeled_imgs],batch_size=int(cfg.batch_size),num_channels=cfg.num_channels,labels_present=0,axis=2)\n","\n","    # sample Labeled shuffled batch\n","    ld_img_batch,ld_label_batch=shuffle_minibatch([train_imgs,train_labels],batch_size=cfg.batch_size,num_channels=cfg.num_channels,axis=2)\n","\n","    if(cfg.aug_en==1):\n","        # Apply affine transformations\n","        ld_img_batch,ld_label_batch=augmentation_function([ld_img_batch,ld_label_batch],dt)\n","        unld_img_batch=augmentation_function([unld_img_batch],dt,labels_present=0)\n","\n","    ld_img_batch_tmp=np.copy(ld_img_batch)\n","    # Compute 1 hot encoding of the segmentation mask labels\n","    ld_label_batch_1hot = sess.run(df_ae['y_tmp_1hot'],feed_dict={df_ae['y_tmp']:ld_label_batch})\n","\n","    if(epoch_i>=seg_tr_limit):\n","        # sample the batch of images and apply deformation field generated by the Generator network on these which are used for the remaining 9500 epochs\n","        # Batch comprosed of both deformed image,label pairs and original affine transformed image, label pairs\n","        # Here, the labels do not change on application of intensity transformation since it is an additive operation\n","        ld_label_batch_tmp=np.copy(ld_label_batch)\n","        ###########################\n","        # use additive intensity field cGAN to generate additional augmented image,label pairs from labeled samples\n","        _,ld_img_batch=sess.run([ae['int_c1'],ae['y_int']],\\\n","                                    feed_dict={ae['x']: ld_img_batch_tmp, ae['z']:z_samples, ae['train_phase']: False})\n","        ld_label_batch=ld_label_batch_1hot\n","\n","        ###########################\n","        # shuffle the quantity/number of images chosen from intensity field cGAN augmented images and rest are original images with conventional affine transformations\n","        no_orig=np.random.randint(5, high=15)\n","        ld_img_batch[0:no_orig] = ld_img_batch_tmp[0:no_orig]\n","        if(params.en_1hot==1):\n","            ld_label_batch = ld_label_batch_1hot\n","        else:\n","            ld_label_batch = ld_label_batch_tmp\n","\n","        #Pick equal number of images from each category\n","        # ld_img_batch[0:10]=ld_img_batch_tmp[0:10]\n","        # ld_label_batch[0:10]=ld_label_batch_1hot[0:10]\n","\n","    elif(epoch_i<seg_tr_limit):\n","        # sample only labeled data batches to optimize only Segmentation Network for initial 500 epochs\n","        ld_img_batch=ld_img_batch\n","        unld_img_batch=unld_img_batch\n","        ld_label_batch=ld_label_batch_1hot\n","\n","    if(epoch_i<seg_tr_limit):\n","        #Optimize only Segmentation Network for initial 500 epochs\n","        train_summary,_ =sess.run([ae['seg_summary'],ae['optimizer_unet_seg']], feed_dict={ae['x']: ld_img_batch, ae['y_l']: ld_label_batch,\\\n","                                   ae['select_mask']: False, ae['train_phase']: True})\n","\n","    if(epoch_i>seg_tr_limit):\n","        #Optimize Generator (G), Discriminator (D) and Segmentation (S) networks for the remaining 9500 epochs\n","\n","        # update both Generator and Segmentation Net parameters in the framework using total loss value\n","        train_summary,_ =sess.run([ae['train_summary'],ae['optimizer_l2_both_gen_unet']], feed_dict={ae['x']: ld_img_batch,ae['y_l']: ld_label_batch,\\\n","                                   ae['z']:z_samples, ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: True})\n","\n","        # update Discriminator Net (D) parameters in the setup using only discriminator loss\n","        train_summary,_ =sess.run([ae['train_summary'],ae['optimizer_disc']], feed_dict={ae['x']: ld_img_batch,ae['z']:z_samples,\\\n","                              ae['y_l']: ld_label_batch,ae['x_unl']: unld_img_batch, ae['select_mask']: True, ae['train_phase']: True})\n","\n","    if(epoch_i%val_step_update==0):\n","        train_writer.add_summary(train_summary, epoch_i)\n","        train_writer.flush()\n","\n","    if(epoch_i%val_step_update==0):\n","        ##Save the model with best DSC for Validation Image\n","        mean_f1_arr=[]\n","        f1_arr=[]\n","        for val_id_no in range(0,len(val_list)):\n","            val_img_crop_tmp=val_img_crop[val_id_no]\n","            val_label_crop_tmp=val_label_crop[val_id_no]\n","            val_label_orig_tmp=val_label_orig[val_id_no]\n","            pixel_size_val=pixel_val_list[val_id_no]\n","\n","            # Compute segmentation mask and dice_score for each validation subject\n","            pred_sf_mask = f1_util.calc_pred_sf_mask_full(sess, ae, val_img_crop_tmp)\n","            re_pred_mask_sys,f1_val = f1_util.reshape_img_and_f1_score(pred_sf_mask, val_label_orig_tmp, pixel_size_val)\n","\n","            #concatenate dice scores of each val image\n","            mean_f1_arr.append(np.mean(f1_val[1:cfg.num_classes]))\n","            f1_arr.append(f1_val[1:cfg.num_classes])\n","\n","        #avg mean over 2 val subjects\n","        mean_f1_arr = np.asarray(mean_f1_arr)\n","        mean_f1 = np.mean(mean_f1_arr)\n","        f1_arr = np.asarray(f1_arr)\n","\n","        if ((epoch_i%disp_step == 0) or (epoch_i==n_epochs-1)):\n","            print('mean_f1',epoch_i, mean_f1)\n","        if (mean_f1-mean_f1_val_prev>threshold_f1 and epoch_i!=start_epoch):\n","            print(\"prev f1_val; present_f1_val\", mean_f1_val_prev, mean_f1, mean_f1_arr)\n","            mean_f1_val_prev = mean_f1\n","            # to save the best model with maximum dice score over the entire n_epochs\n","            print(\"best model saved at epoch no. \", epoch_i)\n","            mp_best = str(best_model_dir) + str(checkpoint_filename) + '_best_model_epoch_' + str(epoch_i) + '.ckpt'\n","            saver.save(sess, mp_best)\n","\n","        #calc. and save validation image dice summary\n","        dsc_summary_msg = sess.run(ae['val_dsc_summary'], feed_dict={ae['rv_dice']:np.mean(f1_arr[:,0]),\\\n","                                ae['myo_dice']:np.mean(f1_arr[:,1]),ae['lv_dice']:np.mean(f1_arr[:,2]),ae['mean_dice']: mean_f1})\n","\n","    if ((epoch_i==n_epochs-1) and (epoch_i != start_epoch)):\n","        # model saved at last epoch\n","        mp = str(save_dir) + str(checkpoint_filename) + '_epochs_' + str(epoch_i) + '.ckpt'\n","        saver.save(sess, mp)\n","        try:\n","            mp_best\n","        except NameError:\n","            mp_best=mp\n","\n","sess.close()\n","######################################\n","# restore best model and predict segmentations on test subjects\n","saver_new = tf.train.Saver()\n","sess_new = tf.Session(config=config)\n","saver_new.restore(sess_new, mp_best)\n","print(\"best model chkpt\",mp_best)\n","print(\"Model restored\")\n","\n","#########################\n","# To compute inference on test images on the model that yields best dice score on validation images\n","f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir,orig_img_dt,test_list,struct_name)\n","#########################\n","# To plot the generated augmented images from the trained deformation cGAN\n","for j in range(0,5):\n","    z_samples,ld_img_batch,unld_img_batch=get_samples(train_imgs,unlabeled_imgs)\n","    save_dir_tmp=str(save_dir)+'/ep_best_model/'\n","    plt_func(sess_new,ae,save_dir_tmp,z_samples,ld_img_batch,unld_img_batch,index=j)\n","######################################\n","# To compute inference on validation images on the best model\n","save_dir_tmp=str(save_dir)+'/val_imgs/'\n","f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir_tmp,orig_img_dt,val_list,struct_name)\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Htfi8elnNp4c"},"source":["# Train Unet with trained GANs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W_67bO43M4_R","colab":{}},"source":["ra_en_val=params.ra_en\n","if(params.ra_en==1):\n","    params.ra_en=True\n","else:\n","    params.ra_en=False\n","\n","if params.dataset == 'acdc':\n","    print('load acdc configs')\n","    import experiment_init.init_acdc as cfg\n","    import experiment_init.data_cfg_acdc as data_list\n","else:\n","    raise ValueError(params.dataset)\n","\n","######################################\n","# class loaders\n","# ####################################\n","#  load dataloader object\n","from dataloaders import dataloaderObj\n","dt = dataloaderObj(cfg)\n","\n","if params.dataset == 'acdc':\n","    print('set acdc img dataloader handle')\n","    orig_img_dt=dt.load_acdc_imgs\n","\n","#  load model object\n","from models import modelObj\n","model = modelObj(cfg)\n","\n","#  load f1_utils object\n","from f1_utils import f1_utilsObj\n","f1_util = f1_utilsObj(cfg,dt)\n","\n","######################################\n","#define save_dir for the model\n","proj_save_name='tr_deform_and_int_cgans_data_aug/ra_en_'+str(ra_en_val)+'_gantype_'+str(params.gan_type)\n","\n","if(params.data_aug_seg==0):\n","    save_dir=str(cfg.base_dir)+'/models/'+str(params.dataset)+'/'+str(proj_save_name)+'/no_data_aug/'\n","    cfg.aug_en=params.data_aug_seg\n","else:\n","    save_dir=str(cfg.base_dir)+'/models/'+str(params.dataset)+'/'+str(proj_save_name)+'/with_data_aug/'\n","\n","save_dir=str(save_dir)+'lamda_dsc_'+str(params.lamda_dsc)+'_lamda_adv_'+str(params.lamda_adv)+\\\n","         '_lamda_g_'+str(params.lamda_l1_g)+'_lamda_i_'+str(params.lamda_l1_i)+\\\n","         '/'+str(params.no_of_tr_imgs)+'/'+str(params.comb_tr_imgs)+'_v'+str(params.ver)+'/unet_model_dsc_loss_'+str(params.dsc_loss)+'_lr_seg_'+str(params.lr_seg)+'/'\n","print('save_dir',save_dir)\n","######################################\n","\n","######################################\n","# load train and val images\n","train_list = data_list.train_data(params.no_of_tr_imgs,params.comb_tr_imgs)\n","#print(train_list)\n","#load train data cropped images directly\n","print('loading train imgs')\n","train_imgs,train_labels = dt.load_img_labels(train_list)\n","\n","if(params.no_of_tr_imgs=='tr1'):\n","    train_imgs_copy=np.copy(train_imgs)\n","    train_labels_copy=np.copy(train_labels)\n","    while(train_imgs.shape[2]<cfg.batch_size):\n","        train_imgs=np.concatenate((train_imgs,train_imgs_copy),axis=2)\n","        train_labels=np.concatenate((train_labels,train_labels_copy),axis=2)\n","    del train_imgs_copy,train_labels_copy\n","\n","val_list = data_list.val_data()\n","#print(val_list)\n","#load both val data and its cropped images\n","print('loading val imgs')\n","val_label_orig,val_img_crop,val_label_crop,pixel_val_list=load_val_imgs(val_list,dt,orig_img_dt)\n","#print(pixel_val_list)\n","\n","# get test list\n","print('get test imgs list')\n","test_list = data_list.test_data()\n","struct_name=cfg.struct_name\n","val_step_update=cfg.val_step_update\n","######################################\n","\n","######################################\n","# Define checkpoint file to save CNN architecture and learnt hyperparameters\n","checkpoint_filename='unet_'+str(params.dataset)\n","logs_path = str(save_dir)+'tensorflow_logs/'\n","best_model_dir=str(save_dir)+'best_model/'\n","######################################\n","\n","########################################################################\n","#load deformation field generator net\n","########################################################################\n","# Define the model graph\n","tf.reset_default_graph()\n","ae_geo = model.spatial_generator_cgan_unet(learn_rate_gen=params.lr_gen,learn_rate_disc=params.lr_disc,\\\n","                        beta1_val=params.beta_val,gan_type=params.gan_type,ra_en=params.ra_en,\\\n","                        learn_rate_seg=params.lr_seg,dsc_loss=params.dsc_loss,en_1hot=params.en_1hot,\\\n","                        lamda_dsc=params.lamda_dsc,lamda_adv=params.lamda_adv,lamda_l1_g=params.lamda_l1_g)\n","\n","# define model path\n","model_path=str(cfg.base_dir)+'/models/'+str(params.dataset)+'/tr_deformation_cgan_unet/ra_en_'+str(ra_en_val)+'_gantype_'+str(params.gan_type)+'/'\n","\n","if(params.data_aug_seg==0):\n","    model_path=str(model_path)+'no_data_aug/'\n","    cfg.aug_en=params.data_aug_seg\n","else:\n","    model_path=str(model_path)+'with_data_aug/'\n","\n","model_path=str(model_path)+'lamda_dsc_'+str(params.lamda_dsc)+'_lamda_adv_'+str(params.lamda_adv)+'_lamda_g_'+str(params.lamda_l1_g)+'/'+\\\n","         str(params.no_of_tr_imgs)+'/'+str(params.comb_tr_imgs)+'_v'+str(params.ver)+\\\n","         '/unet_model_beta1_'+str(params.beta_val)+'_lr_seg_'+str(params.lr_seg)+'_lr_gen_'+str(params.lr_gen)+'_lr_disc_'+str(params.lr_disc)+'/'\n","\n","mp=get_max_chkpt_file(model_path)\n","print('loading deformation field cGAN checkpoint file',mp)\n","# create a session and load the parameters learned\n","saver_geo = tf.train.Saver(max_to_keep=2)\n","sess_geo = tf.Session(config=config)\n","saver_geo.restore(sess_geo,mp)\n","######################################\n","\n","########################################################################\n","#load additive intensity field generator net\n","########################################################################\n","# Define the model graph\n","tf.reset_default_graph()\n","ae_int = model.intensity_transform_cgan_unet(learn_rate_gen=params.lr_gen,learn_rate_disc=params.lr_disc,\\\n","                        beta1_val=params.beta_val,gan_type=params.gan_type,ra_en=params.ra_en,\\\n","                        learn_rate_seg=params.lr_seg,dsc_loss=params.dsc_loss,en_1hot=params.en_1hot,\\\n","                        lamda_dsc=params.lamda_dsc,lamda_adv=params.lamda_adv,lamda_l1_i=params.lamda_l1_i)\n","\n","# define model path\n","model_path=str(cfg.base_dir)+'/models/'+str(params.dataset)+'/tr_intensity_cgan_unet/ra_en_'+str(ra_en_val)+'_gantype_'+str(params.gan_type)+'/'\n","\n","if(params.data_aug_seg==0):\n","    model_path=str(model_path)+'no_data_aug/'\n","    cfg.aug_en=params.data_aug_seg\n","else:\n","    model_path=str(model_path)+'with_data_aug/'\n","\n","model_path=str(model_path)+'lamda_dsc_'+str(params.lamda_dsc)+'_lamda_adv_'+str(params.lamda_adv)+'_lamda_i_'+str(params.lamda_l1_i)+'/'+\\\n","         str(params.no_of_tr_imgs)+'/'+str(params.comb_tr_imgs)+'_v'+str(params.ver)+\\\n","         '/unet_model_beta1_'+str(params.beta_val)+'_lr_seg_'+str(params.lr_seg)+'_lr_gen_'+str(params.lr_gen)+'_lr_disc_'+str(params.lr_disc)+'/'\n","\n","mp=get_max_chkpt_file(model_path)\n","print('loading additive intensity field cGAN checkpoint file ',mp)\n","# create a session and load the parameters learned\n","saver_int = tf.train.Saver(max_to_keep=2)\n","sess_int = tf.Session(config=config)\n","saver_int.restore(sess_int,mp)\n","\n","######################################\n","\n","######################################\n","#  training parameters\n","start_epoch=0\n","n_epochs = 10000\n","disp_step=500\n","mean_f1_val_prev=0.1\n","threshold_f1=0.00001\n","debug_en=0\n","pathlib.Path(best_model_dir).mkdir(parents=True, exist_ok=True)\n","######################################\n","\n","######################################\n","# define current graph - unet\n","tf.reset_default_graph()\n","ae = model.unet(learn_rate_seg=params.lr_seg,en_1hot=params.en_1hot,dsc_loss=params.dsc_loss)\n","######################################\n","\n","######################################\n","# define deformations net for labels\n","df_ae= model.deform_net()\n","######################################\n","\n","######################################\n","#writer for train summary\n","train_writer = tf.summary.FileWriter(logs_path)\n","#writer for dice score and val summary\n","dsc_writer = tf.summary.FileWriter(logs_path)\n","val_sum_writer = tf.summary.FileWriter(logs_path)\n","######################################\n","\n","######################################\n","# create a session and initialize variable to use the graph\n","sess = tf.Session(config=config)\n","sess.run(tf.global_variables_initializer())\n","# Save training data\n","saver = tf.train.Saver(max_to_keep=2)\n","######################################\n","\n","# Run for n_epochs\n","for epoch_i in range(start_epoch,n_epochs):\n","\n","    # sample z's from Gaussian Distribution\n","    z_samples = np.random.normal(loc=0.0, scale=1.0, size=(cfg.batch_size, params.z_lat_dim)).astype(np.float32)\n","\n","    #sample Labelled data shuffled batch\n","    ld_img_batch,ld_label_batch=shuffle_minibatch([train_imgs,train_labels],batch_size=cfg.batch_size,num_channels=cfg.num_channels,axis=2)\n","    if(cfg.aug_en==1):\n","        # Apply affine transformations\n","        ld_img_batch,ld_label_batch=augmentation_function([ld_img_batch,ld_label_batch],dt)\n","\n","    ld_img_batch_orig_tmp=np.copy(ld_img_batch)\n","    ld_label_batch_orig_tmp=np.copy(ld_label_batch)\n","    # Compute 1 hot encoding of the segmentation mask labels\n","    ld_label_batch_orig_1hot = sess.run(df_ae['y_tmp_1hot'],feed_dict={df_ae['y_tmp']:ld_label_batch_orig_tmp})\n","\n","    ############################\n","    ## use Deformation field cGAN to generate additional augmented image,label pairs from labeled samples\n","    flow_vec,ld_img_batch_geo=sess_geo.run([ae_geo['flow_vec'],ae_geo['y_trans']],\\\n","                                feed_dict={ae_geo['x_l']: ld_img_batch_orig_tmp, ae_geo['z']:z_samples, ae_geo['train_phase']: False})\n","\n","    ld_label_batch_geo=sess.run([df_ae['deform_y_1hot']],feed_dict={df_ae['y_tmp']:ld_label_batch_orig_tmp,df_ae['flow_v']:flow_vec})\n","    ld_label_batch_geo=ld_label_batch_geo[0]\n","\n","    ############################\n","    # use additive Intensity field cGAN to generate additional augmented image,label pairs from labeled samples\n","    int_c1,ld_img_batch_int=sess_int.run([ae_int['int_c1'],ae_int['y_int']], feed_dict={ae_int['x']: ld_img_batch_orig_tmp, ae_int['z']:z_samples, ae_int['train_phase']: False})\n","    ld_label_batch_int = ld_label_batch_orig_1hot\n","\n","    ############################\n","    # use additive intensity field cGAN over augmented images generated from deformation field cGAN to create augmented images \\\n","    # that have both spatial deformations and intensity transformations applied in them\n","    ld_img_batch_geo_tmp=np.copy(ld_img_batch_geo)\n","    int_c1,ld_img_batch_geo_int=sess_int.run([ae_int['int_c1'],ae_int['y_int']], feed_dict={ae_int['x']: ld_img_batch_geo_tmp, ae_int['z']:z_samples, ae_int['train_phase']: False})\n","    ld_label_batch_geo_int = np.copy(ld_label_batch_geo)\n","\n","    # shuffle the quantity/number of images chosen from \n","    # deformation field cGAN --> no_g,\n","    # intensity field cGAN   --> no_i,\n","    # both cGANs             --> no_b,\n","    # and rest (batch_size - (no_g+no_i+no_b)) are original images with conventional affine transformations.\n","    no_g=np.random.randint(1, high=5)\n","    no_i=np.random.randint(5, high=10)\n","    no_b=np.random.randint(10, high=15)\n","\n","    ld_img_batch=ld_img_batch_orig_tmp\n","    ld_label_batch=ld_label_batch_orig_1hot\n","\n","    ld_img_batch[0:no_g] = ld_img_batch_geo[0:no_g]\n","    ld_label_batch[0:no_g] = ld_label_batch_geo[0:no_g]\n","    ld_img_batch[no_g:no_i] = ld_img_batch_int[no_g:no_i]\n","    ld_label_batch[no_g:no_i] = ld_label_batch_int[no_g:no_i]\n","    ld_img_batch[no_i:no_b] = ld_img_batch_geo_int[no_i:no_b]\n","    ld_label_batch[no_i:no_b] = ld_label_batch_geo_int[no_i:no_b]\n","\n","    #Optimer over this batch of images\n","    train_summary,_ =sess.run([ae['train_summary'],ae['optimizer_unet_seg']], feed_dict={ae['x']: ld_img_batch, ae['y_l']: ld_label_batch,\\\n","                               ae['select_mask']: False, ae['train_phase']: True})\n","\n","    if(epoch_i%val_step_update==0):\n","        train_writer.add_summary(train_summary, epoch_i)\n","        train_writer.flush()\n","\n","    if(epoch_i%val_step_update==0):\n","        ##Save the model with best DSC for Validation Image\n","        mean_f1_arr=[]\n","        f1_arr=[]\n","        for val_id_no in range(0,len(val_list)):\n","            val_img_crop_tmp=val_img_crop[val_id_no]\n","            val_label_crop_tmp=val_label_crop[val_id_no]\n","            val_label_orig_tmp=val_label_orig[val_id_no]\n","            pixel_size_val=pixel_val_list[val_id_no]\n","\n","            # Compute segmentation mask and dice_score for each validation subject\n","            pred_sf_mask = f1_util.calc_pred_sf_mask_full(sess, ae, val_img_crop_tmp)\n","            re_pred_mask_sys,f1_val = f1_util.reshape_img_and_f1_score(pred_sf_mask, val_label_orig_tmp, pixel_size_val)\n","\n","            #concatenate dice scores of each val image\n","            mean_f1_arr.append(np.mean(f1_val[1:cfg.num_classes]))\n","            f1_arr.append(f1_val[1:cfg.num_classes])\n","\n","        #avg mean over 2 val subjects\n","        mean_f1_arr=np.asarray(mean_f1_arr)\n","        mean_f1=np.mean(mean_f1_arr)\n","        f1_arr=np.asarray(f1_arr)\n","\n","        if (mean_f1-mean_f1_val_prev>threshold_f1 and epoch_i!=start_epoch):\n","            print(\"prev f1_val; present_f1_val\", mean_f1_val_prev, mean_f1, mean_f1_arr)\n","            mean_f1_val_prev = mean_f1\n","\n","            # to save the best model with maximum dice score over the entire n_epochs\n","            print(\"best model saved at epoch no. \", epoch_i)\n","            mp_best = str(best_model_dir) + str(checkpoint_filename) + '_best_model_epoch_' + str(epoch_i) + '.ckpt'\n","            saver.save(sess, mp_best)\n","\n","        #calc. and save validation image dice summary\n","        dsc_summary_msg = sess.run(ae['val_dsc_summary'], feed_dict={ae['rv_dice']:np.mean(f1_arr[:,0]),\\\n","                                ae['myo_dice']:np.mean(f1_arr[:,1]),ae['lv_dice']:np.mean(f1_arr[:,2]),ae['mean_dice']: mean_f1})\n","        val_sum_writer.add_summary(dsc_summary_msg, epoch_i)\n","        val_sum_writer.flush()\n","\n","    if ((epoch_i==n_epochs-1) and (epoch_i != start_epoch)):\n","        # model saved at last epoch\n","        mp = str(save_dir) + str(checkpoint_filename) + '_epochs_' + str(epoch_i) + '.ckpt'\n","        saver.save(sess, mp)\n","        try:\n","            mp_best\n","        except NameError:\n","            mp_best=mp\n","\n","sess.close()\n","######################################\n","# restore best model and predict segmentations on test subjects\n","saver_new = tf.train.Saver()\n","sess_new = tf.Session(config=config)\n","saver_new.restore(sess_new, mp_best)\n","print(\"best model chkpt\",mp_best)\n","print(\"Model restored\")\n","\n","#########################\n","# To compute inference on test images on the model that yields best dice score on validation images\n","f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir,orig_img_dt,test_list,struct_name)\n","######################################\n","# To compute inference on validation images on the best model\n","save_dir_tmp=str(save_dir)+'/val_imgs/'\n","f1_util.pred_segs_acdc_test_subjs(sess_new,ae,save_dir_tmp,orig_img_dt,val_list,struct_name)\n","######################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aQSIfVO4TuWI","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}